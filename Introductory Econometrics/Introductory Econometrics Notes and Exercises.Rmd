---
title: "Introductory Econometrics Notes and Exercises"
output: pdf_document
header-includes:
  - \pagenumbering{gobble}
  - \usepackage{amsmath}
  - \usepackage{mathrsfs}
  - \usepackage{xfrac}
  - \newcommand{\indep}{\perp\!\!\!\!\perp}
---

```{r Setup, echo=FALSE,include=FALSE}
library(tidyverse)
library(car)
library(wooldridge)
library(datawizard)
options(scipen = 99999999)
```

\newpage

# Math Refresher A

## Notes


### Summation Proofs

$$\sum_{i=1}^{n}(x_i-\bar{x})^2=\sum_{i=1}^{n}(x_i^2-2x_i\bar{x}+\bar{x}^2)$$
$$=\sum_{i=1}^{n}x_i^2-2\sum_{i=1}^{n}x_i\bar{x}+\sum_{i=1}^{n}\bar{x}^2$$
$$=\sum_{i=1}^{n}x_i^2-2\bar{x}\sum_{i=1}^{n}x_i+\bar{x}\sum_{i=1}^{n}\bar{x}$$
$$=\sum_{i=1}^{n}x_i^2-2n\bar{x}^2+n\bar{x}^2$$
$$=\sum_{i=1}^{n}x_i^2-n\bar{x}^2$$
$$=\sum_{i=1}^{n}x_i^2-\bar{x}\sum_{i=1}^{n}x_i$$
$$=\sum_{i=1}^{n}(x_i^2-\bar{x}x_i)$$
$$=\sum_{i=1}^{n}x_i(x_i-\bar{x})$$

$$\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})=\sum_{i=1}^{n}(x_iy_i-x_i\bar{y}-y_i\bar{x}+\bar{x})$$
$$=\sum_{i=1}^{n}x_iy_i-\sum_{i=1}^{n}x_i\bar{y}-\sum_{i=1}^{n}y_i\bar{x}+\sum_{i=1}^{n}\bar{x}\bar{y}$$
$$=\sum_{i=1}^{n}x_iy_i-\bar{y}\sum_{i=1}^{n}x_i-\bar{x}\sum_{i=1}^{n}y_i+\bar{y}\sum_{i=1}^{n}\bar{x}$$
$$=\sum_{i=1}^{n}x_iy_i-n\bar{y}\bar{x}-n\bar{x}\bar{y}+n\bar{y}\bar{x}$$
$$=\sum_{i=1}^{n}x_iy_i-n\bar{y}\bar{x}$$
$$=\sum_{i=1}^{n}x_iy_i-\bar{x}\sum_{i=1}^{n}y_i$$
$$=\sum_{i=1}^{n}(x_iy_i-\bar{x}y_i)$$
$$=\sum_{i=1}^{n}y_i(x_i-\bar{x})=\sum_{i=1}^{n}x_i(y_i-\bar{y})$$

### Natural Logarithm
\begin{enumerate}
\item $\ln(xy)=\ln(x)+\ln(y)$
\item $\ln(\frac{x}{y})=\ln(x)-\ln(y)$
\item $\ln(x^c)=c\ln(x)$
\end{enumerate}

The difference in natural logs can be used to approximate proportionate changes. Let $x_0$ and $x_1$ be positive values. Then, for small changes in x
$$\ln(x_1)-\ln(x_0)\approx \frac{x_1-x_0}{x_0}=\frac{\Delta x}{x_0}$$
Thus, $$ 100\cdot\Delta\ln(x)\approx \%\Delta x$$

### Elasticity

The \textbf{elasticity} of $y$ with respect to $x$ equals
$$\frac{\%\Delta y}{\%\Delta x}=\frac{(y_1-y_0)/y_0}{(x_1-x_0)/x_0}=\frac{\Delta y/y_0}{\Delta x /x_0}=\frac{\Delta y}{\Delta x}\cdot\frac{x_0}{y_0}$$
Defining a linear model $y=\beta_0+\beta_1 x$, the elasticity of $y$ with respect to $x$ equals
$$\frac{\%\Delta y}{\%\Delta x}=\frac{\Delta y/y}{\Delta x /x}=\frac{\Delta y}{\Delta x}\cdot\frac{x}{y}=\frac{\beta_1 \Delta x}{\Delta x}\cdot\frac{x}{\beta_0+\beta_1 x}=\beta_1\cdot\frac{x}{\beta_0+\beta_1 x}\approx\frac{\Delta\ln(y)}{\Delta\ln(x)}$$
If we use the above approximation for both $x$ and $y$, then the elasticity is approximately equal to $\frac{\Delta\ln(y)}{\Delta\ln(x)}$. Thus, a \textbf{constant elasticity model} is approximated by
$$\ln(y)=\beta_0+\beta_1\ln(x)$$ where $\beta_1$ is the approximate elasticity of $y$ with respect to $x$.

A \textbf{semi-elasticity model} approximates the percentage change in $y$ with respect to a unit change in $x$ and takes the form $$\ln(y)=\beta_0+\beta_1 x$$ where $\beta_1$ is the semi-elasticity of $y$ with respect to $x$. In other words, $\%\Delta y = 100\beta_1\Delta x \to \beta_1\approx\frac{\%\Delta y}{100\Delta x}$.

Another relationship of some interest is $$y=\beta_0+\beta_1 \ln(x)$$
Using calculus, we can derive $$\Delta y = \beta_1\Delta\ln(x)$$
and thus $$\beta_1 = \frac{\Delta y}{\Delta \ln(x)}\approx\frac{\Delta y}{\frac{\%\Delta x}{100}}$$
In other words, $\beta_1/100$ is the unit change in $y$ when $x$ increases by 1%.


## Exercises
```{r Problem 1,echo=FALSE,include=FALSE}
ex1_data <- c(300,440,350,1100,640,480,450,700,670,530)
```

\begin{enumerate}
\item 
\begin{enumerate}
\item Mean = `r mean(ex1_data)`
\item Median = `r median(ex1_data)`
\item Mean = `r mean(ex1_data) / 100`
Median = `r median(ex1_data) / 100`
\item Mean = `r mean(c(ex1_data[1:7],900,ex1_data[9:10]))`
Median = `r median(c(ex1_data[1:7],900,ex1_data[9:10]))`
\end{enumerate}
\item 
\begin{enumerate}
\item See below. The intercept indicates that, on average, individuals who live on campus (0 miles from the school) will miss three classes.
\item `r 3 + 0.2 * 5` classes
\item `r 0.2 * (20 - 10)` classes
\end{enumerate}
\end{enumerate}
```{r Problem 2 Graph, echo=FALSE,include=TRUE,fig.align='center',fig.width=4,fig.height=3}

distance <- 0:20
missed <- 3 + 0.2*distance

ggplot(mapping = aes(x=distance,y=missed))+
  geom_line(color = "blue")+
  geom_point(color = "red", size=0.5, alpha=0.5)+
  xlab("distance")+
  ylab("missed")+
  theme_bw()
```
\begin{enumerate}
\setcounter{enumi}{2}
\item `r 120-9.8*15+.03*200` CDs. This suggests using linear functions to describe demand curves may not be realistic/a good idea. Some form of an elasticity model would likely be more suitable.
\item 
\begin{enumerate}
\item A `r 6.4-5.6` percentage point decrease.
\item A `r (6.4-5.6)/6.4`\% fall.
\end{enumerate}
\item The correct terminology would be the stock return increased by 3 percentage points, which is a 20\% increase in the return on the stock.
\item 
\begin{enumerate}
\item `r 100 * (42000-35000)/35000`\%
\item $\approx$ `r 100 * (log(42000)-log(35000))`\%
\end{enumerate}
\item 
\begin{enumerate}
\item \$ `r round(exp(10.6 + .027 * 0),2)`

\$ `r format(round(exp(10.6 + .027 * 5),2), nsmall=2L)`
\item $\%\Delta\text{salary}=100(.027)(5)=$ `r .027 * 500`\%
\item `r ((exp(10.6 + .027 * 5)-exp(10.6))/(exp(10.6))-.135)/.135 * 100`\% error
\end{enumerate}
\item The intercept indicates that, with no sales tax, the proportionate growth in employment would be .043. The slope indicates that for every unit increase in sales tax, we would expect the proportionate growth in employment to decrease by .78.
\item 
\begin{enumerate}
\item See below
\item The most notable difference is the change to marginal returns in the nonlinear function. A linear model would have constant marginal returns to yield with respect to fertilizer while the given relationship displays diminishing marginal returns.
\end{enumerate}
\end{enumerate}
```{r Problem 9 Graph, echo=FALSE,include=TRUE,fig.align='center',fig.height=3,fig.width=4}
fertilizer <- seq(1,100,5)
yield <- 120 + .19 * sqrt(fertilizer)
ggplot(mapping = aes(x = fertilizer, y = yield))+
  geom_line(color = "blue")+
  geom_point(color = "red", size = 0.5, alpha = 0.5)+
  xlab("fertilizer")+
  ylab("yield")+
  theme_bw()
```

\begin{enumerate}
\setcounter{enumi}{9}
\item 
\begin{enumerate}
\item It's not of much interest by itself. It suggests a class with 0 students would expect a test score of 45.6, which doesn't make sense or have any real meaning.
\item $$\frac{\partial score}{\partial class}=.082 - .000294\cdot class=0$$
$$\to class^*=\frac{.082}{.000294}\approx 279\text{ students}$$

The highest achievable test score is about `r round(45.6 + .082 * 279 - .000147 * (279^2))`.
\item See below
\item No, this equation may give an idea about what one can expect for \texttt{score} given \texttt{class}, but it's unrealistic to expect exact results.
\end{enumerate}
\end{enumerate}
```{r Problem 10 Graph, echo=FALSE,include=TRUE,fig.align='center',fig.height=3,fig.width=4}
class_ <- seq(1,300,25)
score <- 45.6 + .082 * class_ - .000147 * class_**2
ggplot(mapping = aes(x = class_, y = score))+
  geom_line(color = "blue")+
  geom_point(color = "red", size = 0.5, alpha = 0.5)+
  xlab("class")+
  ylab("score")+
  theme_bw()
```
\newpage
\begin{enumerate}
\setcounter{enumi}{10}
\item 
\begin{enumerate}
\item $$\bar{y}=\frac{y_1+y_2}{2}=\frac{\beta_0+ \beta_1 x_1+\beta_0+ \beta_1 x_2}{2}$$
$$=\frac{2\beta_0+ \beta_1 (x_1+x_2)}{2}$$
$$=\beta_0+\beta_1 \frac{(x_1+x_2)}{2}=\beta_0+\beta_1 \bar{x}$$
\item $$\bar{y}=\frac{\sum_{i=1}^{n}y_i}{n}=\frac{\sum_{i=1}^{n}(\beta_0+\beta_1 x_i)}{n}$$
$$=\frac{n\beta_0+ \beta_1 \sum_{i=1}^{n}x_i}{n}$$
$$=\beta_0+\beta_1 \frac{n\bar{x}}{n}=\beta_0+\beta_1 \bar{x}$$
\end{enumerate}
\item 
\begin{enumerate}
\item $$\bar{x}=\frac{1}{n}\sum_{i=1}^{n}x_i=\frac{1}{n}\left(\sum_{i=1}^{n_1}x_i+\sum_{i=n_1+1}^{n}x_i\right)$$
$$=\frac{1}{n}\left(n_1\bar{x_1}+n_2\bar{x_2}\right)$$
$$=\frac{n_1}{n}\bar{x_1}+=\frac{n_2}{n}\bar{x_2}$$
$$=w_1\bar{x_1}+=w_2\bar{x_2}$$
\item Yes, they represent the relative portions of the sample space in $i=1,\dots,n_1$ and $i=n_1+1,\dots,n$.
\item The case in part (a) applies to all cases for $g\in\mathbb{Z}^+\leq n$.
\end{enumerate}
\item 
\begin{enumerate}
\item No, take the sample $\{x_1,x_2\}=\{2,2\}$. Then,
$$\sum_{i=1}^{n}\frac{1}{x_i}=\frac{1}{2}+\frac{1}{2}=1$$ while $$\frac{1}{\sum_{i=1}^{n}x_i}=\frac{1}{2+2}=\frac{1}{4}$$
\item No, see part (a) where $x_i = 2\ \forall\ i$.
\end{enumerate}
\end{enumerate}

\newpage

# Math Refresher B

## Notes

### Experiments

An \textbf{experiment} is a procedure that can theoretically be conducted an infinite number of times and has a well-defined set of outcomes.

A \textbf{random variable} is a variable that takes on numerical values and has an outcome determined by an experiment.

### Variables

A \textbf{Bernoulli} (or \textbf{binary) random variable} is a random variable that can only take on the values zero and one.

A \textbf{discrete random variable} is one that takes on only a finite or countably infinite number of values. A Bernoulli random variable is the simplest example of a discrete random variable.

A \textbf{continuous random variable} is a random variable that takes on any real value with \textit{zero} probability.

### Density Functions

A \textbf{probability density function (pdf)} summarizes the information concerning the possible outcomes of a random variable and the corresponding probabilities. A pdf of a random variable $X$ is generally denoted $f(x)\text { or }f_x \equiv P(X=x)$.

A \textbf{cumulative distribution function (cdf)} is a function that describes the cumulative probability that a random variable's value is less than (or equal to, if continuous) a given value. A cdf of a random variable $X$ is generally denoted $F(x)\text{ or }F_x \equiv P(X\leq x)$

\underline{Cumulative Distribution Function Properties:}
\begin{enumerate}
\item For any number $c$, $P(X>c)=1-F(c)$
\item For any numbers $a<b$, $P(a<X\leq b)=F(b)-F(a)$
\item For a continuous random variable X, $P(X\geq c)=P(X>c)$
\item For a continuous random variable X, $P(a<X<b)=P(a\leq X \leq b)=P(a\leq X <b )=P(a < X \leq b)$
\end{enumerate}

### Independence

Let $X$ and $Y$ be discrete random variables. Then, $(X,Y)$ have a \textbf{joint distribution}, which is fully described by the \textbf{joint probability density function} of $(X,Y)$: $$f_{X,Y}(x,y)=P(X=x,Y=y)$$

Two random variables $X$ and $Y$ are said to be \textbf{independent} if, and only if, $$f_{X,Y}(x,y)=f_X(x)f_Y(y)$$ or $$P(X=x,Y=y)=P(X=x)P(Y=y)$$ for all $x$ and $y$. The pdfs $f_X$ and $f_Y$ are often called the \textbf{marginal probability density functions} to distinguish the from the joint pdf $f_{X,Y}$.

Beyond the case of two random variables, the same concept applies. Random variables $X_1, X_2,\dots, X_n$ are \textbf{independent random variables} if, and only if, their joint pdf is the product of the individual pdfs for any $(x_1, x_2,\dots, x_n)$. This definition of independence holds for both continuous and discrete random variables.

Given independent outcomes with 'success' rate $\theta$, the pdf $(X \sim \text{Binomial}(n,\theta))$ is equal to \newline
$f(x)=\binom{n}{x}\theta^x(1-\theta)^{n-x}$ where $\binom{n}{x}={{}^{n}C_{x}}=\frac{n!}{x!(n-x)!}$.

### Conditional Distributions

The \textbf{conditional distribution} of $Y$ given $X$ is summarized by the \textbf{conditional probability density function}, defined by $$f_{Y|X}(y|x)=\frac{f_{X,Y}(x,y)}{f_{X}(x)}$$ for all values of $x$ such that $f_X(x)>0$. The interpretation of the conditional probability density function is $$f_{Y|X}(y|x)=P(Y=y|X=x)$$

### Expected Values

If $X$ is a random variable, the \textbf{expected value} (or \textbf{expectation}) of $X$, denoted $E[X]$ and sometimes $\mu_X$ or simply $\mu$, is a weighted average of all possible values of $X$. The weights are determined by the probability distribution function. Sometimes, the expected value is called the \textit{population mean}, especially when we want to emphasize that $X$ represents some variable in a population. For a discrete random variable $X$ that takes on values $\{x_1,\dots,x_n\}$, $$E[X]=x_1f(x_1)+\dots+x_nf(x_n)=\sum_{i=1}^{n}x_if(x_i)$$ If $X$ is a continuous random variable, then $E[X]$ is defined through an integral as $$E[X]=\int_{-\infty}^{\infty}xf(x)dx,$$which we assume is well-defined.

\underline{Expected Values Properties}
\begin{enumerate}
\item For a constant $c$, $E[c]=c$
\item For any constants $a$ and $b$, $E[aX+b]=aE[x]+b$
\item If $\{a_1,a_2,\dots,a_n\}$ are constants and $\{X_1,X_2,\dots,X_n\}$ are random variables, then $$E[a_1X_1+a_2X_2+\dots+a_nE[X_n]=E\left[\sum_{i=1}^{n}a_iX_i\right]=\sum_{i=1}^{n}a_iE[X_i]$$
\end{enumerate}

For $X\sim \text{Binomial}(n,\theta)$, we can rewrite $X$ as $Y_1+\dots+Y_n$, where each $Y_i\sim\text{Bernoulli}(\theta)$. Then, $$E[X]=\sum_{i=1}^{n}E[Y_i]=\sum_{i=1}^{n}\theta=n\theta$$

### Variance

For a random variable $X$, $$\text{Var}(x)=E[(X-\mu)^2]$$ The \textbf{variance} tells us the expected (squared) distance of $X$ from its mean and is sometimes denoted $\sigma_x^2$ or just $\sigma^2$. For a Bernoulli random variable $X$ $$\sigma_x^2 = E[X^2]-E[X]^2=\theta - \theta^2=\theta(1-\theta)$$

\newpage

\underline{Variance Properties}
\begin{enumerate}
\item $\text{Var}(x)=0$ if, and only if, there is a constant c such that $P(X=c)=1$, in which case $E[X]=c$. In other words, this first property says that the variance of any constant is zero and if a random variable has zero variance, then it is essentially constant.
\item For any constants $a$ and $b$, $\text{Var}(aX+b)=a^2\text{Var}(X)$
\item For any constants $a$ and $b$, $$\text{Var}(aX+bY)=a^2\text{Var}(X)+2ab\text{Cov}(X,Y)+b^2\text{Var}(Y)$$
\item If $\{X_1,\dots,X_n\}$ are pairwise uncorrelated random variables and $a_i:i=1,\dots,n$ are constants, then $$\text{Var}(a_1X_1+\dots+a_nX_n)=a_1^2\text{Var}(X_1)+\dots+a_n^2\text{Var}(X_n)=\text{Var}\left(\sum_{i=1}^{n}a_iX_i\right)=\sum_{i=1}^{n}a_i^2\text{Var}(X_i)$$
\end{enumerate}

### Standard Deviation

The \textbf{standard deviation} of a random variable, denoted $\text{sd}(X)$, is simply the positive square root of the variance: $\text{sd}(X)=+\sqrt{\text{Var}(X)}$. The standard deviation is sometimes denoted $\sigma_X$, or simply $\sigma$, when the random variable is understood.

\underline{Standard Deviation Properties}
\begin{enumerate}
\item For any constant $c$, $\text{sd}(c)=0$.
\item For any constants $a$ and $b$, $$\text{sd}(aX+b)=|a|\text{sd}(X)=|a|\sigma_X$$
\end{enumerate}

### Standardized Random Variables

If $X$ is a random variable, we can redefine a random variable $$Z\equiv \frac{X-\mu}{\sigma},$$ which we can write as $Z=aX+b$, where $a\equiv(1/\sigma)$ and $b\equiv -(\mu/\sigma)$. Then, $$E[Z]=aE[X]+b=(\mu/\sigma)-(\mu/\sigma)=0$$ and $$\text{Var}(Z)=a^2\text{Var}(X)=1$$ Thus, the random variable $Z$ has a mean of zero and a variance (and therefore a standard deviation) equal to one. This procedure is sometimes known as standardizing the random variable $X$, and $Z$ is called a \textbf{standardized random variable}.

### Skewness and Kurtosis

We can use the standardized version of a random variable to define other features of the distribution of a random variable. These features are described by using what are called \textit{higher order moments}. For example, the third moment of the standardized random variable $Z$ is used to determine whether a distribution is symmetric about its mean. We can write $$E[Z^3]=\frac{E[(X-\mu)^3]}{\sigma^3}$$ Generally, $\frac{E[(X-\mu)^3]}{\sigma^3}$ is viewed as a measure of \textbf{skewness} in the distribution of $X$. If $X$ has a symmetric distribution about $\mu$, then $Z$ has a symmetric distribution about zero. That means the density of $Z$ at any two points $z$ and $-z$ is the same.

It also can be informative to compute the fourth moment of $Z$ $$E[Z^4]=\frac{E[(X-\mu)^4]}{\sigma^4}$$The fourth moment $E[Z^4]$ is called a measure of \textbf{kurtosis} in the distribution of $X$. Generally, larger values mean that the tails in the distribution of $X$ are thicker.

### Covariance and Correlation

The \textbf{covariance} between two random variables $X$ and $Y$, sometimes called the \textit{population covariance} to emphasize that it concerns the relationship between two variables describing a population, is defined as the expected value of the product $(X-\mu_X)(Y-\mu_Y)$: $$\text{Cov}(X,Y)\equiv E\left[(X-\mu_X)(Y-\mu_Y)\right],$$which is sometimes denoted $\sigma_{XY}$. If $\sigma_{XY}>0$, then, on average, when $X$ is above its mean, $Y$ is also above its mean. If $\sigma_{XY}<0$,  then, on average, when $X$ is above its mean, $Y$ is below its mean. Note that $$\text{Cov}(X,Y) = E\left[(X-\mu_X)(Y-\mu_Y)\right]= E\left[(X-\mu_X)Y\right]$$$$= E\left[X(Y-\mu_Y)\right]=E[XY]-E[X]E[Y]$$ Covariance measures the amount of linear dependence between two random variables. A positive covariance indicates that two random variables move in the same direction, while a negative covariance indicates they move in opposite directions.

\underline{Covariance Properties}
\begin{enumerate}
\item If $X$ and $Y$ are independent, then $\text{Cov}(X,Y)=0$

This property stems from the fact that $E[XY]=E[X]E[Y]$ when $X$ and $Y$ are independent. It is important to remember that the converse of is not true: zero covariance between $X$ and $Y$ does not imply that $X$ and $Y$ are independent.
\item For any constants $a_1,b_1,a_2,$ and $b_2$, $$\text{Cov}(a_1X+b_1,a_2Y+b_2)=a_1a_2\text{Cov}(X,Y)$$
\item From the \textbf{Cauchy-Schwartz inequality}: $$|\text{Cov}(X,Y)|\leq \text{sd}(X)\text{sd}(Y)$$
\end{enumerate}

The fact that the covariance depends on units of measurement is a deficiency that is overcome by the \textbf{correlation coefficient} between $X$ and $Y$: $$\text{Corr}(X,Y)\equiv\frac{\text{Cov}(X,Y)}{\text{sd}(X)\text{sd}(Y)}=\frac{\sigma_{XY}}{\sigma_X\sigma_Y}$$the correlation coefficient between $X$ and $Y$ is sometimes denoted $\rho_{XY}$ (and is sometimes called the \textit{population correlation}).

\underline{Correlation Properties}
\begin{enumerate}
\item $-1\leq\text{Corr}(X,Y)\leq 1$
\item For any constants $a_1,b_1,a_2,$ and $b_2$, $$\text{Corr}(a_1X+b_1,a_2Y+b_2)=\text{Corr}(X,Y)$$ if $a_1a_2>0$ or $$\text{Corr}(a_1X+b_1,a_2Y+b_2)=-\text{Corr}(X,Y)$$ if $a_1a_2<0$.
\end{enumerate}

### Conditional Expectation

The \textbf{conditional expectation} of a random variable is the expected or average value of one random variable, called the dependent or explained variable, that depends on the values of one or more other variables, called the independent or explanatory variables. When $Y$ is a discrete random variable $$E[Y|x]=\sum_{i=1}^{n}y_i f_{Y|X}(y_i|x)$$When $Y$ is a continuous random variable $$E[Y|x]=\int_{-\infty}^{\infty}y f_{Y|X}(y|x)dy$$

\underline{Conditional Expecation Properties}
\begin{enumerate}
\item $E[c(X)|X]=c(X)$, for any function $c(X)$.
\item For any functions $a(X)$ and $b(X)$, $$E[a(X)Y+b(X)|X]=a(X)E[Y|X]+b(X)$$
\item If $X$ and $Y$ are independent, $E[Y|X]=E[Y]$.
\item From the \textbf{law of iterated expectations}, $E[E[Y|X]]=E[Y]$.
\item From a more general version of the law of iterated expectation, $E[Y|X]=E[E[Y|X,Z]|X]$.
\item If $E[Y|X]=E[Y]$, then $\text{Cov}(X,Y)=0$.
\item If $E[Y^2]<\infty$ and $E[g(X)^2]<\infty$ for some function $g$, then $E[[Y-E[Y|X]]^2|X]\leq E[[Y-g(X)]^2|X]$ and $E[[Y-E[Y|X]]^2]\leq E[[Y-g(X)]^2]$. This property is very useful in predicting or forecasting contexts. The first inequality says that, if we measure prediction inaccuracy as the expected squared prediction error, conditional on $X$, then the conditional mean is better than any other function of $X$ for predicting $Y$. The conditional mean also minimizes the unconditional expected squared prediction error.
\end{enumerate}

### Conditional Variance

Given random variables $X$ and $Y$, the variance of $Y$, conditional on $X = x$, is simply the variance associated with the conditional distribution of $Y$, given $X = x:E[\left(Y-E[Y|x]\right)^2|x]$. The formula can be rewritten as $$\text{Var}(Y|X=x)=E[Y^2|x]-E[Y|x]^2$$

\underline{Conditional Variance Properties}
\begin{enumerate}
\item If $X$ and $Y$ are independent, then $\text{Var}(Y|X)=\text{Var}(Y)$
\end{enumerate}

### Normal Distribution

A \textbf{normal random variable} is a continuous random variable that can take on any value. Its probability density function has the familiar bell-shaped graph and can be written as $$f(x)=\frac{1}{\sigma\sqrt{2\pi}}\exp[-(x-\mu)^2/2\sigma^2],\ -\infty<x<\infty$$ We say that $X$ has a \textbf{normal distribution} with expected value $\mu$ and variance $\sigma^2$, written as $X\sim\mathcal{N}(\mu,\sigma^2)$.  Because the normal distribution is symmetric about $\mu$, $\mu$ is also the median of X. The normal distribution is also sometimes called the Gaussian distribution after Carl Friedrich Gauss.

One special case of the normal distribution is the \textbf{standard normal distribution} where the mean is zero and the variance is unity. If a random variable $Z$ has a Normal(0,1) distribution, then we say it has a standard normal distribution, and its pdf is given by $$\phi(z)=\frac{1}{\sqrt{2\pi}}\exp(-z^2/2),\ -\infty<z<\infty$$

\underline{Normal Distribution Properties}
\begin{enumerate}
\item If $X\sim\mathcal{N}(\mu,\sigma^2)$, then $(X-\mu)/\sigma\sim\mathcal{N}(0,1)$
\item If $X\sim\mathcal{N}(\mu,\sigma^2)$, then $aX+b\sim\mathcal{N}(a\mu+b,a^2\sigma^2)$
\item If $X$ and $Y$ are jointly normally distributed, then they are independent if, and only if, $\text{Cov}(X,Y)=0$
\item Any linear combination of independent, identically distributed normal random variables has a normal distribution.
\end{enumerate}

### Chi-Square Distribution

The chi-square distribution is obtained directly from independent, standard normal random variables. Let $Z_i, i=1,2,\dots,n$ be independent random variables, each distributed as standard normal. Define a new random variable as the sum of the squares of the $Z_i$: $$X=\sum_{i=1}^{n}Z_i^2$$Then, $X$ has what is known as a \textbf{chi-square distribution} with $n$ \textbf{degrees of freedom}. We write this as $X\sim\chi_n^2$ where the expected value of $X$ is $n$ and the variance of $X$ is $2n$.

$$E[X]=E\left[\sum_{i=1}^{n}Z_i^2\right]$$
$$=\sum_{i=1}^{n}E\left[Z_i^2\right]$$
$$=\sum_{i=1}^{n}\left(E\left[Z_i^2\right]-E\left[Z_i\right]^2\right)$$
$$=\sum_{i=1}^{n}\sigma_Z^2$$
$$=\sum_{i=1}^{n}1=n$$

$$\text{Var}(X)=\text{Var}\left(\sum_{i=1}^{n}Z_i^2\right)$$
$$=\sum_{i=1}^{n}\text{Var}\left(Z_i^2\right)$$
$$=\sum_{i=1}^{n}\left(E\left[Z_i^4\right]-E\left[Z_i^2\right]^2\right)$$
$$=\sum_{i=1}^{n}\left(3-1\right)=2n$$
Where $E[Z^4]=3$ because the kurtosis of a standard normal variable is 3, and $\text{Var}\left(\sum_{i=1}^{n}Z_i^2\right)=\sum_{i=1}^{n}\text{Var}\left(Z_i^2\right)$ because the $Z_i$ are independent.

### $t$ distribution

A \textbf{t distribution} is obtained from a standard normal and a chi-square random variable. Let $Z$ have a standard normal distribution and let $X$ have a chi-square distribution with $n$ degrees of freedom. Further, assume that $Z$ and $X$ are independent. Then, the random variable $$T=\frac{Z}{\sqrt{X/n}}$$has a $t$ distribution with $n$ degrees of freedom. This is denoted by $T\sim t_n$ where $n$ comes from the degrees of freedom of the chi-square random variable in the denominator. The pdf of the $t$ distribution has a shape similar to that of the standard normal distribution (maintaining a zero expected value), except that it is more spread out (with a variance of $n/(n-2)$) and therefore has more area in the tails. As the degrees of freedom gets large, the $t$ distribution approaches the standard normal distribution.

### F Distribution
To define an $F$ random variable, let $X_1\sim\chi_{k_1}^2$, and $X_2\sim\chi_{k_2}^2$ and assume that $X_1$ and $X_2$ are independent. Then, the random variable $$F=\frac{X_1/k_1}{X_2/k_2}$$ has an \textbf{F distribution} with $(k_1,k_2)$ degrees of freedom. This is denoted as $F\sim F_{k_1,k_2}$

## Exercises
\begin{enumerate}
\item His or her eventual SAT score is viewed as a random variable because his or her score is a variable that takes on numerical values and has an outcome determined by an experiment (the test). The test score is stochastic as it can change from day-to-day or depending on other various circumstances/conditions.
\item 
\begin{enumerate}
\item $P(X\leq 6)=$ `r pnorm(6, mean = 5, sd=2)`
\item $P(X>4)=1-P(X\leq 4)= $ `r 1-pnorm(4, mean = 5, sd=2)`
\item $P(|X-5|>1)=P([X-5>1] \text{ or }[X-5<-1])=1-P(4<X<6)=1-(P(X<6)-P(X<4))=$ `r 1-(pnorm(6, mean = 5, sd=2)-pnorm(4, mean = 5, sd=2))`
\end{enumerate}
\item 
\begin{enumerate}
\item `r .5^10` or `r 100 * .5^10`\%
\item `r .5^10 * 4170` mutual funds
\item $P(\text{At Least One})=1-P(\text{None})=1-(1-.5^{10})^{4170}=$ `r 1-(1-.5^10)^4170`

Similarly, this equals $1-\binom{4170}{0}(.5^{10})^0(1-.5^{10})^{4170}=$ `r 1-pbinom(0, 4170, .5**10)`
\item $P(X\geq5) = 1-P(X<4)=$ `r 1-pbinom(4, 4170, .5**10)`
\end{enumerate}
\item $P(X\geq.6)=1-P(X<.6)=F(.6)=$ `r 3*.6^2-2*.6^3`
\item 
\begin{enumerate}
\item $P(\text{At least one})=1-P(\text{None})=1-\binom{12}{0}(.2)^0(.8)^{12}=$ `r 1-pbinom(0,12,.2)`
\item $P(X\geq 2)=1-P(X<1)=1-\binom{12}{1}(.2)^1(.8)^{11}=$ `r 1-pbinom(1,12,.2)`
\end{enumerate}
\item $E[X]=\int_{0}^{3}\frac{x^2}{9}xdx=\frac{1}{9}\int_{0}^{3}x^3dx=\frac{1}{9}\left[\frac{x^4}{4}\right]_{0}^{3}=\frac{81}{36}=\frac{9}{4}$
\item $E[\text{Made FTs}]=.74*8=$ `r .74*8`
\item $E[GPA]=3.5(\frac{2}{9})+3(\frac{7}{9})=\frac{28}{9}\approx$ `r round(28/9,2)`
\item $E[\text{salary}]=52.3\times 1000=$ \$`r 52.3*1000`

$\sigma_{\text{salary}}=|1000|\times14.6=$ \$`r 1000*14.6`
\item 
\begin{enumerate}
\item $E[GPA|SAT=800]=.70+.002(800)=$ `r .70+.002*(800)`

$E[GPA|SAT=1400]=.70+.002(800)=$ `r .70+.002*(1400)`
The difference in expected GPAs is fairly large, but the difference in SAT scores is also rather large. I don't feel these estimates are entirely unreasonable.
\item $E[GPA]=E[E[GPA|SAT]]=E[.70+.002(1100)]=.70+.002(1100)=$ `r .70+.002*(1100)`
\item No, we don't know any particular student's GPA given his or her SAT score. The provided formula only allows us to derive an expected GPA given an SAT score.
\end{enumerate}
\item 
\begin{enumerate}
\item $E[X]=1/2(-1)+1/2(1)=0$

$E[X^2]=1/2(-1)^2+1/2(1)^2=1$

\item $E[X]=1/2(1)+1/2(2)=3/2$

$E[1/X]=1/2(1)+1/2(1/2)=3/4$

\item From part(a), $E[X^2]=1\neq(E[X])^2=0^2=0$

From part(b), $E[1/X]=3/4\neq(1/E[X])=2/3$

\item $$E[F]=E\left[\frac{X_1/k_1}{X_2/k_2}\right]$$Because $k_1$ and $k_2$ are constants, $$=\frac{k_2}{k_1}E\left[\frac{X_1}{X_2}\right]$$Using the fact that $X_1$ and $X_2$ are assumed independent $$=\frac{k_2}{k_1}E[X_1]E[X_2^{-1}]$$Using the fact that $X_1$ is a chi-square random variable (and thus has a mean of $k_1$) $$=\frac{k_2}{k_1}k_1E[X_2^{-1}]=k_2E[X_2^{-1}]=E\left[\frac{k_2}{X_2}\right]=E\left[\frac{1}{X_2/k_2}\right]$$As we showed in parts (a-c), $E\left[\frac{1}{X_2/k_2}\right]$ has a nonlinear 'internal' function, and thus, we cannot conclude that $E[F]=1$.
\end{enumerate}

\end{enumerate}

\newpage

# Math Refresher C

## Notes

### Populations, Parameters, and Random Sampling

A \textbf{population} is any well-defined group of subjects, which could be individuals, firms, cities, or many other possibilities.

A \textbf{random sample} is a sample obtained by sampling randomly from the specified population. In particular, no unit is more likely to be selected than any other unit, and each draw is independent of all other draws.

When $\{Y_1,\dots,Y_n\}$ is a random sample from the density $f(y;\theta)$, we also say that the $Y_i$ are \textbf{independent, identically distributed} (or \textbf{i.i.d.}) random variables from $f(y;\theta)$.

### Estimators

An \textbf{estimator} is a rule for combining data to produce a numerical value for a population parameter; the form of the rule does not depend on the particular sample obtained. More generally, an estimator $W$ of a parameter $\theta$ can be expressed as an abstract mathematical formula: $$W=h(Y_1,Y_2,\dots,Y_n)$$for some known function $h$ of the random variables $Y_1,Y_2,\dots,Y_n$

### Unbiasedness
An estimator, $W$ of $\theta$, is an \textbf{unbiased estimator} if $$E[W]=\theta$$for all possible values of $\theta$.

The distribution of an estimator is often called its \textbf{sampling distribution}, because this distribution describes the likelihood of various outcomes of $W$ across different random samples.

If $W$ is an estimator of $\theta$, its \textbf{bias} is defined $$\text{Bias}(W)\equiv E[W]-\theta$$

The \textbf{sample average} is an unbiased estimator of the population variance and is defined as  $$\bar{Y}=\frac{1}{n}\sum_{i=1}^{n}Y_i$$

\underline{Proof of Unbiasedness}

$$E[\bar{Y}]=E\left[\frac{1}{n}\sum_{i=1}^{n}Y_i\right]$$
$$=\frac{1}{n}E\left[\sum_{i=1}^{n}Y_i\right]$$
$$=\frac{1}{n}\sum_{i=1}^{n}E[Y_i]=\frac{1}{n}(n\mu)=\mu$$

The \textbf{sample variance} is an unbiased estimator of the population variance and is defined as  $$S^2=\frac{1}{n-1}\sum_{i=1}^{n}(Y_i-\bar{Y})^2$$

\underline{Proof of Unbiasedness}
$$E[S^2]=E\left[\frac{1}{n-1}\sum_{i=1}^{n}(Y_i-\bar{Y})^2\right]$$
$$=\frac{1}{n-1}E\left[\sum_{i=1}^{n}Y_i^2-2\sum_{i=1}^{n}Y_i\bar{Y}+\sum_{i=1}^{n}\bar{Y}^2\right]$$
$$=\frac{1}{n-1}E\left[\sum_{i=1}^{n}Y_i^2-2\bar{Y}\sum_{i=1}^{n}Y_i+\bar{Y}\sum_{i=1}^{n}\bar{Y}\right]$$
$$=\frac{1}{n-1}E\left[\sum_{i=1}^{n}Y_i^2-2n\bar{Y}^2+n\bar{Y}^2\right]$$
$$=\frac{1}{n-1}\Bigg\{\sum_{i=1}^{n}E[Y_i^2]-nE[\bar{Y}^2]\Bigg\}$$
Using the facts that $\text{Var}(\bar{Y})=\frac{\sigma_Y^2}{n}$ and $\sigma_Y^2=E[Y_i^2]-E[Y_i]^2$,
$$=\frac{1}{n-1}\Bigg\{\sum_{i=1}^{n}\left(\sigma_Y^2+E[Y_i]^2\right)-n\left(\frac{\sigma_Y^2}{n}+E[\bar{Y}]^2\right)\Bigg\}$$
$$=\frac{1}{n-1}\Bigg\{n\sigma_Y^2+n\mu_Y^2-\sigma_Y^2-n\mu_Y^2\Bigg\}$$
$$=\frac{1}{n-1}\left[(n-1)\sigma_Y^2\right]=\sigma_Y^2$$

The \textbf{sample covariance} is defined as $$S_{XY}=\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar{X})(Y_i-\bar{Y})$$ and is an unbiased and consistent estimator of $\sigma_{XY}$

The \textbf{sample correlation coefficient} is defined as $$R_{XY}=\frac{S_{XY}}{S_XS_Y}=\frac{\sum_{i=1}^{n}(X_i-\bar{X})(Y_i-\bar{Y})}{\sqrt{\sum_{i=1}^{n}(X_i-\bar{X})^2}\sqrt{\sum_{i=1}^{n}(Y_i-\bar{Y})^2}}$$ and is a consistent but biased estimator of $\rho_{XY}$. Because $S_{XY}$, $S_X$, and $S_Y$ are consistent for the corresponding population parameter, $R_{XY}$ is a consistent estimator of the population correlation, $\rho_{XY}$. However, $R_{XY}$ is a biased estimator for two reasons. First, $S_X$ and $S_Y$ are biased estimators of $\sigma_X$ and $\sigma_Y$, respectively. Second, $R_{XY}$ is a ratio of estimators, so it would not be unbiased, even if $S_X$ and $S_Y$ were.

### Efficiency

The variance of an estimator is often called its \textbf{sampling variance} because it is the variance associated with a sampling distribution. The sampling variance is not a random variable; it is a constant, but it might be unknown.

An estimator, $W_1$, is \textbf{efficient} relative to another estimator, $W_2$, when $\text{Var}(W_1)\leq\text{Var}(W_2)$ for all $\theta$, with strict inequality for at least one value of $\theta$.

One way to compare estimators that are not necessarily unbiased is to compute the \textbf{mean squared error (MSE)} of the estimators. If $W$ is an estimator of $\theta$, then the MSE of $W$ is defined as
$$\text{MSE}(W)=E[(W-\theta)^2]$$
The MSE measures how far, on average, the estimator is away from $\theta$. Since
$$\text{MSE}(W)=E[(W-\theta)^2]$$
$$=E\left[W^2-2\theta W+\theta^2\right]$$
$$=E[W^2]-2\theta E[W]+\theta^2$$
$$=\left(E[W^2]-E[W]^2\right)+\left(E[W]^2-2\theta E[W]+\theta^2\right)$$
$$=\text{Var}(W)+\left(E[W]-\theta\right)^2$$
$$=\text{Var}(W)+\left[\text{Bias}(W)\right]^2$$

$\text{MSE}(W)$ depends on the variance and bias (if any is present). This allows us to compare two estimators when one or both are biased.

### Consistency

Let $W_n$ be an estimator of $\theta$ based on a sample $Y_1,Y_2,\dots,Y_n$ of size $n$. Then, $W_n$ is a \textbf{consistent estimator} of $\theta$ if for every $\epsilon > 0$, $$P(|W_n-\theta|>\epsilon)\to 0 \text{ as }n\to\infty$$When $W_n$ is consistent, we also say that $\theta$ is the \text{probability limit} of $W_n$, written as $\text{plim}(W_n)=\theta$. Unlike unbiasedness—which is a feature of an estimator for a given sample size—consistency involves the behavior of the sampling distribution of the estimator as the sample size $n$ gets large.

\textsl{\underline{Asymptotic Unbiasedness $\leftarrow$ Consistency + Bounded Variance}}

Consider an estimator $W_n$ for a parameter $\theta$. Asymptotic unbiasedness means that the bias of the estimator goes to zero as $n\to\infty$, which means that the expected value of the estimator converges to the true value of the parameter. Consistency is a stronger condition than this; it requires the estimator (not just its expected value) to converge to the true value of the parameter (with convergence interpreted in various ways). Since there is generally some non-zero variance in the estimator, it will not generally be equal to (or converge to) its expected value. Assuming the variance of the estimator is bounded, consistency ensures asymptotic unbiasedness, but asymptotic unbiasedness is not enough to get consistency. To put it another way, under some mild conditions, asymptotic unbiasedness is a necessary but not sufficient condition for consistency.

\textsl{\underline{Asymptotic Unbiasedness + Vanishing Variance $\rightarrow$ Consistency}}

If you have an asymptotically unbiased estimator, and its variance converges to zero, this is sufficient to give weak consistency. (This follows from Markov's inequality, which ensures that convergence in mean-square implies convergence in probability). Intuitively, this reflects the fact that a vanishing variance means that the sequence of random variables is converging closer and closer to the expected value, and if the expected value converges to the true parameter (as it does under asymptotic unbiasedness), then the random variable is converging to the true parameter.

More simply, unbiased estimators are not necessarily consistent, but those whose variances shrink to zero as the sample size grows are \emph{consistent}. For example, the sample variance and standard deviation formulas without \textbf{Bessel's correction} are biased estimators; however, they are also consistent because the converge in probability toward their population values as $n\to\infty$.

The \textbf{law of large numbers (LLN)} is a theorem that states the average from a random sample converges in probability to the population average. It also holds for stationary and weakly dependent time series. This result comes from the fact that $\text{Var}(\bar{Y})=\frac{\sigma_Y}{n}$ (for pairwise uncorrelated $Y$s), which approaches $0$ as $n\to\infty$.

\underline{Plim Properties}
\begin{enumerate}
\item If $\theta$ is a parameter and $\gamma=g(\theta)$ is a newly-defined parameter for some continuous function $g(\theta)$. If $\text{plim}(W_n)=\theta$, then the estimator of $\gamma, G_n=g(W_n),$ has a plim defined by$$\text{plim}(G_n)=\gamma$$This is often stated as $$\text{plim}(g(W_n))=g\left(\text{plim}(W_n)\right)$$
\item If $\text{plim}(T_n)=\alpha$ and $\text{plim}(U_n)=\beta$, then
\begin{enumerate}
\item $\text{plim}(T_n+U_n)=\alpha+\beta$
\item $\text{plim}(T_nU_n)=\alpha\beta$
\item $\text{plim}(T_n/U_n)=\alpha/\beta$, if $\beta\neq 0$
\end{enumerate}
\end{enumerate}

### Asymptotic Normality

Let $\{Z_n:n=1,2,\dots\}$ be a sequence of random variables, such that for numbers $z$, $$P(Z_n\leq z)\to\Phi(z)\text{ as }n\to\infty$$ where $\Phi(z)$ is the standard normal cumulative distribution function. Then, $Z_n$ is said to have an \textbf{asymptotic standard normal distribution}. This is sometimes written as $Z_n\overset{\text{a}}{\sim}\mathcal{N}(0,1)$, where the "$a$" stands for “asymptotically” or “approximately.”

The \textbf{central limit theorem (CLT)} states that the average from a random sample (and many other estimators that depend on the sample mean) for any population (with finite variance), when standardized, has an asymptotic standard normal distribution. More formally, for a random sample $\{Y_1,Y_2,\dots,Y_n\}$ with a mean $\mu$ and a variance $\sigma^2$. Then, $$Z_n=\frac{\bar{Y}-\mu}{\sigma/\sqrt{n}}$$ has an asymptotic standard normal distribution. Note that $Z_n$ is the standardized version of $\bar{Y_n}$: $E[\bar{Y_n}]=\mu$ has been subtracted off and divided by $\text{sd}(\bar{Y_n})=\sigma/\sqrt{n}$.

### Method of Moments Estimators

One way of estimating a parameter is using \textbf{method of moments} estimation. Generally, method of moments estimation proceeds as follows. The parameter $\theta$ is shown to be related to some expected value in the distribution of $Y$, usually $E[Y]$ or $E[Y^2]$ (sometimes other choices are used). If, for example, the parameter of interest, $\theta$, is related to the population mean as $\theta=g(\mu)$ for some function $g$, since $\bar{Y}$ is an unbiased and consistent estimator of $\mu$, method of moments estimators replace $\mu$ with $\bar{Y}$. This gives us a consistent estimator $g(\bar{Y})$ of $\theta$, which is also unbiased if $g(\mu)$ is a linear function of $\mu$. Ultimately, all that's done is replace the population moment, $\mu$, with its sample counterpart, $\bar{Y}$.

### Maximum Likelihood Estimators

The \textbf{maximum likelihood estimator} of $\theta$, call it $W$, is the value of $\theta$ that maximizes the \textbf{likelihood function} $$L(\theta;Y_1,Y_2,\dots,Y_n)=f(Y_1;\theta)f(Y_2;\theta)\dots f(Y_n;\theta)$$which equals $P(Y_1=y_1,Y_2=y_2,\dots,Y_n=y_n)$ in the discrete case. Usually, it is more convenient to work with the \textbf{log-likelihood function}, which is obtained by taking the natural log of the likelihood function: $$\mathscr{L}(\theta)=\ln\left(L(\theta;Y_1,Y_2,\dots,Y_n)\right)=\sum_{i=1}^{n}\ln[f(Y_i,\theta)]=\sum_{i=1}^{n}\ell(\theta;X_i)$$where we use the fact that the log of the product is the sum of the logs.

### Least Squares Estimators

A \text{least squares estimator} is an estimator of a parameter that minimizes the sum of squared differences. That is, an estimator, $W$ is a least squares estimator if it minimizes $$\sum_{i=1}^{n}(W-\theta)^2$$for all estimators of $\theta$. It should be noted that the principles of least squares, method of moments, and maximum likelihood often result in the same estimator. In other cases, the estimators are similar but not identical.

### Confidence Intervals
A \textbf{condidence interval} is a rule used to construct a random interval so that a certain percentage of all data sets, determined by the confidence level, yields an interval that contains the population value. Thus, a 95% confidence interval of an estimator will contain the true population value 95% of the time. Theoretically, for the sample average, the 95% confidence interval can be constructed as follows: $$P\left(-1.96<\frac{\bar{Y}-\mu}{\sigma/\sqrt{n}}<1.96\right)=.95$$
$$\to CI_{95}=[\bar{y}-1.96(\sigma/\sqrt{n}),\bar{y}+1.96(\sigma/\sqrt{n})]$$where $\mu$ is the hypothesized population mean. In practice, however, $\sigma$ is unknown and must be estimated with $s$. Unfortunately, this does not preserve the 95% level of confidence because $s$ depends on the particular sample. In other words, the random interval $[\bar{Y}\pm 1.96(S/\sqrt{n})]$ no longer contains $\mu$ with probability .95 because the constant $\sigma$ has been replaced with the random variable $S$. Thus, rather than using the standard normal distribution, we must rely on the $t$ distribution. The $t$ distribution arises from the fact that $$\frac{\bar{Y}-\mu}{S/\sqrt{n}}\sim t_{n-1}$$The denominator $S/\sqrt{n}$ is an estimate of the $\text{sd}(\bar{Y})$. In general, these estimators of \textbf{sampling standard deviations} are referred to as \textbf{standard errors}.

### Hypothesis Testing

A \textbf{Type I error} is an error in which a true null hypothesis is rejected.

A \textbf{Type II error} is an error in which one fails to reject a false null hypothesis.

A \textbf{significance level} is the probability of committing a Type I error, which is generally denoted $$\alpha=P(\text{Reject }H_0|H_0)$$

A \textbf{$p$-value} is the \emph{largest} significance level at which we could carry out a test and still fail to reject the null hypothesis. Formally,$$p-\text{value}=P(T>t|H_0)=1-\Phi(t)$$where $\Phi(\cdot)$ is the standard normal cdf.

```{r Math Refresher C Values, include=FALSE,echo=FALSE}
#Ex4
y <- c(165.76,96.32,76.08,185.35,116.43,162.08,152.04,161.75,92.88,149.94,64.75,127.07,133.55,77.70,206.39,108.33,118.17)
x <- c(374,209,253,432,367,361,288,369,206,316,145,355,295,223,459,290,307)
y_over_x <- y/x
W_1 <- sum(y_over_x)/17
W_2 <- mean(y)/mean(x)
#Ex7
wage_before <- c(8.30,9.40,9.00,10.50,11.40,8.75,10.00,9.50,10.80,12.55,12.00,8.65,7.75,11.25,12.65)
wage_after <- c(9.25,9.00,9.25,10.00,12.00,9.50,10.25,9.50,11.50,13.10,11.50,9.00,7.75,11.50,13.00)
wage_dif <- wage_after-wage_before
```


## Exercises
\begin{enumerate}
\item 
\begin{enumerate}
\item $$E[\bar{Y}]=E\left[\frac{1}{4}(Y_1+Y_2+Y_3+Y_4)\right]=\frac{1}{4}\big\{E[Y_1]+E[Y_2]+E[Y_3]+E[Y_4]\big\}=\frac{1}{4}(4\mu)=\mu$$

$$\text{Var}(\bar{Y})=\text{Var}\left(\frac{1}{4}(Y_1+Y_2+Y_3+Y_4)\right)=\frac{1}{16}\sum_{i=1}^{4}Var(Y_i)=\frac{1}{16}\sum_{i=1}^{4}\sigma^2=\frac{\sigma^2}{4}$$
\item $$E[W]=E\left[\frac{1}{8}Y_1+\frac{1}{8}Y_2+\frac{1}{4}Y_3+\frac{1}{2}Y_4\right]$$
$$=\frac{1}{8}E[Y_1]+\frac{1}{8}E[Y_2]+\frac{1}{4}E[Y_3]+\frac{1}{2}E[Y_4]$$
$$=\frac{1}{8}\mu+\frac{1}{8}\mu+\frac{1}{4}\mu+\frac{1}{2}\mu=\mu$$

$$\text{Var}(W)=\text{Var}\left(\frac{1}{8}Y_1+\frac{1}{8}Y_2+\frac{1}{4}Y_3+\frac{1}{2}Y_4\right)$$
$$=\frac{1}{64}\text{Var}(Y_1)+\frac{1}{64}\text{Var}(Y_2)+\frac{1}{16}\text{Var}(Y_3)+\frac{1}{4}\text{Var}(Y_4)=\frac{11}{32}\sigma^2$$

\item I prefer $\bar{Y}$ over $W$ because $\bar{Y}$ is a more efficient unbiased estimator.

\textit{Note:} Parts (a) and (b) make use of the fact that the sample is i.i.d., and thus, the variance of the sums of the variables is equal to the sum of the variances of the variables.
\end{enumerate}
\item 
\begin{enumerate}
\item $\sum_{i=1}^{n}a_i=1$
\item $$\text{Var}(W_a)=\text{Var}\left(\sum_{i=1}^{n}a_iY_i\right)=\sum_{i=1}^{n}\text{Var}(a_iY_i)=a_1^2\sigma^2+\dots+a_n^2\sigma^2=\sigma^2\sum_{i=1}^{n}a_i^2$$

$$\min_{a_1,\dots,a_n}\text{Var}(W_a)=a_1^2\sigma^2+\dots+a_n^2\sigma^2\text{ s.t. }a_1+\dots+a_n=1$$
$$\mathscr{L}=a_1^2\sigma^2+\dots+a_n^2\sigma^2+\lambda(1-a_1-\dots-a_n)$$
$$\frac{\partial\mathscr{L}}{\partial a_1}=2a_1\sigma^2-\lambda=0$$
$$\dots$$
$$\frac{\partial\mathscr{L}}{\partial a_n}=2a_n\sigma^2-\lambda=0$$
$$\frac{\partial\mathscr{L}}{\partial \lambda}=1-a_1-\dots-a_n=0$$
$$\rightarrow a_1=\dots=a_n\text{ and } \sum_{i=1}^{n}a_i=1$$
$$\rightarrow \sum_{i=1}^{n}a_1=1$$
$$\rightarrow a_1^{*}=\dots=a_n^{*}=\frac{1}{n}$$
\end{enumerate}
\item 
\begin{enumerate}
\item $E[W_1]=E\left[\frac{n-1}{n}(\bar{Y})\right]=\frac{n-1}{n}E[\bar{Y}]=\frac{n-1}{n}(\mu)\neq \mu$

$\text{Bias}(W_1)=E[W_1]-\mu=\frac{n-1}{n}(\mu)-\mu=\frac{-\mu}{n}$

$E[W_2]=E\left[\frac{\bar{Y}}{2}\right]=\frac{1}{2}E[\bar{Y}]=\frac{\mu}{2}$

$\text{Bias}(W_2)=E[W_2]-\mu=\frac{\mu}{2}-\mu=\frac{-\mu}{2}$

One important difference is that the bias of $W_1$ converges to 0 as $n\to\infty$, while the bias of $W_2$ is constant.
\item $\text{plim}(W_1)=\text{plim}\left(\frac{n-1}{n}(\bar{Y})\right)=\text{plim}\left(\frac{n-1}{n}\right)\text{plim}(\bar{Y})=1\cdot\mu=\mu$

$\text{plim}(W_2)=\text{plim}\left(\frac{\bar{Y}}{2}\right)=\text{plim}(\bar{Y})/\text{plim}(2)=\frac{\mu}{2}$

Of the two estimators, only $W_1$ is consistent.

\item $$\text{Var}(W_1)=\text{Var}\left(\frac{n-1}{n}(\bar{Y})\right)=\left(\frac{n-1}{n}\right)^2\text{Var}(\bar{Y})=\left(\frac{(n-1)^2\sigma^2}{n^3}\right)$$
$$\text{Var}(W_2)=\text{Var}\left(\frac{\bar{Y}}{2}\right)=\frac{1}{4}\text{Var}(\bar{Y})=\frac{\sigma^2}{4n}$$

\item While $\bar{Y}$ is unbiased regardless of the value of $\mu$, when $\mu$ is "close" to zero, the bias of $W_1$ is also close to zero (especially for large samples). Thus, it may be worthwhile to consider $W_1$ over $\bar{Y}$ if $W_1$ is efficient relative to $\bar{Y}$. We know $\text{Var}(\bar{Y})=\frac{\sigma^2}{n}$ and $\text{Var}(W_1)=\left(\frac{(n-1)^2\sigma^2}{n^3}\right)$. Using these calculations to evaluate when $W_1$ has a smaller variance than $\bar{Y}$
$$\left(\frac{(n-1)^2\sigma^2}{n^3}\right)\leq\frac{\sigma^2}{n}$$
$$\left(\frac{n-1}{n}\right)^2\leq 1$$
which holds for all $n\in\mathbb{Z}^+$. Thus, $W_1$ has a smaller variance than $\bar{Y}$ and, given the small amount of bias, it may be a better estimator of $\mu$.
\end{enumerate}
\item 
\begin{enumerate}
\item $E[Z]=E\left[E[Z|X]\right]=E\left[E\left[\frac{Y}{X}|X\right]\right]=E\left[\frac{1}{X}E\left[Y|X\right]\right]=E\left[\frac{1}{X}\theta X\right]=E[\theta]=\theta$
\item $E[W_1]=E\left[n^{-1}\sum_{i=1}^{n}(Y_i/X_i)\right]=n^{-1}\sum_{i=1}^{n}E\left[Y_i/X_i\right]=n^{-1}\sum_{i=1}^{n}\theta=n^{-1}(n\theta)=\theta$
\item In general, the average of the ratios, $Y_i/X_i$, is not the ratio of the averages $\bar{Y}/\bar{X}$. However, $$E\left[\frac{\bar{Y}}{\bar{X}}|X_1,\dots,X_2\right]=\frac{1}{\bar{X}}E[\bar{Y}|X_1,\dots,X_n]$$
$$=\frac{1}{\bar{X}}E\left[n^{-1}\sum_{i=1}^{n}Y_i|X_1,\dots,X_n\right]$$
$$=\frac{1}{n\bar{X}}\sum_{i=1}^{n}E[Y_i|X_1,\dots,X_n]$$
$$=\frac{1}{n\bar{X}}\left(n\theta \bar{X}\right)=\theta$$
\item $W_1=n^{-1}\sum_{i=1}^{n}(Y_i/X_i)=$ `r W_1`

$W_2=\frac{\bar{Y}}{\bar{X}}=$ `r W_2`

Yes, they are similar.
\end{enumerate}
\item 
\begin{enumerate}
\item $G$ is not an unbiased estimator of $\gamma$ because $G$ has a nonlinear relationship with $\bar{Y}$. As we concluded in Math Refresher B, the expected value of the ratio is not the ratio of the expected value.
\item $\text{plim}(G)=\text{plim}\left(\frac{\bar{Y}}{1-\bar{Y}}\right)=\text{plim}(\bar{Y})/\text{plim}(1-\bar{Y})=\theta/(\text{plim}(1)-\text{plim}(\bar{Y}))=\frac{\theta}{1-\theta}=\gamma$
\end{enumerate}
\item 
\begin{enumerate}
\item $$H_0:\mu=0$$
\item $$H_1:\mu<0$$
\item $t=\frac{\bar{y}-\mu}{s/\sqrt{n}}=\frac{-32.8-0}{466.4/\sqrt{900}}\approx$ `r -32.8/(466.4/30)`

$p\approx$ `r pt(-32.8/(466.4/30),899)`

We reject the null hypothesis at the 5\% level but fail to reject $H_0$ at the 1\% level.

\item We've already shown there is a statistically significant difference at the 5\% level but not at the 1\% level. On the other hand, I would struggle to argue there is a practical significance when there is only a 32.8 ounce difference in alcohol consumption over an entire year.

\item This analysis implicitly assumes all other factors that affect liquor consumption have remained the same. Factors such as such as income and changes in prices of factors of production are assumed constant over the two years.
\end{enumerate}
\item 
\begin{enumerate}
\item $CI_{95}=[$ `r mean(wage_dif)-qt(.025,length(wage_dif)-1,lower.tail = F)*sd(wage_dif)/sqrt(length(wage_dif))`,`r mean(wage_dif)+qt(.025,length(wage_dif)-1,lower.tail = F)*sd(wage_dif)/sqrt(length(wage_dif))`$]$
\item $$H_0:\mu=0$$
$$H_1:\mu>0$$
\item $t=\frac{\bar{d}-0}{s_d/\sqrt{n}}=$ `r (mean(wage_dif))/(sd(wage_dif)/sqrt(length(wage_dif)))`

For a one-sided test, we would reject $H_0$ at the 5\% level but fail to reject $H_0$ at the 1\% level.
\item $p=$ `r pt((mean(wage_dif))/(sd(wage_dif)/sqrt(length(wage_dif))), length(wage_dif)-1,lower.tail=F)`
\end{enumerate}
\item 
\begin{enumerate}
\item $\bar{Y}=\frac{188}{429}\approx$ `r 188/429`
\item $$\text{sd}(\bar{Y})=\frac{\sigma_\theta}{\sqrt{n}}=\sqrt{\frac{\theta(1-\theta)}{n}}$$
\item $t=\frac{\bar{Y}-.5}{\text{se}(\bar{Y})}=\frac{\bar{Y}-.5}{\sqrt{\bar{Y}(1-\bar{Y})/n}}=$ `r (188/429 - .5) / (sqrt(188/429 * (1-188/429)/429))`

$p=$ `r pt((188/429 - .5) / (sqrt(188/429 * (1-188/429)/429)),428)`

Thus, we reject $H_0$ at the 1\% level.
\end{enumerate}
\item 
\begin{enumerate}
\item $E[X]=200(.65)=130$
\item $\text{sd}(X)=\sqrt{|200|\times.65(1-.65)}=$ `r sqrt(200*.65*.35)`
\item $t=\frac{(115/200)-.65}{\sqrt{(.65)(.35)/200}}$=`r (115/200 - .65)/(sqrt(.65*.35/200))`

$p=$ `r pt((115/200 - .65)/(sqrt(.65*.35/200)),199)`

\item The value calculated in part(c) is a $p$-value which is the probability of rejecting a true null hypothesis. In the previous part, we would reject the dictator's claim at the 5% level but fail to reject the claim at the 1% level.
\end{enumerate}
\item $CI_{95}=\left[.394-1.96\sqrt{(.394)(1-.394)/419},.394+1.96\sqrt{(.394)(1-.394)/419}\right]=[$ `r .394-1.96*sqrt(.394*(1-.394)/419)`,`r .394+1.96*sqrt(.394*(1-.394)/419)`$]$

Based on his average up to the strike, there is not very strong evidence against $\theta = .400$, as this value is well within the 95\% confidence interval.

\item $t=\frac{\bar{y}-0}{s/\sqrt{n}}=\frac{.132-0}{1.27/20}\approx$ `r .132 / (1.27/20)`

The difference is statistically greater than zero at the 5\% level but not at the 1\% level.
\end{enumerate}

\newpage

# Chapter 1

## Notes

### Data Types

\textbf{Nonexperimental data} are not accumulated through controlled experiments on individuals, firms, or segments of the economy. Non-experimental data are sometimes called \textbf{observational data}, or \textbf{retrospective data}, to emphasize the fact that the researcher is a passive collector of the data.

\textbf{Experimental data} are often collected in laboratory environments in the natural sciences, but they are more difficult to obtain in the social sciences. Although some social experiments can be devised, it is often impossible, prohibitively expensive, or immoral to conduct the kinds of controlled experiments that would be needed to address economic issues.

An \textbf{empirical analysis} uses data to test a theory or to estimate a relationship.

A \textbf{cross-sectional data set} consists of a sample of individuals, households, firms, cities, states, countries, or a variety of other units, taken at a given point in time. An important feature of cross-sectional data is that we can often assume that they have been obtained by \textbf{random sampling} from the underlying population. Sometimes, however, the random sampling assumption is not appropriate for a variety of reasons such as respondents' willingness to answer or sampling from units that are large relative to the population. Nevertheless, random sampling is often assumed with cross-sectional data. The analysis of cross-sectional data is closely aligned with the applied microeconomics fields, such as labor economics, public finance, industrial organization, urban economics, demography, and health economics.

A \textbf{time series data set} consists of observations on a variable or several variables over time. Unlike the arrangement of cross-sectional data, the chronological ordering of observations in a time series conveys potentially important information. A key feature of time series data that makes them more difficult to analyze than cross-sectional data is that economic observations can rarely, if ever, be assumed to be independent across time.

A \textbf{pooled cross section} is a data configuration where independent cross sections, usually collected at different points in time, are combined to produce a single data set.

A \textbf{panel data} (or \textbf{longitudinal data}) \textbf{set} consists of a time series for each cross-sectional member in the data set. The key feature of panel data that distinguishes them from a pooled cross section is that the \emph{same} cross-sectional units are followed over a given time period. Time series and panel data differ in that time series data focus on a single individual at multiple time intervals while panel data focus on multiple individuals at multiple time intervals.

### Causality

In econometrics, we're often concerned about finding a \textbf{causaul effect} or \textbf{ceteris paribus} (meaning all other relevant factors are held fixed) change in one variable that has an effect on another variable.

## Exercises

### Problems

\begin{enumerate}
\item 
\begin{enumerate}
\item I would randomly assign (that is without other factors that affect student performance in mind) fourth grade students to varying class sizes and compare students' performances across the various groups.
\item I might expect a negative correlation between class size and test score because generally, larger classes have less funding per student and students in larger classes receive less individualized instruction. There are many additional factors positively correlated with student performance and negatively correlated with class size.
\item No, causality can only be established when ceteris-paribus is satisfied; however, this is not the case as some of the confounding factors that wouldn't be controlled for are listed in part(b).
\end{enumerate}
\item 
\begin{enumerate}
\item All else equal, do job training programs improve worker productivity?
\item No. For one, perhaps a firm that requires job training because it has less-skilled workers and wants to increase its production efficiency. There are many other factors like this on the firm side that make me believe that a firm’s decision to train its workers will be independent of worker characteristics. Additionally, perhaps the firm does not require but offers job training. It's likely individuals who actually do the job training have different characteristics to those who do not. Some factors may include innate ability, intelligence, and motivation.
\item The quality of the equipment.
\item No, causality can only be established when ceteris-paribus is satisfied; however, this is not the case as some of the confounding factors that wouldn't be controlled for (such as the quality of the equipment).
\end{enumerate}
\item No. Again, ceteris-paribus has not been established. Many other confounding factors correlated with "work" and "study" would not be controlled.
\item 
\begin{enumerate}
\item Ideally, panel data that contains corporate tax rates and GSP.
\item Theoretically, it'd be possible to do a controlled experiment, but it would not be ethical. It would require randomly assigning individuals to varying levels of corporate tax rates and measuring the GSP within these groups.
\item If other factors impacting GSP growth and tax rates are sufficiently controlled for, then yes. Otherwise, such correlational analysis will likely be biased and not convincing.
\end{enumerate}
\end{enumerate}

### Computer Exercises

C1)
```{r Chapter 1 Computer Exercises C1, include=TRUE,echo=TRUE}
#i
mean(wage1$educ)
min(wage1$educ)
max(wage1$educ)

#ii
mean(wage1$wage)
#It seems low for the year 2013

#iii
cpi_1976 <- 55.6
cpi_2013 <- 230.280

#iv
mean(wage1$wage)*cpi_2013/cpi_1976
#Yes, the average wage now seems more reasonable

#v
sum(wage1$female)
sum(wage1$female==0)

rm(cpi_1976,cpi_2008)
gc()
```

C2)
```{r Chapter 1 Computer Exercises C2, include=TRUE,echo=TRUE}
#i
#number of women
sum(bwght$male==0)
#number of women who report smoking during pregnancy
sum(bwght$male==0 & bwght$cigs>0)

#ii and iii
mean(bwght$cigs)
#No, this average includes males, a more descriptive average may be:
mean(bwght$cigs[bwght$male==0])
#for all women and
mean(bwght$cigs[bwght$male==0 & bwght$cigs>0])
#for those that reported smoking

#iv
mean(bwght$fatheduc, na.rm = T)
#There are only 1192 observations because there are 196 missing values for fatheduc

#v
mean(bwght$faminc) * 1000
sd(bwght$faminc) * 1000
```

C3)
```{r Chapter 1 Computer Exercises C3, include=TRUE,echo=TRUE}
#i
min(meap01$math4)
max(meap01$math4)
#Without knowing much about the data, the range seems to make sense covering all 100% of pass rates

#ii
sum(meap01$math4 == 100)

#iii
sum(meap01$math4 == 50)

#iv
mean(meap01$math4)
mean(meap01$read4)
#The reading test seems harder to pass

#v
cor(meap01$math4,meap01$read4)
#Those with higher pass rates on one exam tend to have higher pass rates on the other
#In other words, pass rates on the math and reading tests are highly correlated

#vi
mean(meap01$exppp)
sd(meap01$exppp)

#vii
#actual
500*100/5500
#natural log approximation
100 * (log(6000)-log(5500))
```

C4)
```{r Chapter 1 Computer Exercises C4, include=TRUE,echo=TRUE}
#i
mean(jtrain2$train)

#ii
mean(jtrain2$re78[jtrain2$train==1])
mean(jtrain2$re78[jtrain2$train==0])
#The difference does appear economically large

#iii
mean(jtrain2$unem78[jtrain2$train==1])
mean(jtrain2$unem78[jtrain2$train==0])
#The proportion of unemployed who are not receiving training is about 11% larger

#Yes, the training seems effective, establishing ceteris paribus would make the results more convincing
```

C5)
```{r Chapter 1 Computer Exercises C5, echo=TRUE,include=TRUE}
#i
min(fertil2$children)
max(fertil2$children)
mean(fertil2$children)

#ii
sum(fertil2$children>0) / length(fertil2$children)

#iii
#for those who have electricity
mean(fertil2$children[fertil2$electric==1 & !is.na(fertil2$electric)])
#for those who do not have electricity
mean(fertil2$children[fertil2$electric==0 & !is.na(fertil2$electric)])
#Those without electricity have more children on average (for this sample)

#iv
#No, the ceteris paribus condition is not established
```

C6)
```{r Chapter 1 Computer Exercises C6, echo=TRUE,include=TRUE}
county_murders <- as_tibble(countymurders) %>%
  filter(year == 1996)

#i
n_distinct(county_murders$countyid)
#number with 0 murders
n_distinct(county_murders$countyid[county_murders$murders==0])
#percent with 0 murders
100 * n_distinct(county_murders$countyid[county_murders$murders==0]) / n_distinct(county_murders$countyid)

#ii
max(county_murders$murders)
max(county_murders$execs)
mean(county_murders$execs)

#iii
cor(county_murders$murders,county_murders$execs)

#iv
#No, I would suspect the positive correlation may be due to executions resulting from murders
#and other crimes, which I suspect is correlated with the number of murders
rm(county_murders)
gc()
```

C7)
```{r Chapter 1 Computer Exercises C7, echo=TRUE,include=TRUE}
#i
#percent who report abusing alcohol
100 * mean(alcohol$abuse)
#employment rate
100 * mean(alcohol$employ)

#ii
100 * mean(alcohol$employ[alcohol$abuse==1])

#iii
100 * mean(alcohol$employ[alcohol$abuse==0])

#iv
#No, the ceteris paribus condition is not established
```

C8)
```{r Chapter 1 Computer Exercises C8, echo=TRUE,include=TRUE}
#i
NROW(econmath)

#ii
#for those who took econ in high school
mean(econmath$score[econmath$econhs==1])
#for those who did not take econ in high school
mean(econmath$score[econmath$econhs==0])

#iii
#No, it simply tells us the average scores for those who did and did not take econ in hs
#It may signify some level of correlation but not causality

#iv
#Randomly assigning individuals to two groups (one who does take econ in high school
#and one that does not), then performing part(ii) can be used to obtain a good causal estimate
```

\newpage

# Chapter 2

## Notes

A \textbf{simple linear regression model} is a model relating a dependent variable to one independent variable and generally takes the form$$y=\beta_0+\beta_1x+u$$It is also called the \textbf{two-variable linear regression model} or \textbf{bivariate linear regression model} because it relates the two variables $x$ and $y$.

The variable $u$, which is called the \textbf{error term} or \textbf{disturbance}, represents factors other than $x$ that affect $y$.

If the other factors in $u$ are held fixed, so that the change in $u$ is zero, $\Delta u = 0$, then $x$ has a linear effect on $y$:$$\Delta y=\beta_1\Delta x\text{ if }\Delta u =0$$

As long as the intercept $\beta_0$ is included in the equation, nothing is lost by assuming that the average value of $u$ in the population is zero. Mathematically, $$E[u]=0$$

One of the most important assumptions in econometrics is the \textbf{zero conditional mean assumption}. This crucial assumption says that the average value of the unobservables ($u$) is the same across all slices of the population determined by the value of $x$ and that the common average \emph{is} necessarily equal to the average of $u$ over the entire population. Thus, under this assumption (and the zero unconditional mean assumption), we write$$E[u|x]=E[u]=0$$

Using the zero conditional mean assumption and taking the expected value of $y$ given $x$ from the simple linear regression model gives$$E[y|x]=\beta_0+\beta_1 x,$$This is called the \textbf{population regression function (PRF)} and shows that $E[y|x]$ is a linear function of $x$. The PRF gives us a relationship between the average level of $y$ at different levels of $x$. It is important to understand that the PRF tells us how the average value of $y$ changes with $x$; it does not say that $y$ equals $\beta_0 + \beta_1 x$ for all units in the population.

### OLS Estimators Derivation (SLR)

Let $\{(x_i,y_i):i=1,\dots,n\}$ denote a random sample of size $n$ from the population. Then, we can write$$y_i=\beta_0+\beta_1 x_i+u_i$$for each $i$ where $u_i$ is the error term for each observation $i$ because it contains all factors affecting $y_i$ other than $x_i$. Because there are two unknown parameters to estimate $(\beta_0,\beta_1)$, we might hope to obtain good estimators, which we will denote $\hat{\beta}_0$ and $\hat{\beta}_1$. Thus, we may want to find estimators that minimize the sum of squared residuals$$SSR=\sum_{i=1}^{n}(y_i-\hat{y}_i)^2,$$ where $\hat{y}_i$ represents fitted values from an estimated model using $\hat{\beta}_0$ and $\hat{\beta}_1$. Formally, $$\hat{y}_i=\hat{\beta}_0+\hat{\beta}_1 x_i$$Hence, we can rewrite $$SSR=\sum_{i=1}^{n}(y_i-\hat{\beta}_0-\hat{\beta}_1 x_i)^2$$

To minimize the sum of squared residuals, we take two first order conditions:

\underline{FOC 1:}

$$\frac{\partial SSR}{\partial\hat{\beta}_0}=\sum_{i=1}^{n}-2(y_i-\hat{\beta}_0-\hat{\beta}_1 x_i)=0$$
$$\to \sum_{i=1}^{n}(y_i-\hat{\beta}_0-\hat{\beta}_1 x_i)=0$$
$$\to n\bar{y}-n\hat{\beta}_0-n\hat{\beta}_1\bar{x}=0$$
$$\to \bar{y}=\hat{\beta}_0+\hat{\beta}_1\bar{x}$$
$$\to \hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}$$
\small
\textit{Note:} The significance of this FOC is that, using least squares criteria, our line of best fit runs through the sample means $\bar{y}$ and $\bar{x}$.
\normalsize

\underline{FOC 2:}

$$\frac{\partial SSR}{\partial\hat{\beta}_1}=\sum_{i=1}^{n}-2x_i(y_i-\hat{\beta}_0-\hat{\beta}_1 x_i)=0$$
$$\to\sum_{i=1}^{n}x_i(y_i-\hat{\beta}_0-\hat{\beta}_1 x_i)=0$$
$$\to\sum_{i=1}^{n}(x_iy_i-\hat{\beta}_0x_i-\hat{\beta}_1 x_i^2)=0$$
$$\to\sum_{i=1}^{n}x_iy_i=\sum_{i=1}^{n}\left(\hat{\beta}_0x_i+\hat{\beta}_1 x_i^2\right)$$
$$\to\sum_{i=1}^{n}x_iy_i=\sum_{i=1}^{n}\left((\bar{y}-\hat{\beta}_1\bar{x})x_i+\hat{\beta}_1 x_i^2\right)$$
$$\to\sum_{i=1}^{n}x_iy_i=\sum_{i=1}^{n}\left(\bar{y}x_i-\hat{\beta}_1\bar{x}x_i+\hat{\beta}_1 x_i^2\right)$$
$$\to\sum_{i=1}^{n}x_iy_i=n\bar{x}\bar{y}+\hat{\beta_1}\sum_{i=1}^{n}(x_i^2-\bar{x}x_i)$$
$$\to\sum_{i=1}^{n}(x_iy_i)-n\bar{x}\bar{y}=\hat{\beta}_1\sum_{i=1}^{n}x_i(x_i-\bar{x})$$
$$\to\sum_{i=1}^{n}(x_iy_i-\bar{x}y_i)=\hat{\beta}_1\sum_{i=1}^{n}x_i(x_i-\bar{x})$$
$$\to\hat{\beta}_1=\frac{\sum_{i=1}^{n}y_i(x_i-\bar{x})}{\sum_{i=1}^{n}x_i(x_i-\bar{x})}=\frac{\sum_{i=1}^{n}(y_i-\bar{y})(x_i-\bar{x})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}=\frac{\hat{\sigma}_{xy}}{\hat{\sigma}^2_{x}}=\hat{\rho}_{xy}\cdot\frac{\hat{\sigma}_y}{\hat{\sigma}_x}$$

Note that the FOCs are analogous to the zero mean assumptions (using residuals instead of the error term):

FOC 1 can be restated as
$$E[\hat{u}]=0\to E[y-\hat{y}]=E[y-\hat{\beta}_0-\hat{\beta}_1x]=n^{-1}\sum_{i=1}^{n}(y-\hat{\beta}_0-\hat{\beta}_1x)=0$$

And FOC 2 can be restated as
$$E[\hat{u}|x]=E[\hat{u}]\to \text{Cov}(x,\hat{u})=0\to E[x\hat{u}]-E[x]E[\hat{u}]=E[x\hat{u}]=0$$
$$\to n^{-1}\sum_{i=1}^{n}x_i(y_i-\hat{\beta}_0-\hat{\beta}_1x_i)=0$$

### OLS Regression

The estimators $(\hat{\beta}_0,\hat{\beta}_1)$ are called the \textbf{ordinary least squares (OLS)} estimators of $\beta_0$ and $\beta_1$. A \textbf{fitted value} for $y$ when $x=x_i$ is defined as $$\hat{y}_i=\hat{\beta}_0+\hat{\beta}_1 x_i$$The \textbf{residual} for observation $i$ is the difference between the actual $y_i$ and its fitted value:$$\hat{u}_i=y_i-\hat{y}_i=y_i-\hat{\beta}_0-\hat{\beta}_1 x_i$$Thus, we define  the \textbf{OLS regression line} or \textbf{sample regression function (SRF)} as$$\hat{y}=\hat{\beta}_0+\hat{\beta}_1 x$$

\underline{Properties of OLS Statistics}
\begin{enumerate}
\item The sum, and therefore the sample average of the OLS residuals, is zero, such that$$\sum_{i=1}^{n}\hat{u}_i=0$$This result follows from FOC 1.
\item The sample covariance between the regressors and the OLS residuals is zero. This follows from FOC2, which can be written in terms of the residuals as$$\sum_{i=1}^{n}x_i\hat{u}_i=0$$
\item The point $(\bar{x},\bar{y})$ is always on the OLS regression line. In other words, if we take the OLS regression line and plug in $\bar{x}$ for $x$, then the predicted value is $\bar{y}$. This is exactly what the derivation from FOC 1 showed us.
\end{enumerate}

### Sum of Squares

We can view OLS as decomposing each $y_i$ into two parts, a fitted value and a residual. The fitted values and residuals are uncorrelated in the sample.

The \textbf{total sum of squares (SST)} can be defined as$$SST=\sum_{i=1}^{n}(y_i-\bar{y})^2$$and is a measure of the variation in the $y_i$ of the given sample. If we divided SST by $n-1$, we obtain the sample variance of $y$.

The \textbf{explained sum of squares (SSE)} can be defined as$$SSE=\sum_{i=1}^{n}(\hat{y}_i-\bar{y})^2$$and is a measure of the variation in the $\hat{y}_i$.

The \textbf{residual sum of squares (SSR)} can be defined as$$SSR=\sum_{i=1}^{n}(y_i-\hat{y})^2=\sum_{i=1}^{n}\hat{u}^2$$and measures the sample variation in the $\hat{u}_i$.

In total,$$SST=SSE+SSR$$

\underline{Proof}
$$SSE+SSR=\sum_{i=1}^{n}(\hat{y}_i-\bar{y})^2+\sum_{i=1}^{n}(y_i-\hat{y}_i)^2$$
$$=\sum_{i=1}^{n}(\hat{y}_i^2-2\bar{y}\hat{y}_i+\bar{y}^2)+\sum_{i=1}^{n}(y_i^2-2y_i\hat{y}_i+\hat{y}_i^2)$$
$$=\left[\sum_{i=1}^{n}\hat{y}_i^2-2\bar{y}\sum_{i=1}^{n}\hat{y}_i+\sum_{i=1}^{n}\bar{y}^2\right]+\left[\sum_{i=1}^{n}y_i^2-2\sum_{i=1}^{n}y_i\hat{y}_i+\sum_{i=1}^{n}\hat{y}_i^2\right]$$
$$=\left[\sum_{i=1}^{n}y_i^2-2n\bar{y}^2+\sum_{i=1}^{n}\bar{y}^2\right]+\left[\sum_{i=1}^{n}\hat{y}_i^2-2\sum_{i=1}^{n}y_i\hat{y}_i+\sum_{i=1}^{n}\hat{y}_i^2\right]$$
$$=\left[\sum_{i=1}^{n}y_i^2-2\bar{y}\sum_{i=1}^{n}y_i+\sum_{i=1}^{n}\bar{y}^2\right]+\left[\sum_{i=1}^{n}(2\hat{y}_i^2-2y_i\hat{y}_i)\right]$$
$$=\left[\sum_{i=1}^{n}(y_i^2-2\bar{y}y_i+\bar{y}^2)\right]+\left[\sum_{i=1}^{n}2\hat{y}_i(\hat{y}_i-y_i)\right]$$
$$=\sum_{i=1}^{n}(y_i-\bar{y})^2-2\sum_{i=1}^{n}\hat{y}_i(\hat{u}_i)$$
$$=SST-2\sum_{i=1}^{n}\hat{u}_i(\hat{\beta}_0+\hat{\beta_1}x_i)$$
$$=SST-2\left[\hat{\beta}_0\sum_{i=1}^{n}\hat{u}_i+\hat{\beta_1}\sum_{i=1}^{n}x_iu_i\right]=SST$$

The \textbf{R-squared} of the regression, sometimes called the \textbf{coefficient of determination}, is defined as$$R^2\equiv\frac{SSE}{SST}=1-\frac{SSR}{SST}$$
$R^2$ is the ratio of the explained variation compared to the total variation; thus, it is interpreted as the fraction of the sample variation in $y$ that is explained by $x$. $R^2$ is equal to the square of the sample correlation coefficient between $y_i$ and $\hat{y}_i$:$$R^2=\frac{\left[\sum_{i=1}^{n}\left(y_i-\bar{y}\right)\left(\hat{y}_i-\bar{\hat{y}}\right)\right]^2}{\sum_{i=1}^{n}\left(y_i-\bar{y}\right)^2\sum_{i=1}^{n}\left(\hat{y}_i-\bar{\hat{y}}\right)^2}$$An important fact about $R^2$ is that it never decreases, and it usually increases, when another independent variable is added to a regression and the same set of observations is used for both regressions. The fact that $R^2$ never decreases when any variable is added to a regression makes it a poor tool for deciding whether particular variables should be added to a model; instead, one should focus on whether an explanatory variable has a nonzero partial effect on $y$ in the \emph{population} to decide whether to include an explanatory variable in a model. 


### Nonlinear Models

In a linear regression model, changing the units of the variables has a multiplicative effect on the slope and intercepts but has not effect on the $R^2$.

In a nonlinear regression model that incorporates natural logarithms, changing the units has no effect on the slope but does change the intercept. For example, take $\ln(y)=\beta_0+\beta_1\ln(x)+u$. If we multiply $y$ by some constant $c_1$ and $x$ by some constant $c_2$, the equation becomes
$$\ln(c_1y)=\beta_0+\beta_1\ln(c_2 x)+u$$
$$\to\ln(c_1)+\ln(y)=\beta_0+\beta_1\left[\ln(c_2)+\ln(x)\right]+u$$
$$\to\ln(y)=\alpha+\beta_1\ln(x)+u$$where $\alpha=\beta_0-\ln(c_1)+\beta_1\ln(c_2)$.

\underline{Summary of Functional Forms Involving Natural Logarithms}
\begin{tabular}{|lccc|}
\hline
Model & Dependent Variable & Independent Variable & Interpretation of $\beta_1$\\
\hline
Level-level (linear model) & $y$ & $x$ & $\Delta y=\beta_1 \Delta x$\\
Level-log & $y$ & $\ln(x)$ & $\Delta y=(\beta_1/100)\%\Delta x$\\
Log-level (semi-elasticity model) & $\ln(y)$ & $x$ & $\%\Delta y=(100\beta_1)\Delta x$\\
Log-log (constant elasticity model) & $\ln(y)$ & $\ln(x)$ & $\%\Delta y=\beta_1\%\Delta x$\\
\hline
\end{tabular}

### Gauss-Markov Assumptions

\begin{itemize}
\item Assumption 1: The population model is linear in parameters such that $$y_i=\alpha +\beta_1x_{i1}+...+\beta_kx_{ik}+u_i$$
\item Assumption 2: The sample data $\{x_{i1},...x_{ik},y_i\}$ is a random sample.
\item Assumption 3: The error term has a zero conditional mean such that $$E[u|x_{i1},...x_{ik}]=0$$
For a random sample, this assumption implies that $E[u_i|x_{i1},...x_{ik}]=0\ \forall\ i=1,2,\dots,n$. This also means that $\text{Cov}(u,g(x))=0$ for some function of $g$ (see below for proof).
\item Assumption 4: The error term is homoskedastic such that $$\text{Var}(u|x_{i1},...x_{ik})=\sigma_u^2$$
\item Assumption 5: None of the regressors exhibit perfect collinearity with one another.
\item Assumption 6: There exists no serial correlation between the error terms such that $$\text{Cov}(u_i,u_j)=0 \: \forall \: i\neq j$$Note: Assumption 2 is sufficient to satisfy assumption 6 in the case of cross-sectional data (see below for proof).
\end{itemize}

Under this set of assumptions, the \textbf{Gauss-Markov Theorem} states that the OLS estimator is BLUE (conditional on the sample values of the explanatory variable(s)).

\underline{Proof that the Zero Conditional Mean Assumption Implies Zero Correlation between $u$ and functions of $x$}

$$\text{Cov}\left(u,g(x)\right)=E[ug(x)]-E[u]E[g(x)]$$
By the law of iterated expectations,
$$=E[E[ug(x)|x]]-0\cdot E[g(x)]$$
$$=E[g(x)E[u|x]]-0$$
$$=E[g(x)\cdot 0]=0$$

\underline{Proof that Random Sampling Implies Zero Serial Correlation with Cross-Sectional Data}

Under assumption 2, we know the sample data is independent and identically distributed (i.i.d), meaning drawing one observation does not make drawing another observation any more or less likely and that the observations come from the same distribution. Thus, for some observation $i\neq j$,
$$\text{Cov}(y_i,y_j)=E[y_i y_j]-E[y_i]E[y_j]=E[y_i]E[y_j]-E[y_i]E[y_j]=0$$
Thus, replacing $y$ with the equation in our first assumption
$$0=\text{Cov}(y_i,y_j)=\text{Cov}(\beta_0+\beta_1x_{i1}+...+\beta_kx_{ik}+u_i,\beta_0+\beta_1x_{j1}+...+\beta_kx_{jk}+u_j)$$
$$=\sum_{s=1}^k \sum_{t=1}^k \beta_s \beta_t \text{Cov}(x_{is},x_{jt}) + \sum_{s=1}^k \beta_s \text{Cov}(x_{is},u_j) + \sum_{t=1}^k \beta_t \text{Cov}(x_{jt},u_i) + \text{Cov}(u_i,u_j)$$
Using the fact that the sample is i.i.d., $$\sum_{s=1}^k \sum_{t=1}^k \beta_s \beta_t \text{Cov}(x_{is},x_{jt})=0$$ Also, by the exogeneity assumption, $$\sum_{s=1}^{k} \beta_s \text{Cov}(x_{is},u_j) + \sum_{t=1}^k \beta_t \text{Cov}(x_{jt},u_i)=0$$ Leaving us with $$\text{Cov}(u_i,u_j)=0$$

### Unbiasedness of OLS Estimators (SLR)

$$E\left[\hat{\beta}_{1}\middle|x\right]=E\left[\frac{\sum_{i=1}^{n}y_i(x_i-\bar{x})}{\sum_{i=1}^{n}x_i(x_i-\bar{x})}\middle|x\right]$$
$$=E\left[\frac{\sum_{i=1}^{n}(\beta_0+\beta_1 x_i + u_i)(x_i-\bar{x})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}\middle|x\right]$$
$$=E\left[\frac{\beta_0\sum_{i=1}^{n}(x_i-\bar{x})+\beta_1\sum_{i=1}^{n}x_i(x_i-\bar{x}) +\sum_{i=1}^{n} u_i(x_i-\bar{x})}{SST_x}\middle|x\right]$$
$$=E\left[\frac{\beta_0(n\bar{x}-n\bar{x})+\beta_1\sum_{i=1}^{n}(x_i-\bar{x})^2 +\sum_{i=1}^{n} u_i(x_i-\bar{x})}{SST_x}\middle|x\right]$$
$$=E\left[\beta_1 + \frac{\sum_{i=1}^{n} u_i(x_i-\bar{x})}{SST_x}\middle|x\right]$$
$$=\beta_1 + \frac{E\left[\sum_{i=1}^{n} u_i(x_i-\bar{x})\middle|x\right]}{SST_x}$$
$$=\beta_1 + \frac{\sum_{i=1}^{n} E\left[u_i(x_i-\bar{x})\middle|x\right]}{SST_x}$$
$$=\beta_1 + \frac{\sum_{i=1}^{n} (x_i-\bar{x})E\left[u_i\middle|x\right]}{SST_x}=\beta_1$$
where we have used the fact that the expected value of each $u_i$ (conditional on $\{x_1, x_2,\dots, x_n\}$) is zero under the second and third Gauss-Markov assumptions. Because unbiasedness holds for any outcome on $\{x_1, x_2,\dots, x_n\}$, unbiasedness also holds without conditioning on $\{x_1, x_2,\dots, x_n\}$.

$$E\left[\hat{\beta}_0\middle|x\right]=E\left[\bar{y}-\hat{\beta}_1\bar{x}\middle|x\right]$$
$$=E\left[\bar{y}\middle|x\right]-E\left[\hat{\beta}_1\bar{x}\middle|x\right]$$
$$=E\left[n^{-1}\sum_{i=1}^{n}(\beta_0+\beta_1x_i+u_i)\middle|x\right]-\bar{x}E\left[\hat{\beta}_1\middle|x\right]$$
$$=E\left[\beta_0+\beta_1\bar{x}\middle|x\right]-\beta_1\bar{x}$$
$$=\beta_0+\beta_1\bar{x}-\beta_1\bar{x}=\beta_0$$

Unbiasedness generally fails if any of our first three assumptions fail. This means that it is important to think about the veracity of each assumption for a particular application. The first assumption requires that $y$ and $x$ be linearly related, with an additive disturbance. This can certainly fail. But we also know that $y$ and $x$ can be chosen to yield interesting nonlinear relationships. Random sampling can fail in a cross section when samples are not representative of the underlying population; in fact, some data sets are constructed by intentionally oversampling different parts of the population. Finally, the third assumption is possibly the most important of the Gauss-Markov assumptions. Using simple regression when $u$ contains factors affecting $y$ that are also correlated with $x$ can result in spurious correlation: that is, we find a relationship between $y$ and $x$ that is really due to other unobserved factors that affect $y$ and also happen to be correlated with $x$. In addition to omitted variables, there are other reasons for $x$ to be correlated with $u$ in the simple regression model.

### Sampling Variance of OLS Estimators (SLR)

Using a previous derivation,
$$\hat{\beta}_1=\beta_1+\frac{\sum_{i=1}^{n}u_i(x_i-\bar{x})}{SST_x}$$

Thus,
$$\text{Var}\left(\hat{\beta}_1\middle|x\right)=\text{Var}\left(\beta_1+\frac{\sum_{i=1}^{n}u_i(x_i-\bar{x})}{SST_x}\middle|x\right)$$
$$=\frac{1}{SST_x^2}\text{Var}\left(\sum_{i=1}^{n}u_i(x_i-\bar{x})\middle|x\right)$$
$$=\frac{1}{SST_x^2}\sum_{i=1}^{n}\text{Var}\left(u_i(x_i-\bar{x})\middle|x\right)$$
$$=\frac{1}{SST_x^2}\sum_{i=1}^{n}(x_i-\bar{x})^2\text{Var}\left(u_i\middle|x\right)$$
$$=\frac{1}{SST_x^2}\sum_{i=1}^{n}(x_i-\bar{x})^2\sigma_u^2$$
$$=\frac{\sigma_u^2}{SST_x^2}\sum_{i=1}^{n}(x_i-\bar{x})^2$$
$$=\frac{\sigma_u^2}{SST_x}$$

Again, using a previous derivation,

$$\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}$$
$$\to\text{Var}\left(\hat{\beta}_0\middle|x\right)=\text{Var}\left(\bar{y}-\hat{\beta}_1\bar{x}\middle|x\right)$$
$$=\text{Var}\left(\bar{y}\middle|x\right)+\text{Var}\left(\hat{\beta}_1\bar{x}\middle|x\right)-2\text{Cov}\left(\bar{y},\hat{\beta}_1\bar{x}\middle|x\right)$$
$$=\text{Var}\left(\beta_0+\beta_1\bar{x}+\bar{u}\middle|x\right)+\bar{x}^2\text{Var}\left(\hat{\beta}_1\middle|x\right)-\frac{2\bar{x}}{n}\text{Cov}\left(\sum_{i=1}^{n}y_i,\hat{\beta}_1\middle|x\right)$$
$$=\text{Var}\left(\beta_1\bar{x}\middle|x\right)+\text{Var}\left(\bar{u}\middle|x\right)+2\text{Cov}\left(\beta_1\bar{x},\bar{u}\middle|x\right)+\bar{x}^2\frac{\sigma_u^2}{SST_x}-\frac{2\bar{x}}{n}\text{Cov}\left(\sum_{i=1}^{n}y_i,\beta_1+\frac{\sum_{i=1}^{n}u_i(x_i-\bar{x})}{SST_x}\middle|x\right)$$
$$=0+\frac{1}{n^2}\text{Var}\left(\sum_{i=1}^{n}u_i\middle|x\right)+2\cdot 0+\frac{\sigma_u^2\bar{x}^2}{SST_x}-\frac{2\bar{x}}{nSST_x}\text{Cov}\left(\sum_{i=1}^{n}y_i,\sum_{i=1}^{n}u_i(x_i-\bar{x})\middle|x\right)$$
$$=\frac{1}{n^2}\sum_{i=1}^{n}\text{Var}\left(u_i\middle|x\right)+\frac{\sigma_u^2\bar{x}^2}{SST_x}-\frac{2\bar{x}\sum_{i=1}^{n}(x_i-\bar{x})}{nSST_x}\text{Cov}\left(\sum_{i=1}^{n}y_i,\sum_{i=1}^{n}u_i\middle|x\right)$$
$$=\frac{1}{n^2}\sum_{i=1}^{n}\sigma_u^2+\frac{\sigma_u^2\bar{x}^2}{SST_x}-\frac{2\bar{x}(0)}{nSST_x}\text{Cov}\left(\sum_{i=1}^{n}y_i,\sum_{i=1}^{n}u_i\middle|x\right)$$
$$=\frac{\sigma_u^2}{n}+\frac{\sigma_u^2\bar{x}^2}{SST_x}$$
$$=\frac{SST_x\sigma_u^2+n\sigma_u^2\bar{x}^2}{nSST_x}$$
$$=\frac{\sigma_u^2(SST_x+n\bar{x}^2)}{nSST_x}$$
$$=\frac{\sigma_u^2(\sum_{i=1}^{n}(x_i-\bar{x})^2+n\bar{x}^2)}{nSST_x}$$
$$=\frac{\sigma_u^2(\sum_{i=1}^{n}(x_i^2+\bar{x}^2-2\bar{x}x_i)+n\bar{x}^2)}{nSST_x}$$
$$=\frac{\sigma_u^2(\sum_{i=1}^{n}(x_i^2)+n\bar{x}^2-2n\bar{x}^2+n\bar{x}^2)}{nSST_x}$$
$$=\frac{\sfrac{\sigma_u^2}{n}\sum_{i=1}^{n}x_i^2}{SST_x}$$
Generally, we are interested in $\text{Var}\left(\hat{\beta}_1\right)$. To summarize how this variance depends on the error variance, $\sigma_u^2$, and the total variation in $\{x_1,x_2,\dots,x_n\}$ $SST_x$: 
\begin{enumerate}
\item The larger the error variance, the larger is $\text{Var}\left(\hat{\beta}_1\right)$. This makes sense because more variation in the unobservables affecting $y$ makes it more difficult to precisely estimate $\beta_1$.
\item More variability in the independent variable is preferred: as the variability in the $x_i$ increases, the variance of $\hat{\beta}_1$ decreases. This also makes intuitive sense because the more spread out the sample of independent variables is, the easier it is to trace out the relationship between $E[y|x]$ and $x$ (i.e., it becomes easier to estimate $\beta_1$). If there is little variation in the $x_i$, then it can be hard to pinpoint how $E[y|x]$ varies with $x$. As the sample size increases, so does the total variation in the $x_i$. Therefore, a larger sample size results in a smaller variance for $\hat{\beta}_1$.
\end{enumerate}

These formulas allow us to isolate the factors that contribute to $\text{Var}\left(\hat{\beta}_0\right)$ and $\text{Var}\left(\hat{\beta}_1\right)$. But these formulas are unknown, except in the extremely rare case that $\sigma_u^2$ is known. Considering the $u_i$ are unobserved, an unbiased estimator of $\sigma_u^2$ is$$\hat{\sigma}_u^2=\frac{\sum_{i=1}^{n}\hat{u}_i^2}{n-2},$$ where the $n-2$ is the degrees of freedom in the OLS residuals due to the two OLS first order conditions:$$\sum_{i=1}^{n}\hat{u}_i=0,\sum_{i=1}^{n}x_i\hat{u}_i=0$$

Naturally, the estimators of $\text{Var}\left(\hat{\beta}_0\right)$ and $\text{Var}\left(\hat{\beta}_1\right)$ become
$$\hat{\sigma}_{\beta_0}^2=\frac{\sfrac{\sum_{i=1}^{n}\hat{u}_i^2}{n(n-2)}+\sum_{i=1}^{n}x_i^2}{SST_x}$$
and
$$\hat{\sigma}_{\beta_1}^2=\frac{\sfrac{\sum_{i=1}^{n}\hat{u}_i^2}{(n-2)}}{SST_x}$$

Additionally, the natural estimator of $\sigma_u$ is$$\hat{\sigma}_u=\sqrt{\hat{\sigma}_u^2},$$which is called the \textbf{standard error of the regresion (SER)}.

Although $\hat{\sigma}_u^2$ is an unbiased and consistent estimator of $\sigma_u^2$, $\hat{\sigma}_u$ is consistent but biased.

All together the \textbf{standard errors of the coefficients} are
$$\text{se}\left(\hat{\beta}_0\right)=\sqrt{\frac{\sfrac{\sum_{i=1}^{n}\hat{u}_i^2}{n(n-2)}+\sum_{i=1}^{n}x_i^2}{SST_x}}$$
and
$$\text{se}\left(\hat{\beta}_1\right)=\sqrt{\frac{\sfrac{\sum_{i=1}^{n}\hat{u}_i^2}{(n-2)}}{SST_x}}$$

\underline{A Note on the Conditioning on x}

In addition to restricting the relationship between $u$ and $x$ in the population, the zero conditional mean assumption—coupled with the random sampling assumption—allows for a convenient technical simplification. In particular, we can derive the statistical properties of the OLS estimators as conditional on the values of the $x_i$ in our sample. Technically, in statistical derivations, conditioning on the sample values of the independent variable is the same as treating the $x_i$ as fixed in repeated samples, which we think of as follows. We first choose $n$ sample values for $\{x_1, x_2,\dots, x_n\}$. Given these values, we then obtain a sample on $y$. Next, another sample of $y$ is obtained, using the same values for $\{x_1, x_2,\dots, x_n\}$. Then another sample of $y$ is obtained, again using the same $\{x_1, x_2,\dots, x_n\}$. And so on. While the fixed-in-repeated-samples scenario is not very realistic in non-experimental contexts, random sampling, where individuals are chosen randomly, is representative of how most data sets are obtained for empirical analysis in social sciences. Once we assume that $E[u|x]=0$, and we have random sampling, nothing is lost in derivations by treating the $x_i$ as nonrandom. The danger is that the fixed-in-repeated-samples assumption always implies that $u_i$ and $x_i$ are independent.

### Regression through the Origin

In some cases, we may wish to impose the restriction that, when $x=0$, $E[y]=0$. In these cases we perform \textbf{regression through the origin}, which takes the form$$\tilde{y}=\tilde{\beta}_1x,$$where the tildes are used to distinguish this problem from the much more common problem of estimating an intercept along with a slope. We still rely on the method of OLS to obtain the slope estimate. Thus, we must solve the for the $\tilde{\beta}_1$ that minimizes$$SSR=\sum_{i=1}^{n}\left(y_i-\tilde{\beta_1}x_i\right)^2$$Taking the partial derivative with respect to $\tilde{\beta}_1$, we obtain$$\frac{\partial SSR}{\partial\tilde{\beta}_1}=-2\sum_{i=1}^{n}x_i\left(y_i-\tilde{\beta}_1x_i\right)=0$$Solving for $\tilde{\beta}_1$:
$$\sum_{i=1}^{n}x_i\left(y_i-\tilde{\beta}_1x_i\right)=0$$
$$\to\sum_{i=1}^{n}x_iy_i-\sum_{i=1}^{n}\tilde{\beta}_1x_i^2=0$$
$$\to\tilde{\beta}_1\sum_{i=1}^{n}x_i^2=\sum_{i=1}^{n}x_iy_i$$
$$\to\tilde{\beta}_1=\frac{\sum_{i=1}^{n}x_iy_i}{\sum_{i=1}^{n}x_i^2}$$
Obtaining an estimate of $\beta_1$ using regression through the origin is not done very often in applied work, and for good reason: if the intercept b0 2 0, then $\tilde{\beta}_1$ is a biased estimator of $\beta_1$. In cases where it is appropriate, the $R$-squared is computed as $$1-\frac{\sum_{i=1}^{n}\left(y_i-\tilde{\beta}_1x_i\right)^2}{\sum_{i=1}^{n}y_i^2}=1-\frac{SSR}{SST}$$

Note: If we only regress on a constant, (i.e., we set the slope to zero and estimate an intercept only), the intercept that minimizes the sum of squared deviations is $\bar{y}$.

### Regression on a Binary Explanatory Variable

Simple regression can also be applied to the case where x is a \textbf{binary variable}, often called a \textbf{dummy variable} in the context of regression analysis. When the explanatory variable is binary, the PRF takes two forms (under the exogeneity assumption):$$E[y|x]=E\left[\beta_0+\beta_1x+u|x\right]=\beta_0+\beta_1x$$

\begin{enumerate}
\item $E[y|x=0]=\beta_0$
\item $E[y|x=1]=\beta_0+\beta_1$
\end{enumerate}

It follows that$$\beta_1=E[y|x=1]-E[y|x=0]$$

In cases where we hope to study the effect of an intervention or new policy, the idea of counterfactuals or potential outcomes are used. Define, the \textbf{control group} as those not subject to the intervention or new policy act and the \textbf{treatment group} as those subject to the intervention. Then, the causal (or treatment) effect of the intervention for unit $i$ is$$te_i=y_i(1)-y_i(0),$$the difference between the two potential outcomes. A noteworthy items about $te_i$is it is not observed for any unit $i$ because it depends on both counterfactuals. We cannot hope to estimate $te_i$ for each unit $i$. Instead, the focus is typically on the \textbf{average treatment effect (ATE)}, also called the \textbf{average causal effect (ACE)}. The ATE is simply the average of the treatment effects across the entire population. We can write the ATE parameter as $$\tau_{ate}=E[te_i]=E[y_i(1)-y_i(0)]=E[y_i(1)]-E[y_i(0)]$$

For each unit $i$ let $x_i$ be the program participation status—a binary variable. Then the observed outcome, $y_i$, can be written as$$y_i=(1-x_i)y_i(0)+x_iy_i(1)=y_i(0)+[y_i(1)-y_i(0)]x_i$$
Imposing a usually unrealistic assumption of a constant treatment effect$$\to y_i=y_i(0)+\tau x_i$$
Rewriting $y_i(0)$ as $\beta_0+u_i$ and $\tau$ as $\beta_1$,$$\to y_i=\beta_0+\tau x_i+u_i$$

If $x_i \indep u_i$, then $\tau$ is the unbiased estimator of the treatment effect. The assumption that $x_i \indep u_i$ is the same as $x_i \indep y_i(0)$. This assumption can be guaranteed only under \textbf{random assignment}, whereby units are assigned to the treatment and control groups using a randomization mechanism that ignores any features of the individual units. Random assignment is the hallmark of a \textbf{randomized controlled trial (RCT)}, which is considered the gold standard for determining whether medical interventions have causal effects.

## Exercises

### Problems

\begin{enumerate}
\item 
\begin{enumerate}
\item Among the many factors contained in $u$ are marriage status, desire to have kids, health, and wealth. It's likely that at least wealth is correlated with level of education.
\item No. Since it's likely $u$ is correlated with $kids$ and $educ$, a simple regression analysis will likely give a biased estimate of $\beta_1$.
\end{enumerate}
\item Rewriting the model as $y=\beta_0+\beta_1x+u+\alpha_0-\alpha_0$, define a new error term $e=u-\alpha_0$ and a new intercept $\gamma_0=\alpha_0+\beta_0$. The model becomes $y=\gamma_0+\beta_1x+e$. It follows that $E[e]=E[u-\alpha_0]=E[u]-E[\alpha_0]=\alpha_0-\alpha_0-0$.
\item 
\end{enumerate}
```{r Chapter 2 Exercise 3, include=TRUE,comment=NA,warning=FALSE,echo=FALSE}
GPA <- c(2.8,3.4,3.0,3.5,3.6,3.0,2.7,3.7)
ACT <- c(21,24,26,27,29,25,25,30)
lm1 <- lm(GPA ~ ACT)
cat("a) ")
summary(lm1)
cat("The slope tells us there is a positive correlation between GPA and ACT. The intercept gives")
cat("us an estimate of GPA if a student scores a 0 on the ACT. GPA is predicted to be")
cat(lm1$coefficients[2]*5, "higher if the ACT score increases by five points.")

cat("b) ")
cat("Fitted values = (", paste(as.character(round(lm1$fitted.values,4)), collapse=", "),")")
cat("Residuals = (", paste(as.character(round(lm1$residuals, 4)), collapse=", "),")")
cat("Sum of residuals = ", sum(lm1$residuals))

cat("c) ")
cat("Predicted value of GPA when ACT=20: ", lm1$coefficients[1]+lm1$coefficients[2]*20)

cat("d) ")
cat("The coefficient of determination (R-squared)=", unname(unlist(summary(lm1)["r.squared"])))

rm(lm1,GPA,ACT)
invisible(gc())
```

\begin{enumerate}
\setcounter{enumi}{3}
\item 
\begin{enumerate}
\item The predicted birth weight when $cigs = 0$ is 119.77 ounces. When $cigs=20$, the predicted birth weight is `r 119.77-0.514*20` ounces. This regression analysis suggests a constant loss to birth weight for every additional cigarette smoked. In this case, we would an `r 0.514*20` ounce loss in birth weight for each addition 20 cigarettes smoked.
\item No. Other factors correlated with $cigs$ and $bwght$ are not controlled for in the model. Such factors may include the health of the mother and genetics. This leads to a biased slope estimate.
\item Based on this model, $cigs$ must equal `r (125-119.77)/-0.514` for a birth weight of 125 ounces. Obviously, this doesn't have much meaning because you can't smoke a negative number of cigarettes.
\end{enumerate}
\item 
\begin{enumerate}
\item The intercept predicts a -124.84 dollar consumption value for a family with no annual income. The slope predicts an extra 0.853 dollars of consumption for each additional dollar of annual income. The intercept offers little meaning because you can't consume a negative amount. On the other hand, the slope coefficient signifies a positive correlation between consumption and annual income. Personally, the slope coefficient seems to large to me at first glance.
\item The predicted consumption when family income is \$30,000 = \$ `r -124.84 + 0.853*30000`.
\item 
\end{enumerate}
\end{enumerate}
```{r Chapter 2 Exercise 5 Graph, echo=FALSE,include=TRUE,fig.align='right',fig.height=3}
inc <- seq(0, 200000, 5000)
cons <- -124.84 + 0.853 * inc
mpc <- 0.853
apc <- cons / inc

ggplot()+
  geom_line(aes(x = inc, y = mpc, color = "green"))+
  geom_line(aes(x = inc, y = apc, color = "red"))+
  scale_color_identity(name = "Propensity to Consume",
                          breaks = c("green", "red"),
                          labels = c("MPC", "APC"),
                          guide = "legend")+
  theme_classic()+
  xlab("Annual Income")+
  ylab("")
```

\begin{enumerate}
\setcounter{enumi}{5}
\item 
\begin{enumerate}
\item The slope is an estimated elasticity of $price$ with respect to $dist$. The estimate indicates a 0.312 percentage point increase in $price$ for every percent increase in $dist$. The sign is as we'd expect. As houses move further away from the garbage incinerator, we'd expect housing prices to increase.
\item No. As the problem suggests, it's likely the city put the incinerator close to cheaper homes. Thus, the elasticity estimate is likely upwardly biased and overstates the impact of $dist$ on $price$.
\item Other factors include the size of the home, the number of rooms, the quality of the surrounding area (with regards to safety, education, etc.), and the location. As suggested in part(c), these factors are likely correlated with $dist$. 
\end{enumerate}
\item 
\begin{enumerate}
\item $E\left[u\middle|inc\right]=E\left[\sqrt{inc}\cdot e\middle|inc\right]=\sqrt{inc}\cdot E\left[e\middle|inc\right]=\sqrt{inc}\cdot E\left[e\right]=0$. Recall that $E\left[e\middle|inc\right]=E\left[e\right]$ because we are assuming that $e$ is independent of $inc$.
\item $\text{Var}\left(u\middle|inc\right)=\text{Var}\left(\sqrt{inc}\cdot e\middle|inc\right)=\left(\sqrt{inc}\right)^2\text{Var}\left(e\middle|inc\right)=inc\text{Var}\left(e\right)=\sigma_e^2inc$. Recall that $\text{Var}\left(e\middle|inc\right)=\text{Var}\left(e\right)$ because we are assuming that $e$ is independent of $inc$.
\item This notion makes sense because the greater the amount of income, the more options individuals have with that income. Some individuals may choose to spend their income while others will choose to save it. On the other hand, lower earning individuals have fewer options on how to spend their money. It's likely they have to spend a larger portion of their income on necessary consumption items and have a smaller amounts they can save.
\end{enumerate}
\item 
\begin{enumerate}
\item From the notes,$$\tilde{\beta}_1=\frac{\sum_{i=1}^{n}x_iy_i}{\sum_{i=1}^{n}x_i^2}$$
$$\to E\left[\tilde{\beta}_1\middle|x\right]=E\left[\frac{\sum_{i=1}^{n}x_iy_i}{\sum_{i=1}^{n}x_i^2}\middle|x\right]$$
$$=\frac{1}{\sum_{i=1}^{n}x_i^2}E\left[\sum_{i=1}^{n}x_iy_i\middle|x\right]$$
$$=\frac{1}{\sum_{i=1}^{n}x_i^2}\sum_{i=1}^{n}E\left[x_iy_i\middle|x\right]$$
$$=\frac{1}{\sum_{i=1}^{n}x_i^2}\sum_{i=1}^{n}x_iE\left[\beta_0+\beta_1x_i+u_i\middle|x\right]$$
$$=\frac{1}{\sum_{i=1}^{n}x_i^2}\sum_{i=1}^{n}\left(\beta_0 x_i+\beta_1x_i^2\right)$$
$$=\frac{1}{\sum_{i=1}^{n}x_i^2}\left(n\beta_0\bar{x}+\sum_{i=1}^{n}\beta_1x_i^2\right)$$
$$=\frac{n\beta_0\bar{x}+\beta_1\sum_{i=1}^{n}x_i^2}{\sum_{i=1}^{n}x_i^2}$$
Thus,$$E\left[\tilde{\beta}_1\middle|x,\beta_0=0\right]=\frac{\beta_1\sum_{i=1}^{n}x_i^2}{\sum_{i=1}^{n}x_i^2}=\beta_1$$
Another case in which $\tilde{\beta}_1$ is unbiased is when $\bar{x}=\sum_{i=1}^{n}x_i=0$.
\item 
$$\text{Var}\left(\tilde{\beta}_1\middle|x\right)=\text{Var}\left(\frac{\sum_{i=1}^{n}x_iy_i}{\sum_{i=1}^{n}x_i^2}\middle|x\right)$$
$$=\frac{1}{\left(\sum_{i=1}^{n}x_i^2\right)^2}\text{Var}\left(\sum_{i=1}^{n}x_i\left(\beta_0+\beta_1x_i+u_i\right)\middle|x\right)$$
$$=\frac{1}{\left(\sum_{i=1}^{n}x_i^2\right)^2}\text{Var}\left(\sum_{i=1}^{n}\left(\beta_0x_i+\beta_1x_i^2+x_iu_i\right)\middle|x\right)$$
$$=\frac{1}{\left(\sum_{i=1}^{n}x_i^2\right)^2}\sum_{i=1}^{n}\text{Var}\left(\beta_0x_i+\beta_1x_i^2+x_iu_i\middle|x\right)$$
$$=\frac{1}{\left(\sum_{i=1}^{n}x_i^2\right)^2}\sum_{i=1}^{n}x_i^2\text{Var}\left(u_i\middle|x\right)$$
$$=\frac{\sigma_u^2}{\left(\sum_{i=1}^{n}x_i^2\right)^2}\sum_{i=1}^{n}x_i^2$$
$$=\frac{\sigma_u^2}{\sum_{i=1}^{n}x_i^2}$$
\item $$\text{Var}\left(\tilde{\beta}_1\right)\leq\text{Var}\left(\hat{\beta}_1\right)$$
$$\to \frac{\sigma_u^2}{\sum_{i=1}^{n}x_i^2}\leq\frac{\sigma_u^2}{\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2}$$
$$\to \frac{1}{\sum_{i=1}^{n}x_i^2}\leq\frac{1}{\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2}$$
$$\to \sum_{i=1}^{n}x_i^2\geq\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2,$$which is given to us by the hint.
\item Under the Gauss-Markov assumptions, $\hat{\beta}_1$ is always unbiased while $\tilde{\beta}_1$ requires the additional constraint that $y=0$ when $x=0$ to be unbiased. As we just proved, $\tilde{\beta}_1$ is efficient relative to $\hat{\beta}_1$. Thus, in the case that both estimators are unbiased, $\tilde{\beta}_1$ is preferred.
\end{enumerate}
\item 
\begin{enumerate}
\item As the problem suggests, using our previous derivation of $\hat{\beta}_1$,
$$\tilde{\beta}_1=\frac{\sum_{i=1}^{n}\left((c_2x_i-c_2\bar{x})c_1y_i\right)}{\sum_{i=1}^{n}\left(c_2x_i-c_2\bar{x}\right)^2}=\frac{c_1c_2\sum_{i=1}^{n}\left((x_i-\bar{x})y_i\right)}{c_2^2\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2}=\frac{c_1}{c_2}\hat{\beta}_1$$

We've previously derived $\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}$. Plugging in the new values and solving for $\tilde{\beta}_0$,$$\tilde{\beta}_0=c_1\bar{y}-\frac{c_1}{c_2}\hat{\beta}_1(c_2\bar{x})=c_1\left(\bar{y}-\hat{\beta}_1\bar{x}\right)=c_1\hat{\beta}_0$$
\item Again using our previous derivation of $\hat{\beta}_1$,
$$\tilde{\beta}_1=\frac{\sum_{i=1}^{n}\left(((c_2+x_i)-(c_2+\bar{x}))(c_1+y_i)\right)}{\sum_{i=1}^{n}\left((c_2+x_i)-(c_2+\bar{x})\right)^2}=\frac{\sum_{i=1}^{n}\left((x_i-\bar{x})(c_1+y_i)\right)}{\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2}=\frac{c_1\sum_{i=1}^{n}(x_i-\bar{x})+ \sum_{i=1}^{n}(x_i-\bar{x})y_i}{\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2}=\frac{\sum_{i=1}^{n}(x_i-\bar{x})y_i}{\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2}=\hat{\beta}_1$$

Using $\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}$ and plugging in the new values, $$\tilde{\beta}_0=(c_1+\bar{y})-\hat{\beta}_1(c_2+\bar{x})=\left(\bar{y}-\hat{\beta}_1\bar{x}\right)+c_1-c_2\hat{\beta}_1=\hat{\beta}_0+c_1-c_2\hat{\beta}_1$$
\item Changing the model from $\ln(y_i)=\hat{\beta_0}+\hat{\beta_1}x_i$ to $\ln(c_1y_i)=\tilde{\beta_0}+\tilde{\beta_1}x_i$, the new model can be rewritten as$$\ln(c_1)+\ln(y_i)=\tilde{\beta_0}+\tilde{\beta_1}x_i$$Trivially, by subtracting $\ln(c_1)$ from both sides, we see $\tilde{\beta}_0=\hat{\beta}_0+\ln(c_1)$ and $\tilde{\beta}_1=\hat{\beta}_1$.
\item Changing the model from $y_i=\hat{\beta_0}+\hat{\beta_1}\ln(x_i)$ to $y_i=\tilde{\beta_0}+\tilde{\beta_1}\ln(c_2x_i)$, the new model can be rewritten as$$y_i=\tilde{\beta_0}+\tilde{\beta_1}\left(\ln(c_2)+\ln(x_i)\right)$$Again, it's trivial to see that $\tilde{\beta}_0=\hat{\beta}_0-\tilde{\beta}_1\ln(c_2)=\hat{\beta}_0-\hat{\beta}_1\ln(c_2)$ and $\tilde{\beta}_1=\hat{\beta}_1$.
\end{enumerate}
\item 
\begin{enumerate}
\item $$\hat{\beta}_1=\frac{\sum_{i=1}^{n}(x_i-\bar{x})y_i}{\sum_{i=1}^{n}(x_i-\bar{x})^2}$$
$$=\frac{1}{SST_x}\sum_{i=1}^{n}(x_i-\bar{x})(\beta_0+\beta_1x_i+u_i)$$
$$=\frac{1}{SST_x}\left[\beta_0\sum_{i=1}^{n}(x_i-\bar{x})+\beta_1\sum_{i=1}^{n}x_i(x_i-\bar{x})+\sum_{i=1}^{n}u_i(x_i-\bar{x})\right]$$
$$=\frac{1}{SST_x}\left[\beta_0\left(n\bar{x}-n\bar{x}\right)+\beta_1\sum_{i=1}^{n}(x_i-\bar{x})^2+\sum_{i=1}^{n}u_i(x_i-\bar{x})\right]$$
$$=\beta_1+\frac{1}{SST_x}\sum_{i=1}^{n}u_i(x_i-\bar{x})$$
$$=\beta_1+\sum_{i=1}^{n}\frac{d_iu_i}{SST_x}$$
$$=\beta_1+\sum_{i=1}^{n}w_iu_i$$
\item $$E\left[\left(\hat{\beta}_1-\beta_1\right)\cdot\bar{u}\middle|x\right]=E\left[\left(\beta_1+\sum_{i=1}^{n}w_iu_i-\beta_1\right)\cdot\bar{u}\middle|x\right]$$
$$=E\left[\sum_{i=1}^{n}\bar{u}w_iu_i\middle|x\right]$$
$$=\sum_{i=1}^{n}E\left[\bar{u}w_iu_i\middle|x\right]$$
$$=\sum_{i=1}^{n}w_iE\left[\bar{u}u_i\middle|x\right]$$
$$=\sum_{i=1}^{n}w_iE\left[\bar{u}u_i\middle|x\right]$$
Because $u_i$ and $u_j$ are pairwise uncorrelated for all $i\neq j$, $E[u_iu_j]=E[u_i]E[u_j]=0\cdot0=0$. Thus, $E\left[\bar{u}u_i\middle|x\right]=\frac{1}{n}E\left[u_i^2|x\right]=\frac{1}{n}\bigg\{E\left[u_i^2|x\right]-E\left[u_i|x\right]^2\bigg\}=\frac{1}{n}\bigg\{\text{Var}(u_i|x)\bigg\}=\frac{\sigma_u^2}{n}$. Hence,
$$\sum_{i=1}^{n}w_iE\left[\bar{u}u_i\middle|x\right]=\frac{\sigma_u^2}{n}\sum_{i=1}^{n}w_i=\frac{\sigma_u^2}{n}\cdot 0=0$$
\item $$\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}=\beta_0+\beta_1\bar{x}+\bar{u}-\hat{\beta}_1\bar{x}=\beta_0+\bar{u}-\left(\hat{\beta}_1-\beta_1\right)\bar{x}$$
\item $$\text{Var}\left(\hat{\beta}_0\middle|x\right)=\text{Var}\left(\beta_0+\bar{u}-\left(\hat{\beta}_1-\beta_1\right)\bar{x}\middle|x\right)$$
$$=\text{Var}\left(\bar{u}-\hat{\beta}_1\bar{x}\middle|x\right)$$
$$=\text{Var}\left(\bar{u}\middle|x\right)+\bar{x}^2\text{Var}\left(\hat{\beta}_1\middle|x\right)-2\bar{x}\text{Cov}\left(\bar{u},\hat{\beta}_1\middle|x\right)$$
$$=\text{Var}\left(\frac{1}{n}\sum_{i=1}^{n}u_i\middle|x\right)+\bar{x}^2\text{Var}\left(\beta_1+\frac{\sum_{i=1}^{n}u_i(x_i-\bar{x})}{SST_x}\middle|x\right)-2\bar{x}\text{Cov}\left(\bar{u},\beta_1+\frac{\sum_{i=1}^{n}u_i(x_i-\bar{x})}{SST_x}\middle|x\right)$$
$$=\frac{1}{n^2}\text{Var}\left(\sum_{i=1}^{n}u_i\middle|x\right)+\frac{\bar{x}^2}{SST_x^2}\text{Var}\left(\sum_{i=1}^{n}u_i(x_i-\bar{x})\middle|x\right)-\frac{2\bar{x}}{SST_x}\text{Cov}\left(\frac{1}{n}\sum_{i=1}^{n}u_i,\sum_{i=1}^{n}u_i(x_i-\bar{x})\middle|x\right)$$
$$=\frac{1}{n^2}\sum_{i=1}^{n}\text{Var}\left(u_i\middle|x\right)+\frac{\bar{x}^2}{SST_x^2}\sum_{i=1}^{n}\text{Var}\left(u_i(x_i-\bar{x})\middle|x\right)-\frac{2\bar{x}\sum_{i=1}^{n}(x_i-\bar{x})}{nSST_x}\text{Cov}\left(\sum_{i=1}^{n}u_i,\sum_{i=1}^{n}u_i\middle|x\right)$$
$$=\frac{1}{n^2}\sum_{i=1}^{n}\sigma_u^2+\frac{\bar{x}^2}{SST_x^2}\sum_{i=1}^{n}(x_i-\bar{x})^2\text{Var}\left(u_i\middle|x\right)-\frac{2\bar{x}(n\bar{x}-n\bar{x})}{nSST_x}\text{Cov}\left(\sum_{i=1}^{n}u_i,\sum_{i=1}^{n}u_i\middle|x\right)$$
$$=\frac{\sigma_u^2}{n}+\frac{\sigma_u^2\bar{x}^2}{SST_x^2}\sum_{i=1}^{n}(x_i-\bar{x})^2-0$$
$$=\frac{\sigma_u^2}{n}+\frac{\sigma_u^2\bar{x}^2}{SST_x}$$
\item $$\frac{\sigma_u^2}{n}+\frac{\sigma_u^2\bar{x}^2}{SST_x}=\frac{\sigma_u^2SST_x}{nSST_x}+\frac{n\sigma_u^2\bar{x}^2}{nSST_x}$$
$$=\frac{\sigma_u^2SST_x+n\sigma_u^2\bar{x}^2}{nSST_x}$$
$$=\frac{\sum_{i=1}^{n}\left(\sigma_u^2\left(x_i-\bar{x}\right)^2\right)+n\sigma_u^2\bar{x}^2}{nSST_x}$$
$$=\frac{\sum_{i=1}^{n}\left(\sigma_u^2x_i^2-2\sigma_u^2x_i\bar{x}+\sigma_u^2\bar{x}^2\right)+n\sigma_u^2\bar{x}^2}{nSST_x}$$
$$=\frac{\sigma_u^2\sum_{i=1}^{n}x_i^2-2\sigma_u^2\bar{x}\sum_{i=1}^{n}x_i+\sigma_u^2\sum_{i=1}^{n}\bar{x}^2+n\sigma_u^2\bar{x}^2}{nSST_x}$$
$$=\frac{\sigma_u^2\sum_{i=1}^{n}x_i^2-2n\sigma_u^2\bar{x}^2+n\sigma_u^2\bar{x}^2+n\sigma_u^2\bar{x}^2}{nSST_x}$$
$$=\frac{\sigma_u^2n^{-1}\sum_{i=1}^{n}x_i^2}{\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2}$$
\end{enumerate}
\item
\begin{enumerate}
\item I would randomly assign the students to a wide variety of $hours$ and then measure the SAT results.
\item Two factors that are likely contained in $u$ are intelligence and cognitive clarity (on the day of the test). It's difficult to say whether intelligence has a positive or negative correlation with $hours$, but it's likely that cognitive clarity is positively correlated with $hours$ because more preparation tends to improve a student's ability to remain focused during the exam.
\item $\beta_1$ should be positive.
\item $\beta_0$ is the expected $sat$ for a student that spends 0 hours in the SAT preparation course.
\end{enumerate}
\item 
\begin{enumerate}
\item $$\min_{b_0}SSR = \sum_{i=1}^{n}\left(y_i-b_0\right)^2$$
$$\frac{\partial{SSR}}{\partial{b_0}} = \sum_{i=1}^{n}-2\left(y_i-b_0\right)=0$$
$$\to\sum_{i=1}^{n}y_i-\sum_{i=1}^{n}b_0=0$$
$$\to n\bar{y}=nb_0$$
$$\to b_0^*=\tilde{\beta}_0=\bar{y}$$
\item $\sum_{i=1}^{n}\tilde{u_i}=\sum_{i=1}^{n}\left(y_i-\bar{y}\right)=n\bar{y}-n\bar{y}=0$
\end{enumerate}
\item 
\begin{enumerate}
\item $1-x_i$ equals 1 when $x_i=0$ and equals 0 otherwise. On the other hand, $x_i$ equals one when $x_i=1$ and equals 0 otherwise. Thus, $\sum_{i=1}^{n}(1-x_i)$ equals the number of observations with $x_i=0$ and $\sum_{i=1}^{n}x_i$ equals the number of observations with $x_i=1$.
$$\bar{x}=\frac{1}{n}\sum_{i=1}^{n}x_i=\frac{n_1}{n}$$$\bar{x}$ is the proportion of observations where $x_i=1$.
\item $$\bar{y}_0=\frac{1}{n_0}\sum_{i=1}^{n}y_i(0)=n_0^{-1}\sum_{i=1}^{n}(1-x_i)y_i$$
$$\bar{y}_1=\frac{1}{n_1}\sum_{i=1}^{n}y_i(1)=n_1^{-1}\sum_{i=1}^{n}x_iy_i$$
\item $\bar{y}=\frac{n_0}{n}\bar{y}_0+\frac{n_1}{n}\bar{y}_1=\bar{y}_0\left[\frac{1}{n}\sum_{i=1}^{n}\left(1-x_i\right)\right]+\bar{y}_1\left[\frac{1}{n}\sum_{i=1}^{n}x_i\right]=\bar{y}_0\left[\frac{1}{n}\left(n-n\bar{x}\right)\right]+\bar{y}_1\left[\frac{1}{n}\left(n\bar{x}\right)\right]=\left(1-\bar{x}\right)\bar{y}_0+\bar{x}\bar{y}_1$
\item $n^{-1}\sum_{i=1}^{n}x_i^2-\bar{x}^2=n^{-1}\sum_{i=1}^{n}x_i-\bar{x}^2=n^{-1}\left(n\bar{x}\right)-\bar{x}^2=\bar{x}-\bar{x}^2=\bar{x}\left(1-\bar{x}\right)$
\item $$n^{-1}\sum_{i=1}^{n}x_iy_i-\bar{x}\bar{y}=\frac{n_1}{n}\bar{y}_1-\bar{x}\left((1-\bar{x})\bar{y}_0+\bar{x}\bar{y}_1\right)$$
$$=\bar{x}\bar{y}_1-\bar{x}\left(\bar{y}_0-\bar{x}\bar{y}_0+\bar{x}\bar{y}_1\right)$$
$$=\bar{x}\bar{y}_1-\bar{x}\bar{y}_0+\bar{x}^2\bar{y}_0-\bar{x}^2\bar{y}_1$$
$$=\bar{x}\left(\bar{y}_1-\bar{y}_0+\bar{x}\bar{y}_0-\bar{x}\bar{y}_1\right)$$
$$=\bar{x}\left(\bar{y}_1(1-\bar{x})-\bar{y}_0(1-\bar{x})\right)$$
$$=\bar{x}(1-\bar{x})(\bar{y}_1-\bar{y}_0)$$
\item $$\hat{\beta}_1=\frac{\sum_{i=1}^{n}(x_i-\bar{x})y_i}{\sum_{i=1}^{n}(x_i-\bar{x})^2}$$
$$=\frac{\sum_{i=1}^{n}x_iy_i-\bar{x}\sum_{i=1}^{n}y_i}{\sum_{i=1}^{n}x_i^2-2\bar{x}\sum_{i=1}^{n}x_i+\sum_{i=1}^{n}\bar{x}^2}$$
$$=\frac{\sum_{i=1}^{n}x_iy_i-n\bar{x}\bar{y}}{\sum_{i=1}^{n}x_i^2-2n\bar{x}^2+n\bar{x}^2}$$
$$=\frac{n^{-1}\big\{\sum_{i=1}^{n}x_iy_i-n\bar{x}\bar{y}\big\}}{n^{-1}\big\{\sum_{i=1}^{n}x_i^2-n\bar{x}^2\big\}}$$
$$=\frac{n^{-1}\sum_{i=1}^{n}x_iy_i-\bar{x}\bar{y}}{n^{-1}\sum_{i=1}^{n}x_i^2-\bar{x}^2}$$
$$=\frac{\bar{x}(1-\bar{x})(\bar{y}_1-\bar{y}_0)}{\bar{x}(1-\bar{x})}=\bar{y}_1-\bar{y}_0$$
\item $\bar{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}=(1-\bar{x})\bar{y}_0+\bar{x}\bar{y}_1-\left(\bar{y}_1-\bar{y}_0\right)\bar{x}=\bar{y}_0-\bar{x}\bar{y}_0+\bar{x}\bar{y}_1-\bar{x}\bar{y}_1+\bar{x}\bar{y}_0=\bar{y}_0$
\end{enumerate}
\item As proven in the previous problem, $\hat{\beta}_1=\bar{y}_1-\bar{y}_0$. Thus, it equals the sample average (proportion) of those who were employed and participated in the program minus the sample average (proportion) of those who were unemployed but did not partake in the program. Thus, $\hat{\beta}_1$ is simply the the difference in employment rates between those who participated in the program and those who did not within the given sample.
\item
\begin{enumerate}
\item $E\left[n^{-1}\sum_{i=1}^{n}\left[y_i(1)-y_i(0)\right]\right]=n^{-1}\sum_{i=1}^{n}\left(E\left[y_i(1)\right]-E\left[y_i(0)\right]\right)=n^{-1}\left(nE\left[y_i(1)\right]-nE\left[y_i(0)\right]\right)=E\left[y_i(1)\right]-E\left[y_i(0)\right]=\tau_{ate}$
\item $\bar{y}_0$ is the sample average of $y$ for the observations where $x_i=0$. $\bar{y}_1$ is the sample average of $y$ for the observations where $x_i=1$. $\bar{y}(1)\text{ and }\bar{y}(0)$ are relative to the entire sample such that $\bar{y}(\cdot)=n_{\cdot}n^{-1}\bar{y}_\cdot$.
\end{enumerate}
\item 
\begin{enumerate}
\item The difference in means estimator is generally no longer unbiased because the ceteris paribus condition is no longer satisfied. More specifically, the decision to participate may be correlated with the independent and dependent variables, which would cause the estimator to be biased.
\item One example that would cause bias is if wealthier individuals tended to not participate. It's likely wealth is correlated with both $unemployment$ and $program$, which would cause for a biased estimator.
\end{enumerate}
\item $$\text{Var}(u_i|x_i)=\text{Var}\left((1-x_i)u_i(0)+x_iu_i(1)\middle|x\right)$$
$$=\text{Var}\left((1-x_i)u_i(0)\middle|x\right)+\text{Var}\left(x_iu_i(1)\middle|x\right)$$
$$=(1-x_i)^2\text{Var}\left(u_i(0)\middle|x\right)+x_i^2\text{Var}\left(u_i(1)\middle|x\right)$$
$$=(1-x_i)^2\sigma_0^2+x_i^2\sigma_1^2$$
\item 
\begin{enumerate}
\item $P(\text{All }x=1 \text{ or all } x=0)=P(\text{All }x=1 \cup\text{ all } x=0)=P(\text{All }x=1)+ P(\text{ All } x=0)-P(\text{All }x=1 \cap\text{ all } x=0)=\rho^n+(1-\rho)^n-P(\emptyset)=\rho^n+(1-\rho)^n$. Because $0<\rho<1$ (and thus $0<(1-\rho)<1$),$$\lim_{n\to\infty}P(\text{All }x=1)=\lim_{n\to\infty}\rho^n=0$$Likewise,$$\lim_{n\to\infty}P(\text{All }x=0)=\lim_{n\to\infty}(1-\rho)^n=0$$
\item n=10:

$P(\text{All }x=1 \text{ or all } x=0)=P(\text{All }x=1 \cup\text{ all } x=0)=0.5^{10}+(1-0.5)^{10}=2\cdot 0.5^{10}\approx$ `r 2*0.5^10`

n=100

$P(\text{All }x=1 \text{ or all } x=0)=P(\text{All }x=1 \cup\text{ all } x=0)=0.5^{100}+(1-0.5)^{100}=2\cdot 0.5^{100}\approx$ `r 2*0.5^100`
\item n=10:

$P(\text{All }x=1 \text{ or all } x=0)=P(\text{All }x=1 \cup\text{ all } x=0)=0.9^{10}+(1-0.9)^{10}\approx$ `r 0.9^10*0.1^10`

n=100

$P(\text{All }x=1 \text{ or all } x=0)=P(\text{All }x=1 \cup\text{ all } x=0)=0.9^{100}+(1-0.9)^{100}\approx$ `r 0.9^100*0.1^100`
\end{enumerate}
\end{enumerate}


### Computer Exercises

C1)
```{r Chapter 2 Computer Exercise 1,echo=TRUE,include=TRUE,comment=NA}
#i
mean(k401k$prate)
mean(k401k$mrate)

#ii
lm1 <- lm(prate ~ mrate, data = k401k)
summary(lm1)

#iii
#The intercept suggests a participation rate of 83.07546% when the match rate is 0%
#The coefficient on mrate suggests an additional 5.861079% in participation rate for
#every additional percentage of prate

#iv
unname(lm1$coefficients[1])+unname(lm1$coefficients[2])*3.5
#The percentage exceeds 100% as this is a linear model. Clearly, this is not a reasonable
#prediction

#v
summary(lm1)["r.squared"]
#About 7.5%. Personally, I would consider this a lot for one variable, but I 
#expected it to be higher
rm(lm1)
```

C2)
```{r Chapter 2 Computer Exercise 2,echo=TRUE,include=TRUE,comment=NA}
#i
mean(ceosal2$salary)
mean(ceosal2$ceoten)

#ii
sum(ceosal2$ceoten==0)
max(ceosal2$ceoten)

#iii
lm(log(salary) ~ ceoten, data = ceosal2)
#An additional year as CEO is predicted to increase salary by about 1%.
```

C3)
```{r Chapter 2 Computer Exercise 3,echo=TRUE,include=TRUE,comment=NA}
#i
lm1 <- lm(sleep ~ totwrk, data = sleep75)
summary(lm1)
#The intercept (3586.37695) is the predicted minutes of sleep for an individual with
#0 minutes spent in paid work

#ii
unname(lm1$coefficients[2])*120
#A loss in 18 minutes of sleep for additional hours of totwrk does not seem large
rm(lm1)
```

C4)
```{r Chapter 2 Computer Exercise 4,echo=TRUE,include=TRUE,comment=NA}
#i
mean(wage2$wage)
mean(wage2$IQ)
sd(wage2$IQ)

#ii
lm1 <- lm(wage ~ IQ, data = wage2)
unname(lm1$coefficients[2])*15
summary(lm1)["r.squared"]
#IQ explains about 9.5% of the variation in wage

#iii
lm1 <- lm(log(wage) ~ IQ, data = wage2)
unname(lm1$coefficients[2])*15*100
rm(lm1)
```

C5)
```{r Chapter 2 Computer Exercise 5,echo=TRUE,include=TRUE,comment=NA}
#i
#ln(rd) = beta_0+ beta_1 ln(sales) + u

#ii
lm1 <- lm(lrd ~ lsales, data = rdchem)
summary(lm1)
#The estimated elasticity suggests a 1.07573% increase in rd for every percentage
#increase in sales
```

C6)
```{r Chapter 2 Computer Exercise 6,echo=TRUE,include=TRUE,comment=NA}
#i
#I expected a diminishing marginal return in the pass rate for each additional dollar spent.
#For one, the pass rate cannot exceed 100%. Additionally, only so much can be spent on a student
#before no additional gains are possible. Thus, a constant marginal return seems unlikely/unreasonable.

#ii
#The percentage change in math10 for a 1% increase in expend is beta_1/100
#Thus, beta_1/10 is the percentage point change in math10 given a 10% increase in expend

#iii
lm1 <- lm(math10 ~ lexpend, data = meap93)
summary(lm1)

#iv
unname(lm1$coefficients[2]) / 10

#v
max(meap93$math10)
#In this data set, the largest value of math10 is 66.7, which isn't particularly close to 100
rm(lm1)
```

C7)
```{r Chapter 2 Computer Exercise 7,echo=TRUE,include=TRUE,comment=NA}
#i
mean(charity$gift)
mean(charity$gift > 0)

#ii
mean(charity$mailsyear)
max(charity$mailsyear)
min(charity$mailsyear)

#iii
lm1 <- lm(gift ~ mailsyear, data = charity)
summary(lm1)

#iv
#The slope coefficient predicts about a 2.7 Dutch guilders increase in gift for every additional
#mailsyear. If each mailing costs on guilder, the charity is expected to make a net gain on each
#mailing, but this doesn't mean the charity makes a net gain on every mailing

#v
min(lm1$fitted.values)
#With this regression result, a negative value for mailsyear would be required to predict 0 for
#gift, which is unfeasible
rm(lm1)
```

C9)
```{r Chapter 2 Computer Exercise 8,echo=TRUE,include=TRUE,comment=NA}
#i
x <- runif(500, 0, 10)
mean(x)
sd(x)

#ii
u <- runif(500, 0, 36)
#No, the sample average is not zero because all of the u are positive
sd(u)

#iii
y <- 1 + 2 * x + u
lm1 <- lm(y ~ x)
summary(lm1)
#No, they are not equal because the unobserved factors are not included in the regression model

#iv
sum(lm1$residuals)
sum(lm1$residuals * x)

#v
sum(u)
sum(u * x)
#The sum of the error terms and the errors terms times x generally won't sum
#to 0 like the residuals will

#vi
x <- runif(500, 0, 10)
mean(x)
sd(x)
u <- runif(500, 0, 36)
sd(u)
y <- 1 + 2 * x + u
lm2 <- lm(y ~ x)
summary(lm2)
#The results are not the same because the samples are different
rm(x,y,u,lm1,lm2)
```

C9)
```{r Chapter 2 Computer Exercise 9,echo=TRUE,include=TRUE,comment=NA}
county_murders <- as_tibble(countymurders) %>%
  filter(year == 1996)

#i
sum(county_murders$murders == 0)
sum(county_murders$execs > 0)
max(county_murders$execs)

#ii
lm1 <- lm(murders ~ execs, data = county_murders)
summary(lm1)

#iii
#The model predicts an additional 58.555 executions for every additional murder in a county.
#This does not suggest a deterrent effect of capital punishment

#iv
min(lm1$fitted.values)
unname(lm1$residuals[which(county_murders$execs == 0)[1]])

#v
#A simple regression analysis not well suited for determining whether capital punishment
#has a deterrent effect on murders because there are many unobserved factors that are likely
#correlated with both murders and execs, which leads to a biased slope estimate. Also, there's
#likely reverse causality in which murders causes more execs
rm(county_murders,lm1)
```

C10)
```{r Chapter 2 Computer Exercise 10 parts i-ii,echo=TRUE,include=TRUE,comment=NA}
#i
length(catholic$math12)
mean(catholic$math12)
sd(catholic$math12)
mean(catholic$read12)
sd(catholic$read12)

#ii
lm1 <- lm(math12 ~ read12, data = catholic)
```

$$\widehat{math12} = `r unname(unlist(lm1["coefficients"]))[1]`+`r unname(unlist(lm1["coefficients"]))[2]`read12$$
$$n = `r length(catholic$math12)`, R^2 = `r unname(unlist(summary(lm1)["r.squared"]))`$$

```{r Chapter 2 Computer Exercise 10 parts iii-v,echo=TRUE,include=TRUE,comment=NA}
#iii
#Yes, the intercept suggests prediction of 15.15304 on math12 for a score of 0 on read12

#iv
#I'm not surprised by the coefficient. I would expect the scores to be positively correlated.
#I'm somewhat surprised by how large the coefficient of determination is.

#v
#I would counter their statement by stating it's more correlational rather than causal.
#Unobserved factors correlated with math12 and read12 are uncontrolled for, which would
#make the slope coefficient biased (if interpreted as a causal effect)
```

C11)
```{r Chapter 2 Computer Exercise 11,echo=TRUE,include=TRUE,comment=NA}
#i
length(gpa1$colGPA)
mean(gpa1$colGPA)
max(gpa1$colGPA)

#ii
sum(gpa1$PC)

#iii
lm1 <- lm(colGPA ~ PC, data = gpa1)
summary(lm1)
#The intercept estimate predicts a colGPA of 2.98941 for a student without a PC.
#The slope estimate predicts a 0.16952 estimate of the "treatment" effect of having a PC.
#The slope suggests a positive relationship between colGPA and PC (as one might expect).

#iv
unname(unlist(summary(lm1)["r.squared"]))
#The r-squared suggests not much of the variation in colGPA is explained by PC

#v
#No, unobserved factors that are correlated with colGPA and PC do not allow for beta_1
#to be interpreted as an unbiased estimate of the causal effect of owning a PC on colGPA.

rm(lm1)
```

\newpage

# Chapter 3

## Notes

### Multiple Regression Analysis

\textbf{Multiple regression analysis} is generally better fit to predict causal effects over simple regression analysis because it allows us to explicitly control for many factors that are correlated with the singular independent variable (from the simple regression model) and the dependent variable.

In general, the \textbf{multiple linear regression (MLR) model} takes the form$$y=\beta_0+\beta_1x_1+\beta_2x_2+\dots+\beta_kx_k+u$$

### OLS Estimators Derivation (MLR)

Synonymous to the case with simple linear regression analysis, using OLS to obtain estimators of the parameters $\beta_0,\beta_1,\dots,\beta_k$ yields the following:
$$\min_{\hat{\beta}_0,\hat{\beta}_1,\dots,\hat{\beta}_k}SSR=\sum_{i=1}^{n}\left(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_kx_{ik}\right)^2$$
$$\frac{\partial{SSR}}{\partial{\hat{\beta}_0}}=\sum_{i=1}^{n}-2\left(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_kx_{ik}\right)=0$$
$$\frac{\partial{SSR}}{\partial{\hat{\beta}_1}}=\sum_{i=1}^{n}-2x_{i1}\left(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_kx_{ik}\right)=0$$
$$\dots$$
$$\frac{\partial{SSR}}{\partial{\hat{\beta}_k}}=\sum_{i=1}^{n}-2x_{ik}\left(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_kx_{ik}\right)=0$$
These are often called the OLS \textbf{first order conditions}. As with the simple regression model, the OLS first order conditions can be obtained by the method of moments: under the assumptions that $E[u]=0$ and $E[x_ju]=0\ \forall \ j=1,\dots,k$.

From the first FOC,$$\sum_{i=1}^{n}\left(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_kx_{ik}\right)=0$$
$$\to\sum_{i=1}^{n}\hat{\beta}_0=\sum_{i=1}^{n}\left(y_i-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_kx_{ik}\right)$$
$$\to n\hat{\beta}_0=n\bar{y}-n\hat{\beta}_1\bar{x_1}-\dots-n\hat{\beta}_k\bar{x_k}$$
$$\to \hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x_1}-\dots-\hat{\beta}_k\bar{x_k}$$
\small
\textit{Note:} Like in the simple linear regression case, the significance of this FOC is that, using least squares criteria, our line of best fit runs through the sample means $\bar{y}$ and $\bar{x}_j$ for $j=1,\dots,k$.
\normalsize

To derive the slope estimators, start by regressing $x_\ell$ on all of the other regressors in the model, which takes the form$$x_\ell=\gamma_0+\gamma_1x_1+\dots+\gamma_{\ell-1}x_{\ell-1}+\gamma_{\ell+1}x_{\ell+1}+\dots+\gamma_{k}x_{k}+r_{i\ell}$$If we denote the residuals of the model as $\hat{r}_{i\ell}$ and the predicted values as $\hat{x}_{i\ell}$, then$$x_{i\ell}=\hat{x}_{i\ell}+\hat{r}_{i\ell}$$Plugging this derivation into the $(\ell+1)^{th}$ first order condition:
$$\frac{\partial{SSR}}{\partial{\beta_\ell}}=\sum_{i=1}^{n}-2x_{i\ell}\left(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_kx_{ik}\right)=0$$
$$\to\sum_{i=1}^{n}\left(\hat{x}_{i\ell}+\hat{r}_{i\ell}\right)\left(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_kx_{ik}\right)=0$$
$$\to\sum_{i=1}^{n}\hat{x}_{i\ell}\left(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_kx_{ik}\right)+\sum_{i=1}^{n}\hat{r}_{i\ell}\left(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_kx_{ik}\right)=0$$
$$\to\sum_{i=1}^{n}\hat{x}_{i\ell}\hat{u}_i+\sum_{i=1}^{n}\hat{r}_{i\ell}\left(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_kx_{ik}\right)=0$$
Because $\hat{x}_{i\ell}$ is simply a linear combination of all the other regressors in the model (i.e., $\hat{x}_{i\ell}=\hat{\gamma}_0+\hat{\gamma}_1x_{i1}+\dots+\hat{\gamma}_{\ell-1}x_{i\ell-1}+\gamma_{\ell+1}x_{i\ell+1}+\dots+\gamma_{k}x_{ik}$), it follows that $\sum_{i=1}^{n}\hat{x}_{i\ell}\hat{u}_i=0$.
$$\to\sum_{i=1}^{n}\hat{r}_{i\ell}\left(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_kx_{ik}\right)=0$$
$$\to\sum_{i=1}^{n}\hat{r}_{i\ell}\left(y_i-\hat{\beta}_\ell x_{i\ell}\right) +\sum_{i=1}^{n}\hat{r}_{i\ell}\left(-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_{\ell-1}x_{i\ell-1}-\hat{\beta}_{\ell+1}x_{i\ell+1}-\dots-\hat{\beta}_kx_{ik}\right)=0$$
Because $\hat{r}_{i\ell}$ are the residuals from regressing $x_\ell$ on all the other regressors, $\sum_{i=1}^{n}x_{ij}\hat{r}_{i\ell}=0\ \forall\ j\neq \ell$.
$$\to\sum_{i=1}^{n}\hat{r}_{i\ell}\left(y_i-\hat{\beta}_\ell x_{i\ell}\right)=0$$
$$\to\sum_{i=1}^{n}\hat{r}_{i\ell}\left(y_i-\hat{\beta}_\ell\left(\hat{x}_{i\ell}+\hat{r}_{i\ell}\right)\right)=0$$
$$\to\sum_{i=1}^{n}\hat{r}_{i\ell}\left(y_i-\hat{\beta}_\ell\hat{r}_{i\ell}\right)-\hat{\beta}_\ell\sum_{i=1}^{n}\hat{r}_{i\ell}\hat{x}_{i\ell}=0$$
Since $\sum_{i=1}^{n}\hat{r}_{i\ell}\hat{x}_{i\ell}=0$ (This can be thought of as $\sum_{i=1}^{n}\hat{r}_{i\ell}\hat{x}_{i\ell}=0\to\sum_{i=1}^{n}\hat{u}_{i}\hat{y}_{i}=0\to\hat{\beta}_0\sum_{i=1}^{n}\hat{u}_{i}+\hat{\beta}_1\sum_{i=1}^{n}x_i\hat{u}_{i}=0+0=0$ in the SLR case),
$$\to\sum_{i=1}^{n}\hat{r}_{i\ell}\left(y_i-\hat{\beta}_\ell\hat{r}_{i\ell}\right)=0$$
$$\to\hat{\beta}_\ell\sum_{i=1}^{n}\hat{r}_{i\ell}^2=\sum_{i=1}^{n}\hat{r}_{i\ell}y_i$$
$$\to\hat{\beta}_\ell=\frac{\sum_{i=1}^{n}\hat{r}_{i\ell}y_i}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}$$

### MLR Interpretation

In MLR analysis, the slope estimates $\hat{\beta}_1,\dots,\hat{\beta}_k$ are interpreted as the \textbf{partial effects} of the corresponding explanatory variables on the dependent variable. In other words, they have a \textbf{ceteris paribus} interpretation.

\newpage

\underline{OLS Fitted Values and Residuals Properties (MLR)}

\textit{Note:} The OLS fitted values and residuals have some important properties that are immediate extensions from the bivariate case:

\begin{enumerate}
\item The sample average of the residuals is zero and so $\bar{y}=\bar{\hat{y}}$
\item The sample covariance between each independent variable and the OLS residuals is zero (i.e., $\hat{\sigma}_{x_j\hat{u}}=0\ \forall\ j=1,\dots,k$). Hence,$$\hat{\sigma}_{\hat{y}\hat{u}}=0$$
\item The point $\left(\bar{x}_1,\dots,\bar{x}_k,\bar{y}\right)$ always lies on the OLS regression line: $\bar{y}=\hat{\beta}_0+\hat{\beta}_1x_1+\dots+\hat{\beta}_kx_k$
\end{enumerate}

The first two properties are immediate consequences of the OLS first order conditions used to obtain the OLS estimators. Namely,$$\sum_{i=1}^{n}u_i=0;\ \sum_{i=1}^{n}x_{ij}u_i=0,$$where $\sum_{i=1}^{n}x_{ij}u_i=0$ implies that each regressor has a zero sample covariance with $\hat{u}_i$. The third property was derived earlier in the "OLS Estimators Derivation (MLR)" section.

### Partialling Out Interpretation

Earlier, we derived $$\hat{\beta}_{\ell}=\frac{\sum_{i=1}^{n}\hat{r}_{i\ell}y_i}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}$$Let $\ell=1$ in the model $y=\beta_0+\beta_1x_1+\beta_2x_2+u$. Then, $\hat{r}_{i1}$ are the OLS residuals from a simple regression of $x_1$ on $x_2$. The above derivation shows that we can then do a simple regression of $y$ on $\hat{r}_1$ to obtain $\hat{\beta}_1$. (Note that the residuals $\hat{r}_{i1}$ have a zero sample average, and so $\hat{\beta}_1$ is the usual slope estimate from simple regression). The interpretation of this is that the residuals $\hat{r}_{i1}$ are the part of $x_{i1}$ that are uncorrelated with $x_{i2}$. In other words, $\hat{r}_{i1}$ is $x_{i1}$ after the effects of $x_{i2}$ have been partialled out. Thus, $\hat{\beta}_1$ measures the sample relationship between $y$ and $x_1$ after $x_2$ has been partialled out. This result is usually called the \textbf{Frisch-Waugh theorem}.

### Simple vs. Multiple Regression Estimates

Define a simple regression model $\tilde{y}=\tilde{\beta}_0+\tilde{\beta}_1x_1$ and a multiple regression model $\hat{y}=\hat{\beta}_0+\hat{\beta}_1x_1+\hat{\beta}_2x_2$. Then, it turns out$$\tilde{\beta}_1=\hat{\beta}_1+\hat{\beta}_2\tilde{\delta}_1,$$where $\tilde{\delta}_1$ is the slope coefficient from regressing $x_{i2}$ on $x_{i1}$ and thus the confounding term is the partial effect of $x_2$ on $\hat{y}$ times $\tilde{\delta}_1$. The two cases in which $\tilde{\beta}_1=\hat{\beta}_1$ are

\begin{enumerate}
\item The partial effect of $x_2$ on $\hat{y}$ is zero in the sample (i.e., $\hat{\beta}_2=0$)
\item $x_1 \text{ and } x_2$ are uncorrelated in the sample (i.e., $\tilde{\delta}_1=0$)
\end{enumerate}

More generally, if we define a model $\hat{y}=\hat{\beta}_0+\hat{\beta}_1x_1+\dots+\hat{\beta}_kx_k$ and another model $\tilde{y}=\tilde{\beta}_0+\tilde{\beta}_1x_1+\dots+\tilde{\beta}_{k-1}x_{k-1}$, then$$\tilde{\beta}_j=\hat{\beta}_j+\hat{\beta}_k\tilde{\delta}_j$$for $j=1,\dots,k-1$. Taking the conditional expectation of this value (on $x$):$$E\left[\tilde{\beta}_j\middle|x\right]=E\left[\hat{\beta}_j\middle|x\right]+E\left[\hat{\beta}_k\tilde{\delta}_j\middle|x\right]=\beta_j+\tilde{\delta}_jE\left[\hat{\beta}_k\middle|x\right]=\beta_j+\beta_k\tilde{\delta}_j,$$which shows $\tilde{\beta}_j$ is biased for $\beta_j$ unless $\beta_k=0$ (meaning $x_k$ is uncorrelated with $y$ in the population) or $\tilde{\delta}_j=0$ (meaning $x_k$ is uncorrelated with $x_j$ in the sample).

### Unbiasedness of OLS Estimators (MLR)

Earlier, we derived $$\hat{\beta}_{\ell}=\frac{\sum_{i=1}^{n}\hat{r}_{i\ell}y_i}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}$$
$$=\frac{\sum_{i=1}^{n}\hat{r}_{i\ell}\left(\beta_0+\beta_1x_{i1}+\dots+\beta_kx_{ik}+u_i\right)}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}$$
$$=\frac{\beta_0\sum_{i=1}^{n}\hat{r}_{i\ell}+\beta_1\sum_{i=1}^{n}x_{i1}\hat{r}_{i\ell}+\dots+\beta_k\sum_{i=1}^{n}x_{ik}\hat{r}_{i\ell}+\sum_{i=1}^{n}u_i\hat{r}_{i\ell}}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}$$
$$=\frac{\beta_\ell\sum_{i=1}^{n}x_{i\ell}\hat{r}_{i\ell}+\sum_{i=1}^{n}u_i\hat{r}_{i\ell}}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}$$
$$=\frac{\beta_\ell\sum_{i=1}^{n}\left(\hat{x}_{i\ell}+\hat{r}_{i\ell}\right)\hat{r}_{i\ell}+\sum_{i=1}^{n}u_i\hat{r}_{i\ell}}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}$$
$$=\frac{\beta_\ell\sum_{i=1}^{n}\hat{x}_{i\ell}\hat{r}_{i\ell}+\beta_\ell\sum_{i=1}^{n}\hat{r}_{i\ell}^2+\sum_{i=1}^{n}u_i\hat{r}_{i\ell}}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}$$
$$=\frac{\beta_\ell\sum_{i=1}^{n}\left(\hat{\gamma}_0+\hat{\gamma}_1x_{i1}+\dots+\hat{\gamma}_{\ell-1}x_{i\ell-1}+\gamma_{\ell+1}x_{i\ell+1}+\dots+\gamma_{k}x_{ik}\right)\hat{r}_{i\ell}+\beta_\ell\sum_{i=1}^{n}\hat{r}_{i\ell}^2+\sum_{i=1}^{n}\hat{r}_{i\ell}u_i}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}$$
$$=\frac{0+\beta_\ell\sum_{i=1}^{n}\hat{r}_{i\ell}^2+\sum_{i=1}^{n}\hat{r}_{i\ell}u_i}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}$$
$$=\beta_\ell+\frac{\sum_{i=1}^{n}\hat{r}_{i\ell}u_i}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}$$
Taking the expectation of this value (conditional on $x$):$$E\left[\hat{\beta}_\ell\middle|x\right]=E\left[\beta_\ell+\frac{\sum_{i=1}^{n}\hat{r}_{i\ell}u_i}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}\middle|x\right]$$
$$=\beta_\ell+\frac{1}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}E\left[\sum_{i=1}^{n}\hat{r}_{i\ell}u_i\middle|x\right]$$
$$=\beta_\ell+\frac{1}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}\sum_{i=1}^{n}E\left[\hat{r}_{i\ell}u_i\middle|x\right]$$
$$=\beta_\ell+\frac{1}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}\sum_{i=1}^{n}\hat{r}_{i\ell}E\left[u_i\middle|x\right]$$
$$=\beta_\ell+\frac{1}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}\sum_{i=1}^{n}\hat{r}_{i\ell}\cdot 0=\beta_\ell$$

An important implication of using OLS is that the \emph{procedure} by which OLS estimates are obtain is unbiased; however, it's almost always not the case that the estimate obtain is unbiased. Thus, if $\hat{\beta}_1$ is the OLS slope parameter estimator and $b_1$ is an OLS estimate from a particular, one may say $\hat{\beta}_1$ is unbiased under the G-M assumptions (i.e., $E\left[\hat{\beta}_1\right]=\beta_1$), but one should refrain from saying $b_1$ is unbiased.

### Specifying a Model

In multiple regression analysis \textbf{inclusion of an irrelevant variable} or \textbf{overspecifying/overfitting the model} is a case in which an explanatory variable is included in a regression model that has a zero population parameter in estimating an equation by OLS. While overspecifying the model does not affect the unbiasedness of the OLS estimators, including irrelevant variables can have undesirable effects on the variances of the OLS estimators.

\newpage

On the other hand, omitting a variable that actually belongs in the true (or population) model is called the problem of \textbf{excluding a relevant variable} or \textbf{underspecifying the model}. Looking back to the "Sample vs. Multiple Regression Estimates" section, define a model $\hat{y}=\hat{\beta}_0+\hat{\beta}_1x_1+\dots+\hat{\beta}_kx_k$ and another model $\tilde{y}=\tilde{\beta}_0+\tilde{\beta}_1x_1+\dots+\tilde{\beta}_{k-1}x_{k-1}$, then$$\tilde{\beta}_j=\hat{\beta}_j+\hat{\beta}_k\tilde{\delta}_j$$for $j=1,\dots,k-1$. Taking the conditional expectation of this value (on $x$):$$E\left[\tilde{\beta}_j\middle|x\right]=E\left[\hat{\beta}_j\middle|x\right]+E\left[\hat{\beta}_k\tilde{\delta}_j\middle|x\right]=\beta_j+\tilde{\delta}_jE\left[\hat{\beta}_k\middle|x\right]=\beta_j+\beta_k\tilde{\delta}_j,$$Thus, we can derive the \textbf{omitted variable bias} as$$\text{Bias}\left(\tilde{\beta}_j\right)=E\left[\tilde{\beta}_j\middle|x\right]-\beta_j=\beta_j+\beta_k\tilde{\delta}_j-\beta_j=\beta_k\tilde{\delta}_j$$

\underline{Summary of Bias in $\tilde{\beta}_j$ when $x_k$ is Omitted}
\begin{tabular}{|lll|}
\hline
 & $\text{Corr}(x_j,x_k)>0$ & $\text{Corr}(x_j,x_k)<0$ \\
\hline
$\beta_k>0$ & Positive Bias & Negative Bias \\
$\beta_k<0$ & Negative Bias & Positive Bias \\
\hline
\end{tabular}

More generally, it's difficult to derive the sign of omitted variable bias with multiple regressors. For example, suppose$$y=\beta_0+\beta_1 x_1+\beta_2 x_2+\beta_3 x_3+u$$satisfies the unbiasedness G-M assumptions, but we estimate the model as $$\tilde{y}=\tilde{\beta}_0+\tilde{\beta}_1x_1+\tilde{\beta}_2x_2$$If $x_1$ is correlated with $x_3$ but $x_2$ and $x_3$ are uncorrelated, both $\tilde{\beta}_1$ and $\tilde{\beta}_2$ will both be biased unless both $x_1$ and $x_2$ are also uncorrelated. In general, we commonly will assume $x_1$ and $x_2$ are uncorrelated to derive a likely direction of biasedness; however, it should be noted that this assumption generally doesn't hold.

In estimating parameters, we say an estimator has an \textbf{upward bias} if it has positive bias and a \textbf{downward bias} if it has a negative bias. In general, we say an estimator is \textbf{biased toward zero} if the estimator is closer to zero than the true parameter.  

### Sampling Variance of OLS Estimators (MLR)

Using a previous derivation,$$\hat{\beta}_\ell=\beta_\ell+\frac{\sum_{i=1}^{n}\hat{r}_{i\ell}u_i}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}$$
Thus,
$$\text{Var}\left(\hat{\beta}_\ell\middle|x\right)=\text{Var}\left(\beta_\ell+\frac{\sum_{i=1}^{n}\hat{r}_{i\ell}u_i}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}\middle|x\right)$$
$$=\frac{1}{\left(\sum_{i=1}^{n}\hat{r}_{i\ell}^2\right)^2}\text{Var}\left(\sum_{i=1}^{n}\hat{r}_{i\ell}u_i\middle|x\right)$$
$$=\frac{1}{\left(\sum_{i=1}^{n}\hat{r}_{i\ell}^2\right)^2}\sum_{i=1}^{n}\text{Var}\left(\hat{r}_{i\ell}u_i\middle|x\right)$$
$$=\frac{1}{\left(\sum_{i=1}^{n}\hat{r}_{i\ell}^2\right)^2}\sum_{i=1}^{n}\hat{r}_{i\ell}^2\text{Var}\left(u_i\middle|x\right)$$
$$=\frac{1}{\left(\sum_{i=1}^{n}\hat{r}_{i\ell}^2\right)^2}\sum_{i=1}^{n}\hat{r}_{i\ell}^2\sigma_u^2$$
$$=\frac{\sigma_u^2\sum_{i=1}^{n}\hat{r}_{i\ell}^2}{\left(\sum_{i=1}^{n}\hat{r}_{i\ell}^2\right)^2}$$
$$=\frac{\sigma_u^2}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}$$
$$=\frac{\sigma_u^2}{SST_{x_\ell}-SSE_{x_\ell}}$$
$$=\frac{\sigma_u^2}{SST_{x_\ell}\left(1-R_\ell^2\right)}$$where $R_\ell^2$ is the $R$-squared from regressing $x_\ell$ on all other independent variables.

High but imperfect correlation between two or more independent variables is called \textbf{multicollinearity}. Multicollinearity leads to large values of $R_\ell^2$, which in turn, leads to large values for $\text{Var}\left(\hat{\beta}_\ell\right)$. Worrying about high degrees of correlation among the independent variables in the sample is really no different from worrying about a small sample size as both work to increase $\text{Var}(\hat{\beta}_\ell)$. Thus, ideas surrounding small values for $R_\ell^2$ and $SST_{x_\ell}$ pertain to \textbf{micronumerosity}, or the problem of small sample size. While multicollinearity can pose an issue in statistical inference, it needn't always be cause for stress for two reasons. First, multicollinearity can point out "improper" questions in which we are asking questions that may be too subtle for the available data to answer with any precision. Additionally, regression analysis where there is a high degree of correlation between certain independent variables can be irrelevant as to how well we can estimate parameters of interest in a model. For example, suppose we hope to estimate$$y=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3+u$$where $x_2$ and $x_3$ are highly correlated by $x_1$ is not highly correlated with either. It turns out that the high level of correlation between $x_2$ and $x_3$ has no direct effect on $\text{Var}\left(\hat{\beta}_1\right)$. Thus, if $\beta_1$ is the parameter of interest, including $x_2$ and $x_3$ maintains the unbiasedness of $\hat{\beta}_1$ while imposing little to no additional variance in $\hat{\beta}_1$. The most common statistic for detecting multicollinearity for a particular coefficient is the \textbf{variance inflation factor (VIF)}, which equals $\frac{1}{1-R_\ell^2}$. Hence, $\text{Var}\left(\hat{\beta}_\ell\right)$ can be rewritten as$$\text{Var}\left(\hat{\beta}_\ell\right)=\frac{\sigma_u^2}{SST_\ell}\cdot \text{VIF}_\ell,$$which shows that $\text{VIF}_\ell$ is the factor by which $\text{Var}\left(\hat{\beta}_\ell\right)$ is higher because $x_\ell$ is not uncorrelated with the other explanatory variables.

Similar to the SLR case, these formulas allow us to isolate the factors that contribute to $\text{Var}\left(\hat{\beta}_\ell\right)$. But these formulas are unknown, except in the extremely rare case that $\sigma_u^2$ is known. Considering the $u_i$ are unobserved, an unbiased estimator of $\sigma_u^2$ is$$\hat{\sigma}_u^2=\frac{\sum_{i=1}^{n}\hat{u}_i^2}{n-k-1},$$ where the $n-k-1$ is the degrees of freedom in the OLS residuals due to the $k+1$ OLS first order conditions:$$\sum_{i=1}^{n}\hat{u}_i=0,\sum_{i=1}^{n}x_1\hat{u}_i=0,\dots,\sum_{i=1}^{n}x_k\hat{u}_i=0$$

Naturally, the estimators of $\text{Var}\left(\hat{\beta}_\ell\right)$ is
$$\hat{\sigma}_{\beta_\ell}^2=\frac{\sfrac{\sum_{i=1}^{n}\hat{u}_i^2}{(n-k-1)}}{SST_{x_\ell}\left(1-R_\ell^2\right)}$$

Additionally, the natural estimator of $\sigma_u$ is$$\hat{\sigma}_u=\sqrt{\hat{\sigma}_u^2},$$which is called the \textbf{standard error of the regresion (SER)}.

Although $\hat{\sigma}_u^2$ is an unbiased and consistent estimator of $\sigma_u^2$, $\hat{\sigma}_u$ is consistent but biased.

All together the \textbf{standard errors of the coefficients} equal $$\text{se}\left(\hat{\beta}_\ell\right)=\sqrt{\frac{\sfrac{\sum_{i=1}^{n}\hat{u}_i^2}{(n-k-1)}}{SST_{x_\ell}\left(1-R_\ell^2\right)}}$$

### Gauss-Markov Theorem

Under the Gauss-Markov assumptions, using least squares criteria to estimate the parameters yields the \textbf{best linear unbiased estimators (BLUE)} of the of parameters, a result known as the \textbf{Guass-Markov Theorem}. Restricting the scope to linear unbiased estimators, we can define an estimator$$\tilde{\beta}_\ell=\sum_{i=1}^{n}w_{i\ell}y_i$$where each $w_{i\ell}$ can be a function of the sample values of all the independent variables and
$$E\left[\tilde{\beta}_\ell\middle|x\right]=\beta_\ell$$
$$E\left[\sum_{i=1}^{n}w_{i\ell}y_i\middle|x\right]=\beta_\ell$$
$$E\left[\sum_{i=1}^{n}w_{i\ell}\left(\beta_0+\beta_1x_{i1}+\dots+\beta_kx_{ik}+u_i\right)\middle|x\right]=\beta_\ell$$
$$\beta_0\sum_{i=1}^{n}w_{i\ell}+\beta_1\sum_{i=1}^{n}x_{i1}w_{i\ell}+\dots+\beta_k\sum_{i=1}^{n}x_{ik}w_{i\ell}+\sum_{i=1}^{n}w_{i\ell}E\left[u_i\middle|x\right]=\beta_\ell$$
$$E\left[\beta_0\sum_{i=1}^{n}w_{i\ell}+\beta_1\sum_{i=1}^{n}x_{i1}w_{i\ell}+\dots+\beta_{\ell}\sum_{i=1}^{n}x_{i\ell}w_{i\ell}+\dots+\beta_k\sum_{i=1}^{n}x_{ik}w_{i\ell}+\sum_{i=1}^{n}u_iw_{i\ell}\middle|x\right]=\beta_\ell$$where $\sum_{i=1}^{n}w_{i\ell}E\left[u_i\middle|x\right]=0$ by the zero conditional mean assumption. Thus, it must be the case that
$$\sum_{i=1}^{n}w_{i\ell}=0$$
$$\sum_{i=1}^{n}w_{i\ell}x_{ij}=0\ \forall \ j\neq\ell$$
$$\sum_{i=1}^{n}w_{i\ell}x_{i\ell}=1$$
Using this fact, we can derive the variance of $\tilde{\beta}_\ell$ as
$$\text{Var}\left(\tilde{\beta}_\ell\middle|x\right)=\text{Var}\left(\beta_0\sum_{i=1}^{n}w_{i\ell}+\beta_1\sum_{i=1}^{n}x_{i1}w_{i\ell}+\dots+\beta_{\ell}\sum_{i=1}^{n}x_{i\ell}w_{i\ell}+\dots+\beta_k\sum_{i=1}^{n}x_{ik}w_{i\ell}+\sum_{i=1}^{n}u_iw_{i\ell}\middle|x\right)$$
$$=\text{Var}\left(\beta_{\ell}+\sum_{i=1}^{n}u_iw_{i\ell}\middle|x\right)$$
$$=\sum_{i=1}^{n}\text{Var}\left(u_iw_{i\ell}\middle|x\right)$$
$$=\sum_{i=1}^{n}w_{i\ell}^2\text{Var}\left(u_i\middle|x\right)$$
$$=\sigma_u^2\sum_{i=1}^{n}w_{i\ell}^2$$
Suppose $w_{i\ell}=v_{i\ell}+c_{i\ell}$ where $v_{i\ell}=\frac{\hat{r}_{i\ell}}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}$ (the OLS weights) and $c_{i\ell}$ is some arbitrary difference from the least squares weights, then
$$\text{Var}\left(\tilde{\beta}_\ell\middle|x\right)=\sigma_u^2\sum_{i=1}^{n}w_{i\ell}^2=\sigma_u^2\sum_{i=1}^{n}\left(v_{i\ell}+c_{i\ell}\right)^2$$
$$=\sigma_u^2\sum_{i=1}^{n}v_{i\ell}^2+\sigma_u^2\sum_{i=1}^{n}c_{i\ell}^2+2\sigma_u^2\sum_{i=1}^{n}v_{i\ell}c_{i\ell}$$
$$=\sigma_u^2\sum_{i=1}^{n}\left(\frac{\hat{r}_{i\ell}}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}\right)^2+\sigma_u^2\sum_{i=1}^{n}c_{i\ell}^2+2\sigma_u^2\sum_{i=1}^{n}v_{i\ell}c_{i\ell}$$
$$=\sigma_u^2\frac{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}{\left(\sum_{i=1}^{n}\hat{r}_{i\ell}^2\right)^2}+\sigma_u^2\sum_{i=1}^{n}c_{i\ell}^2+2\sigma_u^2\sum_{i=1}^{n}v_{i\ell}c_{i\ell}$$
$$=\frac{\sigma_u^2}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}+\sigma_u^2\sum_{i=1}^{n}c_{i\ell}^2+2\sigma_u^2\sum_{i=1}^{n}v_{i\ell}c_{i\ell}$$
$$=\text{Var}\left(\hat{\beta}_\ell\right)+\sigma_u^2\sum_{i=1}^{n}c_{i\ell}^2+2\sigma_u^2\sum_{i=1}^{n}v_{i\ell}c_{i\ell}$$
Hence, to show OLS estimators are BLUE under the G-M assumptions, it remains to show that
$$\text{Var}\left(\hat{\beta}_\ell\right)\leq\text{Var}\left(\hat{\beta}_\ell\right)+\sigma_u^2\sum_{i=1}^{n}c_{i\ell}^2+2\sigma_u^2\sum_{i=1}^{n}v_{i\ell}c_{i\ell}$$
$$\to 0\leq\sigma_u^2\sum_{i=1}^{n}c_{i\ell}^2+2\sigma_u^2\sum_{i=1}^{n}v_{i\ell}c_{i\ell},$$ which just requires showing $2\sigma_u^2\sum_{i=1}^{n}v_{i\ell}c_{i\ell}\geq 0$ since $\sigma_u^2\sum_{i=1}^{n}c_{i\ell}^2\geq 0$. Finalizing the proof,
$$2\sigma_u^2\sum_{i=1}^{n}v_{i\ell}c_{i\ell}\geq 0$$
$$\to\sum_{i=1}^{n}v_{i\ell}c_{i\ell}\geq 0$$
Returning to the previous derivations that $\sum_{i=1}^{n}w_{i\ell}=0$, $\sum_{i=1}^{n}w_{i\ell}x_{ij}=0\ \forall \ j\neq\ell$, and $\sum_{i=1}^{n}w_{i\ell}x_{i\ell}=1$,
$$0=\sum_{i=1}^{n}w_{i\ell}=\sum_{i=1}^{n}\left(v_{i\ell}+c_{i\ell}\right)=\sum_{i=1}^{n}v_{i\ell}+\sum_{i=1}^{n}c_{i\ell}=0+\sum_{i=1}^{n}c_{i\ell}$$
$$\to \sum_{i=1}^{n}c_{i\ell}=0$$
$$0=\sum_{i=1}^{n}w_{i\ell}x_{ij}=\sum_{i=1}^{n}\left(v_{i\ell}+c_{i\ell}\right)x_{ij}=\sum_{i=1}^{n}v_{i\ell}x_{ij}+\sum_{i=1}^{n}c_{i\ell}x_{ij}=0+\sum_{i=1}^{n}c_{i\ell}x_{ij}$$
$$\to\sum_{i=1}^{n}c_{i\ell}x_{ij}=0 \ \forall\  j\neq\ell$$
$$1=\sum_{i=1}^{n}w_{i\ell}x_{i\ell}=\sum_{i=1}^{n}\left(v_{i\ell}+c_{i\ell}\right)x_{i\ell}=\sum_{i=1}^{n}v_{i\ell}x_{i\ell}+\sum_{i=1}^{n}c_{i\ell}x_{i\ell}=1+\sum_{i=1}^{n}c_{i\ell}x_{i\ell}$$
$$\to\sum_{i=1}^{n}c_{i\ell}x_{i\ell}=0$$
Thus, $$\sum_{i=1}^{n}v_{i\ell}c_{i\ell}=\sum_{i=1}^{n}\frac{\hat{r}_{i\ell}c_{i\ell}}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}=\frac{\sum_{i=1}^{n}\hat{r}_{i\ell}c_{i\ell}}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}$$
$$=\frac{\sum_{i=1}^{n}\left(x_{i\ell}-\hat{x}_{i\ell}\right)c_{i\ell}}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}$$
$$=\frac{\sum_{i=1}^{n}x_{i\ell}c_{i\ell}-\sum_{i=1}^{n}\hat{x}_{i\ell}c_{i\ell}}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}$$
$$=\frac{0-\sum_{i=1}^{n}\left(\hat{\gamma}_0+\hat{\gamma}_1x_{i1}+\dots+\hat{\gamma}_{\ell-1}x_{i\ell-1}+\hat{\gamma}_{\ell+1}x_{i\ell+1}+\dots+\hat{\gamma}_kx_{ik}\right)c_{i\ell}}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}$$
$$=-\frac{\hat{\gamma}_0\sum_{i=1}^{n}c_{i\ell}+\hat{\gamma}_1\sum_{i=1}^{n}x_{i1}c_{i\ell}+\dots+\hat{\gamma}_{\ell-1}\sum_{i=1}^{n}x_{i\ell-1}c_{i\ell}+\hat{\gamma}_{\ell+1}\sum_{i=1}^{n}x_{i\ell+1}c_{i\ell}+\dots+\hat{\gamma}_k\sum_{i=1}^{n}x_{ik}c_{i\ell}}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}$$
$$=-\frac{\hat{\gamma}_0\cdot 0+\hat{\gamma}_1\cdot 0+\dots+\hat{\gamma}_{\ell-1}\cdot 0+\hat{\gamma}_{\ell+1}\cdot 0+\dots+\hat{\gamma}_k\cdot 0}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}=0$$
$\therefore \hat{\beta}_\ell$ is the best linear unbiased estimator of the population parameter $\beta_\ell$ because any other linear unbiased estimator has at least as much variance as the OLS estimator under the Gauss-Markov assumptions.

## Exercises

### Problems

\begin{enumerate}
\item 
\begin{enumerate}
\item We would expect a negative coefficient on $hsperc$ because we would expect those who do better in high school to do better in college as well. The better an individual does in college the larger is $colgpa$ and the better one does in high school the smaller is $hsperc$.
\item `r 1.392-.0135*20+.00148*1050`
\item The predicted difference in $colgpa$ is `r .00148*140`. In my opinion the difference is fairly large. To others, the difference may seem insignificant.
\item Holding $hsperc$ fixed, a difference in SAT scores of `r .5/.00148` leads to a predicted $colgpa$ difference of .50.
\end{enumerate}
\item 
\begin{enumerate}
\item Yes, we would expect $educ$ and $sibs$ to be negatively correlated because more siblings may restrict the amount of education an individual can receive. To reduce predicted years of education by one year, $sibs$ must increase by `r 1/.094`.
\item The coefficient on $meduc$ indicates a predicted increase of 0.131 years of schooling for every additional year of schooling the individual's mother received.
\item The predicted difference in schooling is `r 4*.131+4*.210` years.
\end{enumerate}
\item 
\begin{enumerate}
\item If adults trade off sleep for work, $\beta_1$ is negative.
\item I would guess that $\beta_2$ is positive and $\beta_3$ is negative.
\item If someone works five more hours per week, $sleep$ is predicted to fall by `r .148*5*60` minutes.
\item The coefficient on $educ$ indicates a predicted loss of 11.13 minutes in sleep per week for every additional year of education an individual receives. This tradeoff appears small.
\item Not particularly, the $R^2$ indicates $totwrk$, $educ$, and $age$ explain about 11.3\% of the variation in $sleep$. Some other factors that might affect time spent sleeping are an individual's health, occupation, and stress levels. It goes without saying that $totwrk$ is likely correlated with these other factors. 
\end{enumerate}
\item 
\begin{enumerate}
\item We would expect $\beta_5\leq0$ because we would expect those who attend better schools to earn a higher wage.
\item I expect the other slope parameters to be positive. Each of the corresponding dependent variables are indicators of the quality of a school or the quality of a given class, and I would expect those graduating from among more "talented" peers to earn higher wages.
\item The predicted ceteris paribus difference in salaries for schools with a median GPA different by one point is 24.8\%.
\item The coefficient on $\ln(libvol)$ is an estimate of the elasticity of $salary$ with respect $libvol$ (or the number of volumes in the law school library). The coefficient indicates a predicted 0.095\% increase in $salary$ for a 1\% increase in the number of volumes in the law school library.
\item Yes, I would say it's better to attend higher ranked schools. A difference in ranking of 20 is predicted to impact salary by `r .0033*100*20`\%.
\end{enumerate}
\item 
\begin{enumerate}
\item No. Because the time spent on these activities must sum to 168 hours, changing $study$ must change the time spent on at least one of the other activities.
\item This model violates the no perfect collinearity assumption because the variables can be expressed perfectly as a linear combination of the other independent variables. For example, $study$ can be expressed as $168-sleep-work-leisure$. This same concept applies to all of the explanatory variables.
\item Dropping one of the explanatory variables would alleviate the perfect collinearity issue and allow for a useful interpretation.
\end{enumerate}
\item Conditioning on $x$:
\begin{enumerate}
\item $E\left[\hat{\theta}_1\right]=E\left[\hat{\beta_1}+\hat{\beta}_2\right]=E\left[\hat{\beta_1}\right]+E\left[\hat{\beta}_2\right]=\beta_1+\beta_2=\theta_1$
\item $$\text{Var}\left(\hat{\theta}_1\right)=\text{Var}\left(\hat{\beta}_1+\hat{\beta}_2\right)$$
$$=\text{Var}\left(\hat{\beta}_1\right)+2\text{Cov}\left(\hat{\beta}_1,\hat{\beta}_2\right)+\text{Var}\left(\hat{\beta}_2\right)$$
$$=\text{Var}\left(\hat{\beta}_1\right)+2\text{Corr}\left(\hat{\beta}_1,\hat{\beta}_2\right)\cdot\sqrt{\text{Var}\left(\hat{\beta}_1\right)\text{Var}\left(\hat{\beta}_2\right)}+\text{Var}\left(\hat{\beta}_2\right)$$
\end{enumerate}
\item Of the provided options, only "(ii) Omitting an important variable" can cause OLS estimators to be biased. Heteroskedasticity and multicollinearity have "negative" implications on the variance of the OLS estimators but do not cause OLS estimators to be biased.
\item Using intuition, $\beta_1$ and $\beta_2$ are likely positive as one would expect more trained and able workers are more productive. If $avgtrain$ and $avgabil$ are negatively correlated, we can derive the bias in $\tilde{\beta}_1$ as$$\text{Bias}\left(\tilde{\beta}_1\right)=\beta_1+\tilde{\delta}_1\beta_2-\beta_1=\tilde{\delta}_1\beta_2,$$which we would expect to be negative since $avgtrain$ and $avgabil$ are negatively correlated (i.e., $\tilde{\delta}_1<0$) and $\beta_2$ is likely positive.
\item 
\begin{enumerate}
\item $\beta_1$ is likely negative because more pollution in a community should lead to lower house prices on average. $\beta_2$ is likely positive as more rooms in a house commands higher house prices on average. $\beta_1$ is the elasticity of the median housing price in a community with respect to the amount of pollution in a community.
\item $\ln(nox)$ and $rooms$ may be negatively correlated because people may choose to build homes with more rooms where there is less pollution. For example, families with children need more rooms and would prefer to be in an area with cleaner air. If there is negative correlation between $\ln(nox)$ and $rooms$, then $\text{Bias}\left(\tilde{\beta}_1\right)=\beta_1+\tilde{\delta}_1\beta_2-\beta_1=\tilde{\delta}_1\beta_2<0$, meaning the simple regression estimator of $\beta_1$ will produce a downward biased estimator.
\item Yes, this is what we would have expected since the estimated elasticity in the simple regression model is less than that of the multiple regression model. However, this does not necessarily mean that -.718 is definitely closer to the true elasticity than -1.043. It simply means that we expect the method by which the multiple regression coefficient was produced to be closer to the true elasticity.
\end{enumerate}
\item 
\begin{enumerate}
\item If $x_1$ is highly correlated with $x_2$ and $x_3$ in the sample, and $x_2$ and $x_3$ have large partial effects on $y$, I would expect $\tilde{\beta}_1$ and $\hat{\beta}_1$ to be very different. Solely looking at the case of omitting $x_2$, we can derived the bias in $\tilde{\beta}_1$ as $\text{Bias}\left(\tilde{\beta}_1\right)=\beta_1+\tilde{\delta}_1\beta_2-\beta_1=\tilde{\delta}_1\beta_2$. A large correlation between $x_1$ and $x_2$ indicate $\tilde{\delta}_1$ should be relatively large, and $x_2$ having a large partial effect on $y$ indicates $\beta_2$ is large. Thus, we can conclude the bias in $\tilde{\beta}_1$ should be large. Since we know (under the G-M assumptions) that $\hat{\beta}_1$ is unbiased, I would expect the estimates produced by the separate estimators to be very different.
\item If $x_1$ is almost uncorrelated with $x_2$ and $x_3$ but $x_2$ and $x_3$ are highly correlated, I would expect $\tilde{\beta}_1$ to be very similar to $\hat{\beta}_1$. A large correlation between $x_2$ and $x_3$ doesn't massively impact the bias in $\tilde{\beta}_1$. Looking at part (a), we derived the bias in $\tilde{\beta}_1$ as $\tilde{\delta}_1\beta_2$ when the bias from omitting $x_3$ is ignored. If $x_1$ is almost uncorrelated with $x_2$ in the population, it's likely $x_1$ is almost uncorrelated with $x_2$ in the sample and thus $\tilde{\delta}_1$ should be quite small and hence the bias in $\tilde{\beta}_1$ should be quite small. Again, since we know (under the G-M assumptions) that $\hat{\beta}_1$ is unbiased, I would expect the estimates produced by the separate estimators to be very similar.
\item A large correlation between $x_2$ and $x_3$ means we're likely to experience a "high" level of multicollinearity in our model. Additionally, since $x_2$ and $x_3$ have small partial effects on $y$, omitting these variables from the model should not lead to a large increase in the sum of squared residuals. Altogether, we would expect $\text{se}\left(\tilde{\beta}_1\right)$ to be smaller than $\text{se}\left(\hat{\beta}_1\right)$.
\item A small correlation between $x_2$ and $x_3$ means we're likely to experience a "low" level of multicollinearity in our model. Additionally, since $x_2$ and $x_3$ have large partial effects on $y$, omitting these variables from the model will likely lead to a large increase in the sum of squared residuals. Altogether, we would expect $\text{se}\left(\hat{\beta}_1\right)$ to be smaller than $\text{se}\left(\tilde{\beta}_1\right)$.
\end{enumerate}
\item $$\tilde{\beta}_1=\frac{\sum_{i=1}^{n}\hat{r}_{i1}y_i}{\sum_{i=1}^{n}\hat{r}_{i1}^2}$$
$$=\frac{\sum_{i=1}^{n}\hat{r}_{i1}\left(\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\beta_3x_{i3}+u_i\right)}{\sum_{i=1}^{n}\hat{r}_{i1}^2}$$
$$=\frac{\beta_0\sum_{i=1}^{n}\hat{r}_{i1}+\beta_1\sum_{i=1}^{n}\hat{r}_{i1}x_{i1}+\beta_2\sum_{i=1}^{n}\hat{r}_{i1}x_{i2}+\beta_3\sum_{i=1}^{n}\hat{r}_{i1}x_{i3}+\sum_{i=1}^{n}\hat{r}_{i1}u_i}{\sum_{i=1}^{n}\hat{r}_{i1}^2}$$
$$=\frac{\beta_0\cdot 0+\beta_1\sum_{i=1}^{n}\hat{r}_{i1}\left(\hat{r}_{i1}+\hat{x}_{i1}\right)+\beta_2\cdot 0+\beta_3\sum_{i=1}^{n}\hat{r}_{i1}x_{i3}+\sum_{i=1}^{n}\hat{r}_{i1}u_i}{\sum_{i=1}^{n}\hat{r}_{i1}^2}$$
$$=\frac{\beta_1\sum_{i=1}^{n}\hat{r}_{i1}^2+\beta_1\sum_{i=1}^{n}\hat{r}_{i1}\hat{x}_{i1}+\beta_3\sum_{i=1}^{n}\hat{r}_{i1}x_{i3}+\sum_{i=1}^{n}\hat{r}_{i1}u_i}{\sum_{i=1}^{n}\hat{r}_{i1}^2}$$
$$=\beta_1+\frac{\beta_1\sum_{i=1}^{n}\hat{r}_{i1}\left(\hat{\gamma}_0+\hat{\gamma}_1x_{i2}\right)+\beta_3\sum_{i=1}^{n}\hat{r}_{i1}x_{i3}+\sum_{i=1}^{n}\hat{r}_{i1}u_i}{\sum_{i=1}^{n}\hat{r}_{i1}^2}$$
$$=\beta_1+\frac{\beta_1\cdot 0+\beta_3\sum_{i=1}^{n}\hat{r}_{i1}x_{i3}+\sum_{i=1}^{n}\hat{r}_{i1}u_i}{\sum_{i=1}^{n}\hat{r}_{i1}^2}$$
$$=\beta_1+\frac{\beta_3\sum_{i=1}^{n}\hat{r}_{i1}x_{i3}+\sum_{i=1}^{n}\hat{r}_{i1}u_i}{\sum_{i=1}^{n}\hat{r}_{i1}^2}$$
Thus,$$E\left[\tilde{\beta}_1\middle|x\right]=E\left[\beta_1+\frac{\beta_3\sum_{i=1}^{n}\hat{r}_{i1}x_{i3}+\sum_{i=1}^{n}\hat{r}_{i1}u_i}{\sum_{i=1}^{n}\hat{r}_{i1}^2}\middle|x\right]$$
$$=\beta_1+\frac{1}{\sum_{i=1}^{n}\hat{r}_{i1}^2}E\left[\beta_3\sum_{i=1}^{n}\hat{r}_{i1}x_{i3}+\sum_{i=1}^{n}\hat{r}_{i1}u_i\middle|x\right]$$
$$=\beta_1+\frac{1}{\sum_{i=1}^{n}\hat{r}_{i1}^2}\Bigg\{\beta_3\sum_{i=1}^{n}E\left[\hat{r}_{i1}x_{i3}\middle|x\right]+\sum_{i=1}^{n}E\left[\hat{r}_{i1}u_i\middle|x\right]\Bigg\}$$
$$=\beta_1+\frac{1}{\sum_{i=1}^{n}\hat{r}_{i1}^2}\Bigg\{\beta_3\sum_{i=1}^{n}\hat{r}_{i1}x_{i3}+\sum_{i=1}^{n}\hat{r}_{i1}E\left[u_i\middle|x\right]\Bigg\}$$
$$=\beta_1+\beta_3\frac{\sum_{i=1}^{n}\hat{r}_{i1}x_{i3}}{\sum_{i=1}^{n}\hat{r}_{i1}^2}$$
\item 
\begin{enumerate}
\item We must omit one of the tax share variables from the equation to avoid violating the no perfect collinearity assumption since the four shares add up to one.
\item $\beta_1$ is the constant partial effect of $share_P$ on $growth$. By itself, $\beta_1$ doesn't have much meaning because increasing a non-zero proportion by one has no meaning; however, $\beta_1/100$ can be interpreted as the partial effect of increasing the share of property taxes in total tax revenue by 1 percentage point on $growth$.
\end{enumerate}
\item 
\begin{enumerate}
\item $\tilde{\beta}_1$ is linear because it can be expressed in the form $\tilde{\beta}_1=\sum_{i=1}^{n}w_{i1}y_i$, where $w_{i1}=\frac{z_i-\bar{z}}{\sum_{i=1}^{n}\left(z_i-\bar{z}\right)x_i}$. $\tilde{\beta}_1$ can be narrowed down to
$$\tilde{\beta}_1=\frac{\sum_{i=1}^{n}\left(z_i-\bar{z}\right)y_i}{\sum_{i=1}^{n}\left(z_i-\bar{z}\right)x_i}$$
$$=\frac{\sum_{i=1}^{n}\left(z_i-\bar{z}\right)\left(\beta_0+\beta_1x_i+u_i\right)}{\sum_{i=1}^{n}\left(z_i-\bar{z}\right)x_i}$$
$$=\frac{\beta_0\sum_{i=1}^{n}\left(z_i-\bar{z}\right)+\beta_1\sum_{i=1}^{n}\left(z_i-\bar{z}\right)x_i+\sum_{i=1}^{n}\left(z_i-\bar{z}\right)u_i}{\sum_{i=1}^{n}\left(z_i-\bar{z}\right)x_i}$$
$$=\beta_1+\frac{\sum_{i=1}^{n}\left(z_i-\bar{z}\right)u_i}{\sum_{i=1}^{n}\left(z_i-\bar{z}\right)x_i}$$
Taking the conditional expectation:
$$E\left[\tilde{\beta}_1\middle|x\right]=E\left[\beta_1+\frac{\sum_{i=1}^{n}\left(z_i-\bar{z}\right)u_i}{\sum_{i=1}^{n}\left(z_i-\bar{z}\right)x_i}\middle|x\right]$$
$$=\beta_1+\frac{1}{\sum_{i=1}^{n}\left(z_i-\bar{z}\right)x_i}\sum_{i=1}^{n}E\left[\left(z_i-\bar{z}\right)u_i\middle|x\right]$$
$$=\beta_1+\frac{1}{\sum_{i=1}^{n}\left(z_i-\bar{z}\right)x_i}\sum_{i=1}^{n}\left(z_i-\bar{z}\right)E\left[u_i\middle|x\right]$$
$$=\beta_1+\frac{1}{\sum_{i=1}^{n}\left(z_i-\bar{z}\right)x_i}\sum_{i=1}^{n}\left(z_i-\bar{z}\right)\cdot 0=\beta_1$$
\item Using the previous derivation,
$$\text{Var}\left(\tilde{\beta}_1\middle|x\right)=\text{Var}\left(\beta_1+\frac{\sum_{i=1}^{n}\left(z_i-\bar{z}\right)u_i}{\sum_{i=1}^{n}\left(z_i-\bar{z}\right)x_i}\middle|x\right)$$
$$=\frac{1}{\left(\sum_{i=1}^{n}\left(z_i-\bar{z}\right)x_i\right)^2}\text{Var}\left(\sum_{i=1}^{n}\left(z_i-\bar{z}\right)u_i\middle|x\right)$$
$$=\frac{1}{\left(\sum_{i=1}^{n}\left(z_i-\bar{z}\right)x_i\right)^2}\sum_{i=1}^{n}\text{Var}\left(\left(z_i-\bar{z}\right)u_i\middle|x\right)$$
$$=\frac{1}{\left(\sum_{i=1}^{n}\left(z_i-\bar{z}\right)x_i\right)^2}\sum_{i=1}^{n}\left(z_i-\bar{z}\right)^2\text{Var}\left(u_i\middle|x\right)$$
$$=\frac{\sigma^2\left(\sum_{i=1}^{n}\left(z_i-\bar{z}\right)^2\right)}{\left(\sum_{i=1}^{n}\left(z_i-\bar{z}\right)x_i\right)^2}$$
\item Under the G-M assumptions,$$\text{Var}\left(\hat{\beta}_1\middle|x\right)=\frac{\sigma^2}{SST_x}$$Thus, we must show
$$\frac{\sigma^2}{SST_x}\leq \frac{\sigma^2\left(\sum_{i=1}^{n}\left(z_i-\bar{z}\right)^2\right)}{\left(\sum_{i=1}^{n}\left(z_i-\bar{z}\right)x_i\right)^2}$$
$$\to\frac{1}{SST_x}\leq \frac{\sum_{i=1}^{n}\left(z_i-\bar{z}\right)^2}{\left(\sum_{i=1}^{n}\left(z_i-\bar{z}\right)x_i\right)^2}$$
$$\to\frac{1}{SST_x}\leq \frac{\sum_{i=1}^{n}\left(z_i-\bar{z}\right)^2}{\left(\sum_{i=1}^{n}\left(z_i-\bar{z}\right)\left(x_i-\bar{x}\right)\right)^2}$$
From the Cauchy-Schwartz inequality, we know that $\left(\sum_{i=1}^{n}\left(z_i-\bar{z}\right)\left(x_i-\bar{x}\right)\right)^2\leq\sum_{i=1}^{n}\left(z_i-\bar{z}\right)^2\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2$

$$\to\frac{1}{SST_x}\leq \frac{\sum_{i=1}^{n}\left(z_i-\bar{z}\right)^2}{\sum_{i=1}^{n}\left(z_i-\bar{z}\right)^2\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2}$$
$$\to\frac{1}{SST_x}\leq \frac{1}{SST_x}\blacksquare$$

\end{enumerate}
\item The standard error of $\tilde{\beta}_1$ can be rewritten as
$$\text{se}\left(\tilde{\beta}_1\right)=\frac{\tilde{\sigma}}{\sqrt{SST_1}}=\sqrt{\frac{\sfrac{\sum_{i=1}^{n}\tilde{u}_i^2}{n-2}}{\sum_{i=1}^{n}\left(x_{i1}-\bar{x}_1\right)^2}}$$
Similarly the standard error of $\hat{\beta}_1$ can be rewritten as
$$\text{se}\left(\hat{\beta}_1\right)=\frac{\hat{\sigma}}{\sqrt{SST_1}}\cdot\sqrt{\text{VIF}_1}=\sqrt{\frac{\sfrac{\sum_{i=1}^{n}\hat{u}_i^2}{n-3}}{\left(1-R_1^2\right)\sum_{i=1}^{n}\left(x_{i1}-\bar{x}_1^2\right)}}$$
Thus, for a large sample size, the two important factors in determining the size of one standard error to the other is the sums of the squared residuals and the magnitude of $R_1^2$. If $x_2$ does not have a large partial effect on $y$ but is highly correlated with $x_1$, we would expect $\text{se}\left(\tilde{\beta}_1\right)<\text{se}\left(\hat{\beta}_1\right)$. On the other hand, if $x_2$ has a large partial effect on $y$ but has a small correlation with $x_1$, we would expect $\text{se}\left(\tilde{\beta}_1\right)>\text{se}\left(\hat{\beta}_1\right)$.
\item 
\begin{enumerate}
\item There are 351 degrees of freedom in the first regression and 350 in the second regression. The SER is smaller in the second regression because the SSR is `r 326.196-198.475` smaller than that of the first regression, which outweighs the loss of an extra degree of freedom.
\item A sample correlation coefficient of about 0.487 between $years$ and $rbisyr$ makes sense because those who bat more runs in tend to better players, and better players tend to play in the major leagues longer. The variance inflation factor for the slope coefficients in the multiple regression is $\frac{1}{1-0.487^2}\approx$ `r 1/(1-.487^2)`. Personally, I would say there is a moderate level of collinearity between $years$ and $rbisyr$.
\item The additional SSR in the first regression to that of the second regression is `r 326.196-198.475`, which outweighs the "cost" induced from the collinearity between $years$ and $rbisyr$.
\end{enumerate}
\item The sample size is smaller when $age$ is added to the equation, which means it's no longer guaranteed that the $R^2$ increases by adding an additional explanatory variable to the equation.
\item The estimated percentage change in $wage$ from getting one more year of schooling is 9.4\%.
\end{enumerate}

### Computer Exercises

C1)
```{r Chapter 3 Computer Exercise 1,echo=TRUE,include=TRUE,comment=NA}
#i
#beta 2 is likely positive

#ii
#Yes, cigs and faminc are likely correlated
#They may be negatively correlated because one may expect
#families who smoke more to earn less
#On the other hand, they may be positively correlated
#since wealthier families can afford more cigarettes

#iii
summary(lm(bwght ~ cigs + faminc, data = bwght))
summary(lm(bwght ~ cigs, data = bwght))
```

C2)
```{r Chapter 3 Computer Exercise 2,echo=TRUE,include=TRUE,comment=NA}
lm1 <- lm(price ~ sqrft + bdrms, data = hprice1)
#i
lm1

#ii
lm1$coefficients[3]*1000

#iii
(lm1$coefficients[3] + lm1$coefficients[2]*140)*1000

#iv
100*summary(lm1)$r.squared

#v
lm1$fitted.values[1]*1000

#vi
lm1$residuals[1]*1000
#This suggests the buyer underpaid for the house

rm(lm1)
```

C3)
```{r Chapter 3 Computer Exercise 3,echo=TRUE,include=TRUE,comment=NA}
#i
lm(lsalary ~ lsales + lmktval, data = ceosal2)

#ii
lm2 <- lm(lsalary ~ lsales + lmktval + profits, data = ceosal2)
#profits cannot be in logarithmic form because of negative profits
lm2
summary(lm2)$r.squared

#iii
lm3 <- lm(lsalary ~ lsales + lmktval + profits + ceoten, data = ceosal2)
lm3$coefficients[5] * 100

#iv
cor(ceosal2$lmktval, ceosal2$profits)
#This tells us the OLS estimators may have large variances

rm(lm2,lm3)
```

C4)
```{r Chapter 3 Computer Exercise 4,echo=TRUE,include=TRUE,comment=NA}
#i
#atndrte
min(attend$atndrte)
max(attend$atndrte)
mean(attend$atndrte)
#priGPA
min(attend$priGPA)
max(attend$priGPA)
mean(attend$priGPA)
#ACT
min(attend$ACT)
max(attend$ACT)
mean(attend$ACT)

#ii
lm1 <- lm(atndrte ~ priGPA + ACT, data = attend)
lm1
#The intercept indicates a predicted attendance rate of 75.7%
#for a student whose prior GPA is zero and ACT score is zero
#This clearly doesn't have much meaning

#iii
#The coefficient on priGPA indicates a predicted 17.3 percentage
#points higher attendance rate for every point increase in prior GPA
#The coefficient on ACT indicates a predicted 1.7 percentage
#points lower attendance rate for every additional point on the ACT
#The negative slope coefficient on ACT is a little surprising


#iv
unname(lm1$coefficients[1]+lm1$coefficients[2]*3.65+lm1$coefficients[3]*20)
sum(near(attend$priGPA,3.65,.001) & attend$ACT==20)

#v
unname(lm1$coefficients[2]+lm1$coefficients[3]*-5)

rm(lm1)
```

C5)
```{r Chapter 3 Computer Exercise 5,echo=TRUE,include=TRUE,comment=NA}
lm1 <- lm(educ ~ exper + tenure, data = wage1)
lm(lwage ~ lm1$residuals, data = wage1)

lm(lwage ~ educ + exper + tenure, data = wage1)

rm(lm1)
```

C6)
```{r Chapter 3 Computer Exercise 6,echo=TRUE,include=TRUE,comment=NA}
#i
lm1 <- lm(IQ ~ educ, data = wage2)
lm1$coefficients[2]

#ii
lm2 <- lm(lwage ~ educ, data = wage2)
lm2$coefficients[2]

#iii
lm3 <- lm(lwage ~ educ + IQ, data = wage2)
lm3$coefficients[2]
lm3$coefficients[3]

#iv
unname(near(lm2$coefficients[2],lm3$coefficients[2] + lm3$coefficients[3] * lm1$coefficients[2], .001))

rm(lm1,lm2,lm3)
```

C7)
```{r Chapter 3 Computer Exercise 7,echo=TRUE,include=TRUE,comment=NA}
#i
summary(lm(math10 ~ lexpend + lnchprg, data = meap93))

#ii
#The intercept predicts a pass rate when log(expend) and lnchprg=0
#This doesn't make sense though because setting log(expend)=0
#is the same as setting expend=1

#iii
lm(math10 ~ lexpend, data = meap93)
#The estimated spending effect is now larger

#iv
cor(meap93$lexpend, meap93$lnchprg)

#v
#The result in part iv indicates a negative correlation between
#spending and the lunch program. Additionally, part (i) shows a
#negative correlation between the lunch program and the pass rate
#Thus, the simple regression coefficient likely overstates the
#spending effect as shown by the difference in the coefficient estimates
```

C8)
```{r Chapter 3 Computer Exercise 8,echo=TRUE,include=TRUE,comment=NA}
#i
#prpblck
mean(discrim$prpblck, na.rm = T)
sd(discrim$prpblck, na.rm = T)
#income
mean(discrim$income, na.rm = T)
sd(discrim$income, na.rm = T)
#prpblck is a proportion
#income is measured in dollars

#ii
summary(lm(psoda ~ prpblck + income, data = discrim))
#The coefficient on prpblck indicates a predicted increase
#in the price of soda by about 11 cents for a 100 percentage
#point increase in the proportion of black citizens

#iii
lm(psoda ~ prpblck, data = discrim)
#The discrimination effect is larger when you control for income

#iv
lm(lpsoda ~ prpblck + lincome, data = discrim)
#If prpblck increases by .20, the estimated percentage change in
#psoda is 2.4316%

#v
lm(lpsoda ~ prpblck + lincome + prppov, data = discrim)
#The coefficient on prpblck falls

#vi
cor(discrim$lincome, discrim$prppov, use = "pairwise.complete.obs")
#Yes, I would expect a strong negative correlation between income
#and the proportion of impoverished citizens

#vii
#The statement displays a misunderstanding of multicollinearity
#Adding both to the regression may cause the variances of the estimators
#to be large, but it may also cause the variances to fall. On the other
#hand, including both reduces the amount of bias in the estimators
```

C9)
```{r Chapter 3 Computer Exercise 9,echo=TRUE,include=TRUE,comment=NA}
#i
summary(lm(gift ~ mailsyear + giftlast + propresp, data = charity))
summary(lm(gift ~ mailsyear, data = charity))
#The R-squared increases from about .01 to about .08

#ii
#The multiple regression equation predicts one more mailing per year
#increases gifts by 2.17 guilders.
#The simple regression equation predicts one more mailing per year
#increases gifts by 2.65 guilders, a larger prediction.

#iii
#Because propresp is a proportion and thus a 100 percentage point
#increase offers little meaning (unless starting at zero), we can
#instead interpret the coefficient as a predicted increase in gifts
#by .15358605 guilders for every percentage point increase in propresp

#iv
lm(gift ~ mailsyear + giftlast + propresp + avggift, data = charity)
#The coefficient on mailsyear falls from 2.166259 to 1.2012

#v
#The coefficient on giftlast falls from 0.005927 to -0.2609
#Clearly, controlling for the average gift size, it's apparent that
#the current gift amount and the most recent gift amount are negatively
#correlated
```

\newpage

# Chapter 4

## Notes

### Classical Linear Model

In addition to the Gauss-Markov assumptions, there is an additional assumption commonly made called the \textbf{normality assumption}. Under the normality assumption, we assume the population error $u$ is \emph{independent} of the explanatory variables $x_1,x_2,\dots,x_k$ and is normally-distributed with zero mean and variance $\sigma_u^2$: $u\sim\mathcal{N}\left(0,\sigma_u^2\right)$. It should be obvious that this assumption is stronger than any of the previous assumptions and that the exogeneity and homoskedasticity assumptions hold by the independence assumptions.

All together, the normality and Gauss-Markov assumptions are called the \textbf{classical linear model (CLM) assumptions}. Additionally, a model under these six assumptions are referred to as the \textbf{classical linear model}. Under the CLM assumptions, the OLS estimators $\hat{\beta}_0,\dots\hat{\beta}_k$ are the \textbf{minimum variance unbiased estimators}, meaning OLS estimators have the smallest variance among all unbiased estimators (not just linear unbiased estimators as in the G-M theorem). More succinctly, the population assumptions of CLM can be summarized by$$y|x\sim\mathcal{N}\left(\beta_0+\beta_1x_1+\dots+\beta_kx_k,\sigma_u^2\right)$$

The argument justifying the normal distribution for the errors usually invokes the CLT (because $u$ is the sum of many different unobserved factors affecting y) to suggest that $u$ has an approximate normal distribution. This argument has some merit, but it is not without weaknesses. The two main weaknesses are that the factors in $u$ can have very different distributions in the population, which may lead to "poor" normal distributions depending on how many factors appear in $u$ and how different their distributions are, and if $u$ is a complicated function of the unobserved factors, then the CLT argument doesn't really apply. Sometimes, using a transformation, such as taking the $\ln$, yields a distribution that is closer to normal, but in other cases, normality cannot be reasonably assumed. In any application, whether normality of u can be assumed is really an empirical matter. Nevertheless, we'll see later that nonnormality of the errors is not a serious problem with large sample sizes.

The important implication of the CLM assumptions is the following theorem: Under the CLM assumptions,
$$\hat{\beta}_j\sim\mathcal{N}\left(\beta_j,\sigma_u^2\right)$$
Therefore, standardizing $\hat{\beta}_j$,
$$\frac{\hat{\beta}_j-\beta_j}{\text{sd}\left(\hat{\beta}_j\right)}\sim\mathcal{N}\left(0,1\right)$$

### Hypothesis Testing about a Single Population Parameter: The $t$ Test

Directly following the previous theorem, under the CLM assumptions:
$$\frac{\hat{\beta}_j-\beta_j}{\text{se}\left(\hat{\beta}_j\right)}\sim t_{n-k-1}$$

This theorem ultimately allows for testing hypotheses about population parameters. In econometrics, if we are interested in whether an explanatory variable has an effect on the dependent variable, we state the \textbf{null hypothesis} as
$$\text{H}_0:\beta_j=0$$
and the corresponding \textbf{t statistic} or \textbf{t ratio} of $\hat{\beta}_j$ as
$$t_{\hat{\beta}_j}=\frac{\hat{\beta}_j}{\text{se}\left(\hat{\beta}_j\right)}$$

In hypothesis testing, one must also come up with a relevant \textbf{alternative hypothesis}. A \textbf{one-sided alternative} to $B_j=0$ takes the form
$$\text{H}_1:\beta_j>0$$
or
$$\text{H}_1:\beta_j<0$$
To decide on a \textbf{rejection rule}, one must decide on a \textbf{significance level} or a probability of committing a type I error (rejecting a true null hypothesis). After deciding on a rejection rule, one can obtain a \textbf{critical value}, $c$, from the $t$ distribution (or the standard normal distribution for a sufficiently large sample size). For example, if our alternative hypothesis is $\beta_j>0$, we reject the null hypothesis if$$t_{\hat{\beta}_j}>c$$ and \emph{fail to reject} the null hypothesis if $$t_{\hat{\beta}_j}\leq c$$

Similarly, a \textbf{two-sided alternative} to $B_j=0$ takes the form
$$\text{H}_1:\beta_j\neq 0$$
in which case we reject the null in favor of the alternative hypothesis if
$$|t_{\hat{\beta}_j}|>c$$
Generally, if we reject the null in favor of the alternative hypothesis (for testing against $\beta_j=0$) we say that $x_j$ is \textbf{statistically significant} or \textbf{statistically different from 0} at the specified significance level. Otherwise, we say $x_j$ is \textbf{statistically insignificant}.

Often, it's useful to compute a \textbf{p-value}, which provides the \emph{smallest} significance level at which the null hypothesis would be rejected. In other words, the p-value is the probability of observing a $t$ statistic as extreme as we did if the null hypothesis is true and is equal to
$$P\left(|T|>|t|\right),$$
where $T$ denotes a $t$ distributed random variable with $n-k-1$ degrees of freedom and $t$ denotes the numerical value of the test statistic. It's important to note that these statements for the p-value hold for two-sided tests. For one-sided tests the p-value equals
$$P\left(T>|t|\right)=P\left(|T|>|t|\right)/2$$

The CLM assumptions also make it simple to construct a \textbf{confidence interval}, which is a rule used to construct a random interval so that confidence level percentage of all data sets yields an interval that contains the population value. Confidence intervals are still dependent on the underlying assumptions, and, if these assumptions hold, take the form
$$\text{CI}=\left[\hat{\beta}_j-c\cdot\text{se}\left(\hat{\beta}_j\right),\hat{\beta}_j+c\cdot\text{se}\left(\hat{\beta}_j\right)\right]$$

###  Hypothesis Testing about a Single Linear Combination of the Parameters

It's often the case where we want to compare the coefficients on the explanatory variables. In this case, we can write the hypotheses
$$\text{H}_0: \beta_j=\beta_\ell$$
$$\text{H}_1: \beta_j\neq\beta_\ell$$
Thus, we can construct the $t$ statistic as
$$t=\frac{\hat{\beta}_j-\hat{\beta}_\ell}{\text{se}\left(\hat{\beta}_j-\hat{\beta}_\ell\right)}$$
The numerator is trivial to compute; however, the standard error in the denominator is somewhat difficult to compute. We know
$$\text{Var}\left(\hat{\beta}_j-\hat{\beta}_\ell\right)=\text{Var}\left(\hat{\beta}_j\right)+\text{Var}\left(\hat{\beta}_\ell\right)-2\text{Cov}\left(\hat{\beta}_j,\hat{\beta}_\ell\right)$$
The standard deviation of $\hat{\beta}_j-\hat{\beta}_\ell$ is just the square root of the above equation and $\left[\text{se}\left(\hat{\beta}_j\right)\right]^2$ and $\left[\text{se}\left(\hat{\beta}_\ell\right)\right]^2$ are unbiased estimators of $\text{Var}\left(\hat{\beta}_j\right)$ and $\text{Var}\left(\hat{\beta}_\ell\right)$. Thus we have,
$$\text{se}\left(\hat{\beta}_j-\hat{\beta}_\ell\right)=\sqrt{\left[\text{se}\left(\hat{\beta}_j\right)\right]^2+\left[\text{se}\left(\hat{\beta}_\ell\right)\right]^2-2\hat{\sigma}_{\hat{\beta}_j,\hat{\beta}_\ell}}$$
An alternative to the approach involves rewriting the model. If our model starts as
$$y=\beta_0+\beta_1 x_1+\dots+\beta_j x_j+\dots+\beta_\ell x_\ell+\dots+\beta_k x_k+u$$
$$\to y=\beta_0+\beta_1 x_1+\dots+\beta_j x_j+\dots+\beta_\ell x_\ell+\dots+\beta_k x_k+u+\beta_\ell x_j-\beta_\ell x_j$$
$$\to y=\beta_0+\beta_1 x_1+\dots+\beta_j x_j-\beta_\ell x_j+\dots+\beta_\ell x_\ell+\beta_\ell x_j+\dots+\beta_k x_k+u$$
$$\to y=\beta_0+\beta_1 x_1+\dots+\left(\beta_j-\beta_\ell\right) x_j+\dots+\beta_\ell\left(x_\ell+x_j\right)+\dots+\beta_k x_k+u$$
$$\to y=\beta_0+\beta_1 x_1+\dots+\left(\beta_j-\beta_\ell\right) x_j+\dots+\beta_\ell\left(x_j+x_\ell\right)+\dots+\beta_k x_k+u$$
Creating a new variable $x_j+x_\ell$, it's easy to estimate the coefficient on $x_j$ and evaluate the desired $t$ statistic.

### Hypothesis Testing Multiple Linear Restrictions: The $F$ Test

In many cases, we are interested in whether a group of variables has no effect on the dependent variable. In these cases, we state the hypotheses as
$$\text{H}_0:\beta_{k-q+1}=0,\dots,\beta_k=0$$
$$\text{H}_1:\text{H}_0\text{ is not true}$$
The null hypothesis consists of $q$ \textbf{exclusion restrictions}. This exemplifies \textbf{multiple restrictions} because we are putting more than one restriction on the parameters. Thus, such a test in which multiple restrictions are tested is called a \textbf{multiple hypotheses test} or a \textbf{joint hypotheses test}. In multiple hypotheses tests, the alternative holds if at least one of $\beta_{k-q+1},\dots,\beta_k$ is different from zero.

In the context of multiple hypothesis testing, we define two models. One model is the \textbf{restricted model}, which takes the form
$$y=\beta_0+\beta_1 x_1+\dots+\beta_{k-q}x_{k-q}+u$$
And the other model is the \textbf{unrestricted model}, which takes the form
$$y=\beta_0+\beta_1 x_1+\dots+\beta_{k}x_{k}+u$$
Using these two models, we can construct the \textbf{F statistic}, which is defined by
$$F\equiv\frac{\sfrac{\left(\text{SSR}_r-\text{SSR}_{ur}\right)}{q}}{\sfrac{\text{SSR}_{ur}}{n-k-1}}=\frac{\sfrac{\left(R^2_{ur}-R^2_r\right)}{q}}{\sfrac{\left(1-R^2_{ur}\right)}{n-k-1}},$$
where the $ur$ denotes the unrestricted model and the $r$ denotes the restricted model. The second equality is called the \textbf{R-squared form of the F statistic}.

It should immediately be obvious that $F\geq 0$ because $\text{SSR}_{ur}\leq\text{SSR}_r$. The easiest way to remember where the SSRs appear is to think of $F$ as measuring the relative increase in SSR when moving from the unrestricted to the restricted model.

The difference in SSRs in the numerator of $F$ is divided by $q$, which is the number of restrictions imposed in moving from the unrestricted to the  restricted model (q independent variables are dropped). Thus, $q=\text{numerator df}=df_r-df_{ur}$

The SSR in the denominator of $F$ is divided by the degrees of freedom in the unrestricted model which equals $n-k-1=\text{denominator df}=df_{ur}$. It should be noted that the denominator of $F$ is simply the unbiased estimator of $\sigma_u^2$ in the unrestricted model.

Under the null hypothesis and the CLM assumptions, $F$ is distributed as an $F$ random variable with $(q,n-k-1)$ degrees of freedom. This can be written as
$$F\sim F_{q,n-k-1}$$
Similar to the case of the $t$ test, after a significance level is decided on, we reject $\text{H}_0$ in favor of $\text{H}_1$ if $$F>c$$ and fail to reject $\text{H}_0$ otherwise. If $\text{H}_0$ is rejected, then we say that $x_{k-q+1},\dots,x_k$ are \textbf{jointly statistically significant} at the appropriate significance level. If the null is not rejected, then the variables are \textbf{jointly insignificant}, which often justifies dropping them from the model.

One should be careful in interpreting joint hypotheses using $t$ statistics on individual parameters. In many cases, using the $F$ statistic and individually evaluating $t$ statistics yields different results. This generally happens when there is multicollinearity that prevents uncovering statistically significant partial effects of individual explanatory variables. The $F$ statistic tests whether explanatory variables are \emph{jointly} significant, and multicollinearity between explanatory variables is much less relevant for testing this hypothesis. Conversely, it is also possible that, in a group of several explanatory variables, one variable has a significant $t$ statistic but the group of variables is jointly insignificant at the usual significance levels. Generally, such cases occur when a bunch of insignificant variables are grouped with a significant variable, leading to joint insignificance. Another caution to take when using the $F$ statistic is making sure the same observations are used across the restricted and unrestricted models. When estimating the restricted model to compute an $F$ test, we must use the same observations to estimate the unrestricted model; otherwise, the test is not valid. Obviously, when there are no missing data, this is not an issue.

In the context of $F$ tests, the p-value is defined as
$$p=P(\mathscr{F}>F),$$
where $\mathscr{F}$ denotes an $F$ random variable with $(q,n-k-1)$ degrees of freedom and $F$ is the actual value of the test statistic. As with the case for the $t$ test, the p-value is the probability of observing a value of $F$ at least as large as we did, given that the null hypothesis is true.

In some cases, we're interested in testing whether \emph{none} of the explanatory variables has an effect on the dependent variable. In such cases the null takes the form
$$\text{H}_0:\beta_1=\beta_2=\dots=\beta_k=0$$
and the alternative is that at least one of the slope coefficients is different from zero. Thus, there are $k$ restriction imposed and the restricted model becomes
$$y=\beta_0+u$$
Since all of the explanatory variables have been dropped, $R_r^2=0$, and the $F$ statistic can be written as
$$F=\frac{R_{ur}^2/k}{\left(1-R_{ur}^2\right)/ \left(n-k-1\right)}$$
This process of testing joint exclusion of all the explanatory variables is sometimes called determining the \textbf{overall significance of the regression}.

Finally, we can extend the use of the $F$ statistic to test general linear restrictions. Suppose we are interested in testing the null
$$\text{H}_0: \beta_1=1,\beta_2=\dots=\beta_k=0$$
Then the unrestricted model is simply
$$y=\beta_0+\beta_1x_1+\dots+\beta_kx_k+u$$
and the restricted model is
$$y=\beta_0+1\cdot x_1+u,$$
which can be rewritten as
$$y-x_1=\beta_0+u$$
The process of computing the $F$ statistic is the same as in previous problems with one point of caution. Since the dependent variable in the restricted model is different than that of the unrestricted model, we can no longer use the $R^2$ form of the $F$ statistic and must refer to the SSR form. As a general rule, the SSR form of the $F$ statistic should be used if a different dependent variable is needed in running the restricted regression.

## Exercises

### Problems

\begin{enumerate}
\item (i) Heteroskedasticity and (iii) Omitting an important explanatory variable can cause the usual OLS $t$ statistics to be invalid. Multicollinearity does not violate any of the CLM assumptions.
\item 
\begin{enumerate}
\item $$\text{H}_0:\beta_3=0$$
$$\text{H}_1:\beta_3 > 0$$
\item $salary$ is predicted to increase by `r .00024*50*100`\% if $ros$ increases by 50 points. No, $ros$ does not have a practically large effect on $salary$.
\item The $t$ statistic on $ros$ is approximately `r .00024/.00054`, which is below the one-sided critical value at the 10\% significance level. Thus, we fail to reject $H_0$ at the 10\% significance level.
\item Based on the provided results, $ros$ does not have a statistically or practically significant impact on $salary$. Thus, I would not include it in a final model explaining CEO compensation in terms of firm performance.
\end{enumerate}
\item 
\begin{enumerate}
\item The coefficient on $\ln(sales)$ indicates, if sales increases by 10\%, the estimated percentage point change in $rdintens$ is .0321. This is not an economically large effect.
\item The $t$ statistic on $sales$ (or $\ln(sales)$) is approximately `r .321/.216`. The critical value at the 10\% significance level is `r qt(p=.1, df=29, lower.tail= FALSE)`, and the critical value at the 5\% significance level is `r qt(p=.05, df=29, lower.tail= FALSE)`. Thus, we reject $H_0$ at the 10\% significance level but fail to reject $H_0$ at the 5\% significance level.
\item The coefficient on $profmarg$ indicates a predicted .050 increase in $rdintens$ for every percentage point increase in $profmarg$. Again, this effect does not seem economically large.
\item No, the one-sided p-value is approximately `r pt(.050/.046, 29, lower.tail= FALSE)`. Thus, $profmarg$ does not have a statistically significant effect on $rdtens$ at every conventional significance level based on these results.
\end{enumerate}
\item 
\begin{enumerate}
\item $$\text{H}_0:\beta_3 = 0$$
$$\text{H}_1:\beta_3 \neq 0$$
\item I expect $\beta_1$ and $\beta_2$ to be positive as greater income and population values shift demand curves right, which increases equilibrium prices.
\item The elasticity in the statement is off by a decimal place. A more accurate statement would be: "Controlling for the average city income and the student population as a percentage of the total population, a 10\% increase in population is associated with a 0.66\% increase in $rent$."
\item The two-sided p-value is approximately `r pt(.0056/.0017, df=60, lower.tail = FALSE)`. Thus, we reject $H_0$ at the 1\% significance level.
\end{enumerate}
\item 
\begin{enumerate}
\item $CI_{95}=[.412\pm `r qnorm(.025, lower.tail = TRUE)`\cdot .094]=[`r .412 + qnorm(.025, lower.tail = TRUE) * .094`,`r .412 + qnorm(.025, lower.tail = FALSE) * .094`]$
\item No, 0.4 is within the bounds of the confidence interval constructed in part (a).
\item Yes, 1 is not within the bounds of the confidence interval constructed in part (a).
\end{enumerate}
\item 
\begin{enumerate}
\item The p-value associated with the hypothesis that $\beta_0 = 0$ is `r pt((-14.47/16.27)/2, df = 86, lower.tail = FALSE)`.
The p-value associated with the hypothesis that $\beta_1 = 1$ is `r pt(((.976 - 1)/.049)/2, df = 86, lower.tail = FALSE)`. Thus, in both cases, we fail to reject $H_0$ at every conventional significance level.
\item $F=\frac{(209,448.99-165,644.51)/2}{165,644.51/(88-2)}\approx$`r ((209448.99-165644.51)/2)/(165644.51/(88-2))` and the associated p-value is `r pf(((209448.99-165644.51)/2)/(165644.51/(88-2)), df1 = 2, df2 = 86, lower.tail = FALSE)` in which case we reject $H_0$ in favor of $H_1$ at every conventional significance level.
\item $F=\frac{(.829-.820)/3}{(1-.829)/86}\approx$`r ((.829-.820)/3)/((1-.829)/86)`. The associated p-value is `r pf(((.829-.820)/3)/((1-.829)/81),df1 = 3, df2 = 81, lower.tail = FALSE)`. Thus, we fail to reject $H_0$ at every conventional significance level.
\item The $F$ test is no longer valid because this notion violates the homoskedasticity assumption.
\end{enumerate}
\item 
\begin{enumerate}
\item In the equation estimated in example 4.7, the $t$ statistics are: $t_{hrsemp}=`r -.029/.023`, t_{sales}=`r -.962/.453`, t_{employ}=`r .761/.407`$.

In the equation estimated in the problem, the $t$ statistics are: $t_{hrsemp}=`r -.042/.019`, t_{sales}=`r -.951/.370`, t_{employ}=`r .992/.360`$.
\item $$\ln(scrap)=\beta_0+\beta_1 hrsemp+\beta_2\ln(sales)+\beta_3\ln(employ)+u$$
$$\to\ln(scrap)=\beta_0+\beta_1 hrsemp+\beta_2\ln(sales)+\beta_3\ln(employ)+u-\beta_2\ln(employ)+\beta_2\ln(employ)$$
$$\to\ln(scrap)=\beta_0+\beta_1 hrsemp+\beta_2\left[\ln(sales)-\ln(employ)\right]+(\beta_2+\beta_3)\ln(employ)+u$$
$$\to\ln(scrap)=\beta_0+\beta_1 hrsemp+\beta_2\ln(sales/employ)+\theta_3\ln(employ)+u$$
\item No, the $t$ statistic on $employ$ is approximately `r .041/.205`. The corresponding one-sided p-value is `r pt(.041/.205, df = 39, lower.tail = FALSE)`. Thus, at every conventional significance level, we would fail to reject the null hypothesis that $\theta_3=0$.
\item $t_{sales/employ}=\frac{-.951-(-1)}{.370}\approx$`r (1-.951)/.370`. The corresponding one-sided p-value is about `r pt((1-.951)/.370, df = 39, lower.tail = TRUE)`. Thus, at every conventional significance level, we fail to reject the null hypothesis that $\beta_2=-1$.
\end{enumerate}
\item 
\begin{enumerate}
\item $$\text{Var}\left(\hat{\beta}_1-3\hat{\beta}_2\right)=\text{Var}\left(\hat{\beta}_1\right)+9\text{Var}\left(\hat{\beta}_2\right)-6\text{Cov}\left(\hat{\beta}_1,\hat{\beta}_2\right)$$
$$\text{se}\left(\hat{\beta}_1-3\hat{\beta}_2\right)=\sqrt{\hat{\sigma}_{\hat{\beta}_1}^2+9\hat{\sigma}_{\hat{\beta}_2}^2-6\hat{\sigma}_{\hat{\beta}_1\hat{\beta}_2}}$$
\item $$t=\frac{\hat{\beta}_1-3\hat{\beta}_2-1}{\text{se}\left(\hat{\beta}_1-3\hat{\beta}_2\right)}=\frac{\hat{\beta}_1-3\hat{\beta}_2-1}{\sqrt{\hat{\sigma}_{\hat{\beta}_1}^2+9\hat{\sigma}_{\hat{\beta}_2}^2-6\hat{\sigma}_{\hat{\beta}_1\hat{\beta}_2}}}$$
\item $$y=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3+u$$
$$\to y=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3+u-3\beta_2x_1+3\beta_2x_1$$
$$\to y=\beta_0+(\beta_1-3\beta_2)x_1+\beta_2(3x_1+x_2)+\beta_3x_3+u$$
$$\to y=\beta_0+\theta_1x_1+\beta_2(3x_1+x_2)+\beta_3x_3+u$$
\end{enumerate}
\item 
\begin{enumerate}
\item $t_{educ}\approx `r -11.13/5.88`$, which is not below the 5\% two-sided critical value of `r qt(.025,df=702,lower.tail = T)`. $t_{age}\approx `r 2.2/1.45`$, which does not exceed the 5\% two-sided critical value of `r qt(.025,df=702,lower.tail = F)`. Thus, neither $educ$ nor $age$ is individually significant at the 5\% level against a two-sided alternative.
\item $F=\frac{(.113-.103)/2}{(1-.113)/702}\approx `r ((.113-.103)/2)/((1-.113)/702)`$. The corresponding 5\% critical value is about `r qf(.05, df1=2, df2 = 702, lower.tail = F)`. Thus, we fail to reject $H_0$ that $educ$ and $age$ are jointly insignificant at the 5\% level.
\item No, omitting $age$ and $educ$ from the equation only changes the coefficient on $totwrk$ from -.148 to -.151.
\item Heteroskedasticity implies the tests computed in parts (a) and (b) are no longer "valid" and the computed test statistics do not have much meaning.
\end{enumerate}
\item 
\begin{enumerate}
\item $F=\frac{(.0395-0)/4}{(1-.0395)/137}\approx `r (.0395/4)/((1-.0395)/137)`$, which is well below the 5\% level critical value. Thus, we cannot reject the null that the explanatory variables are jointly insignificant at the 5\% level.

The largest $t$ statistic (in absolute value) on the explanatory variables is that on $dkr$, where $t_{dir}\approx `r .321/.201`$. The corresponding two-sided critical value at the 5\% level is `r qt(.025, df=137, lower.tail=F)`. Thus, none of the explanatory variables is individually significant at the 5\% level.
\item After reestimating the model, $F=\frac{(.0330-0)/4}{(1-.0330)/137}\approx `r (.0330/4)/((1-.0330)/137)`$, which is still well below the 5\% level critical value. Thus, we still cannot reject the null that the explanatory variables are jointly insignificant at the 5\% level.

The largest $t$ statistic (in absolute value) on the explanatory variables is  still that on $dkr$, where $t_{dir}\approx `r .327/.203`$. The corresponding two-sided critical value at the 5\% level is `r qt(.025, df=137, lower.tail=F)`. Thus, none of the explanatory variables is individually significant at the 5\% level. In total, none of the conclusions change.
\item No, one should not use natural logarithm form for explanatory variables that take on the value 0.
\item  Overall, the evidence for predictability of stock returns is weak.
\end{enumerate}
\item 
\begin{enumerate}
\item The partial effect of $profmarg$ on CEO salary is not statistically significant at any conventional significance level, regardless of whether we control for $ceoten$ and $comten$ or not.
\item Controlling for $sales$, $profmar$, $ceoten$, and $comten$, $t_{mktval}\approx `r .100/.049`$. The corresponding two-sided critical value at the 5\% level is approximately `r qt(.025, df = 171, lower.tail = F)`. Thus, we reject the null that market value does not have an effect on CEO salary at the 5\% level in favor of the two-sided alternative.
\item The coefficient on $ceoten$ indicates a predicted 1.71\% increase in a CEO's salary for an additional year as the CEO with the current company. The coefficient on $comten$ indicates a predicted 0.92\% decrease in a CEO's salary for an additional year with the current company.

The $t$ statistic on $ceoten$ is approximately `r .0171/.0055`, and  the $t$ statistic on $comten$ is approximately `r -.0092/.0033`. Individually, these explanatory variables are statistically significant at the 5\% significance level.

Testing the hypothesis that the explanatory variables are jointly insignificant, the $F$ statistic is about `r ((.353-.304)/2)/((1-.353)/171)`, which provides strong evidence that these explanatory variables are jointly statistically significant.
\item It seems odd that longer tenure with the company, holding the other factors fixed, is associated with a lower salary. I suppose, since we're controlling for $ceoten$, a larger $comten$ implies less experience as a CEO, which may be the reason we see this association.
\end{enumerate}
\end{enumerate}

### Computer Exercises

C1)
```{r Chapter 4 Computer Exercise 1,echo=TRUE,include=TRUE,comment=NA}
#i
#beta_1/100 is the partial effect on voteA from a 1% increase in expendA

#ii
#H_0: beta_1+beta_2=0

#iii
summary(lm(voteA ~ lexpendA + lexpendB + prtystrA, data = vote1))$coefficients[,1:3]

#iv
VOTE1 <- as_tibble(vote1) %>%
  mutate(
    lexpend_diff = lexpendB - lexpendA
  )

summary(lm(voteA ~ lexpendA + lexpend_diff + prtystrA, data = VOTE1))$coefficients[,1:3]

rm(VOTE1)
```

C2)
```{r Chapter 4 Computer Exercise 2,echo=TRUE,include=TRUE,comment=NA}
lm1 <- lm(lsalary ~ LSAT + GPA + llibvol + lcost + rank, data = lawsch85)

#i
#H_0: beta_5=0
summary(lm1)$coefficients[6,]

#ii
summary(lm1)$coefficients[2:3,]
linearHypothesis(lm1, c("LSAT = 0", "GPA = 0"))

#iii
lm2 <- lm(lsalary ~ LSAT + GPA + llibvol + lcost + rank + clsize + faculty, data = lawsch85)
linearHypothesis(lm2, c("clsize = 0", "faculty = 0"))

#iv
#Other factors may include history of the school and the demographics

rm(lm1, lm2)
```

C3)
```{r Chapter 4 Computer Exercise 3,echo=TRUE,include=TRUE,comment=NA}
#i
HPRICE1 <- as_tibble(hprice1) %>%
  mutate(
    bdrms_150 = sqrft - 150 * bdrms
  )
lm1 <- lm(lprice ~ bdrms_150 + bdrms, data = HPRICE1)

summary(lm1)$coefficients[3,]

#ii
#beta_2=theta_1-150*beta_1

#iii
confint(lm1)[3,]

rm(lm1, HPRICE1)
```

C4)
```{r Chapter 4 Computer Exercise 4,echo=TRUE,include=TRUE,comment=NA}
summary(lm(bwght ~ cigs + parity + faminc + motheduc + fatheduc, data = bwght))["r.squared"]
```

C5)
```{r Chapter 4 Computer Exercise 5,echo=TRUE,include=TRUE,comment=NA}
#i
summary(lm(lsalary ~ years + gamesyr + bavg + hrunsyr, data = mlb1))$coefficients[5,]

#ii
lm1 <- lm(lsalary ~ years + gamesyr + bavg + hrunsyr + runsyr + fldperc + sbasesyr, data = mlb1)
summary(lm1)$coefficients[6:8,]

#iii
linearHypothesis(lm1, c("bavg = 0", "fldperc = 0", "sbasesyr = 0"))
rm(lm1)
```

C6)
```{r Chapter 4 Computer Exercise 6,echo=TRUE,include=TRUE,comment=NA}
#i
#beta_2-beta_3=0

WAGE2 <- as_tibble(wage2) %>%
  mutate(
    tenure_exper = tenure + exper
  )

#ii
confint(lm(lwage ~ educ + exper + tenure_exper, data = WAGE2))[3,]

rm(WAGE2)
```

C7)
```{r Chapter 4 Computer Exercise 7,echo=TRUE,include=TRUE,comment=NA}
#i
min(twoyear$phsrank)
max(twoyear$phsrank)
mean(twoyear$phsrank)

#ii
summary(lm(lwage ~ jc + totcoll + exper + phsrank, data = twoyear))$coefficients[5,]

#iii
#Adding phsrank makes the absolute value of the t statistic on jc smaller

#iv
#id a worker identification number. Assuming random assignment,
#it shouldn't be correlated with any of the variables
summary(lm(lwage ~ jc + totcoll + exper + phsrank + id, data = twoyear))$coefficients[6,]
```

C8)
```{r Chapter 4 Computer Exercise 8,echo=TRUE,include=TRUE,comment=NA}
K401KSUBS <- as_tibble(k401ksubs) %>%
  filter(fsize == 1)

#i
nrow(K401KSUBS)

#ii
lm1 <- lm(nettfa ~ inc + age, data = K401KSUBS)
summary(lm1)$coefficients[,1:3]

#iii
#The intercept does not have much meaning
#It predicts the net financial wealth for a (single-person) household
#Where the survey respondent is 0 years old and has no income

#iv
pt((lm1$coefficients[[3]]-1) / coefficients(summary(lm1))[, "Std. Error"][[3]], df = 2014, lower.tail = TRUE)

#v
lm(nettfa ~ inc, data = K401KSUBS)
cor(K401KSUBS$age,K401KSUBS$inc)
#The small level of correlation is why the coefficient doesn't change much

rm(K401KSUBS, lm1)
```

C9)
```{r Chapter 4 Computer Exercise 9,echo=TRUE,include=TRUE,comment=NA}
#i
summary(lm(lpsoda ~ prpblck + lincome + prppov, data = discrim))$coefficients

#ii
cor(discrim$lincome, discrim$prppov, use = "pairwise.complete.obs")

#iii
lm1 <- lm(lpsoda ~ prpblck + lincome + prppov + lhseval, data = discrim)
summary(lm1)$coefficients

#iv
linearHypothesis(lm1, c("lincome = 0", "prppov = 0"))

#v
#Because the regression equation in part (iii) controls for more variables,
#it's likely to be more accurate
rm(lm1)
```

C10)
```{r Chapter 4 Computer Exercise 10,echo=TRUE,include=TRUE,comment=NA}
#i
confint(lm(lavgsal ~ bs, data = elem94_95))[2,]

#ii
summary(lm(lavgsal ~ bs + lenrol + lstaff, data = elem94_95))$coefficients[2,]

#iii
#The sum of squared residuals "effect" exceeds that from correlation between the
#explanatory variables in the sample

#iv
#The negative coefficient indicates an association between smaller classes
#and lower teacher salaries

#v
summary(lm(lavgsal ~ bs + lenrol + lstaff + lunch, data = elem94_95))$coefficients[5,]

#vi
#Yes. These results are very similar to those in Table 4.1
```

\newpage

# Chapter 5

## Notes

### Consistency of OLS Estimators

\textbf{Asymptotic} or \textbf{large sample properties} of estimators and test statistics are properties that are undefined for a particular sample size but are defined as the sample size grows without bound. Among the most important of the asymptotic properties for an estimator is \textbf{consistency}. Partially because we cannot always obtain unbiased estimators, not all useful estimators are unbiased. However, virtually all economists agree that consistency is a minimal requirement for an estimator. Luckily, under the same set of assumptions that allow OLS estimators to be unbiased also allow OLS estimators to be consistent.

\underline{Consistency of OLS Estimators (SLR)}

Previously, we've derived
$$\hat{\beta}_1=\beta_1+\frac{\sum_{i=1}^{n}\left(x_i-\bar{x}\right)u_i}{\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2}$$
Invoking properties of probability limits and the law of large numbers we can show
$$\text{plim}\left(\hat{\beta}_1\right)=\text{plim}\left(\beta_1+\frac{n^{-1}\sum_{i=1}^{n}\left(x_i-\bar{x}\right)u_i}{n^{-1}\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2}\right)$$
$$=\text{plim}\left(\beta_1\right)+\frac{\text{plim}\left(n^{-1}\sum_{i=1}^{n}\left(x_i-\bar{x}\right)u_i\right)}{\text{plim}\left(n^{-1}\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2\right)}$$
$$=\beta_1+\frac{\text{Cov}\left(x,u\right)}{\text{Var}\left(x\right)}=\beta_1$$
since $\text{Cov}(x,u)=0$ under the zero conditional mean assumption. In proving the consistency of OLS estimators, the zero conditional mean assumption can be relaxed to a similar yet different assumption that $E[u]=0$ and $\text{Cov}(x_j,u)=0$ for $j=1,\dots,k$. One way to characterize the zero conditional mean assumption is that any function of the explanatory variables is uncorrelated with $u$. The zero mean and zero correlation assumption requires only that each $x_j$ is uncorrelated with $u$ (and that u has a zero mean in the population). While the zero mean and zero correlation assumption is more natural, it should be noted that OLS estimators are only consistent and not unbiased since we can no longer assume a zero conditional mean of the error term. Another fault of the zero mean and zero correlation assumption is that we can no longer assume we have properly modeled the population regression function (PRF). Under the zero conditional mean assumption, we needn't worry that $u$ is correlated with some nonlinear function of the explanatory variables; however, the zero mean and zero correlation makes no such guarantee. Such situations mean that we have neglected nonlinearities in the model that could help us better explain $y$. Nevertheless, the weaker zero correlation assumption turns out to be useful in interpreting OLS estimation of a linear model as providing the best linear approximation to the PRF.
```{r Proof Of Consistency for OLS Estimators, echo=FALSE}
# \underline{Proof of Consistency for OLS Estimators}
# 
# Previously, we've derived
# $$\hat{\beta}_\ell=\frac{\sum_{i=1}^{n}\hat{r}_{i\ell}y_i}{\sum_{i=1}^{n}\hat{r}_{i\ell}}=\beta_\ell+\frac{\sum_{i=1}^{n}\hat{r}_{i\ell}u_i}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}$$
# $$=\beta_\ell+\frac{\sum_{i=1}^{n}\left(x_{i\ell}-\hat{x}_{i\ell}\right)u_i}{\sum_{i=1}^{n}\left(x_{i\ell}-\hat{x}_{i\ell}\right)^2}$$
# $$=\beta_\ell+\frac{\sum_{i=1}^{n}\left(x_{i\ell}-\left(\hat{\gamma}_0+\hat{\gamma}_1x_{i1}+\dots+\hat{\gamma}_{\ell-1}x_{i\ell-1}+\hat{\gamma}_{\ell+1}x_{i\ell+1}+\dots+\hat{\gamma}_{k}x_{ik}\right)\right)u_i}{\sum_{i=1}^{n}\left(x_{i\ell}-\hat{x}_{i\ell}\right)^2}$$
# $$=\beta_\ell+\frac{\sum_{i=1}^{n}\left(x_{i\ell}u_i-\hat{\gamma}_0u_i-\hat{\gamma}_1x_{i1}u_i-\dots-\hat{\gamma}_{\ell-1}x_{i\ell-1}u_i-\hat{\gamma}_{\ell+1}x_{i\ell+1}u_i-\dots-\hat{\gamma}_{k}x_{ik}u_i\right)}{\sum_{i=1}^{n}\left(x_{i\ell}-\hat{x}_{i\ell}\right)^2}$$
# $$=\beta_\ell+\frac{\sum_{i=1}^{n}x_{i\ell}u_i-\hat{\gamma}_0\sum_{i=1}^{n}u_i-\hat{\gamma}_1\sum_{i=1}^{n}x_{i1}u_i-\dots-\hat{\gamma}_{\ell-1}\sum_{i=1}^{n}x_{i\ell-1}u_i-\hat{\gamma}_{\ell+1}\sum_{i=1}^{n}x_{i\ell+1}u_i-\dots-\hat{\gamma}_{k}\sum_{i=1}^{n}x_{ik}u_i}{\sum_{i=1}^{n}\left(x_{i\ell}-\hat{x}_{i\ell}\right)^2}$$
# $$=\beta_\ell+\frac{\sum_{i=1}^{n}x_{i\ell}u_i-\hat{\gamma}_0\sum_{i=1}^{n}u_i-\hat{\gamma}_1\sum_{i=1}^{n}x_{i1}u_i-\dots-\hat{\gamma}_{\ell-1}\sum_{i=1}^{n}x_{i\ell-1}u_i-\hat{\gamma}_{\ell+1}\sum_{i=1}^{n}x_{i\ell+1}u_i-\dots-\hat{\gamma}_{k}\sum_{i=1}^{n}x_{ik}u_i}{\sum_{i=1}^{n}\left(x_{i\ell}-\hat{x}_{i\ell}\right)^2}$$
# $$=\beta_\ell+\frac{n^{-1}\Big\{\sum_{i=1}^{n}x_{i\ell}u_i-\hat{\gamma}_0\sum_{i=1}^{n}u_i-\hat{\gamma}_1\sum_{i=1}^{n}x_{i1}u_i-\dots-\hat{\gamma}_{\ell-1}\sum_{i=1}^{n}x_{i\ell-1}u_i-\hat{\gamma}_{\ell+1}\sum_{i=1}^{n}x_{i\ell+1}u_i-\dots-\hat{\gamma}_{k}\sum_{i=1}^{n}x_{ik}u_i\Big\}}{n^{-1}\Big\{\sum_{i=1}^{n}\left(x_{i\ell}-\hat{x}_{i\ell}\right)^2\Big\}}$$
# Through the properties of probabilities limits and the use of the LLN, we can conclude
# $$\text{plim}\left(\hat{\beta}_\ell\right)=\text{plim}\left(\beta_\ell+\frac{n^{-1}\Big\{\sum_{i=1}^{n}x_{i\ell}u_i-\hat{\gamma}_0\sum_{i=1}^{n}u_i-\hat{\gamma}_1\sum_{i=1}^{n}x_{i1}u_i-\dots-\hat{\gamma}_{\ell-1}\sum_{i=1}^{n}x_{i\ell-1}u_i-\hat{\gamma}_{\ell+1}\sum_{i=1}^{n}x_{i\ell+1}u_i-\dots-\hat{\gamma}_{k}\sum_{i=1}^{n}x_{ik}u_i\Big\}}{n^{-1}\Big\{\sum_{i=1}^{n}\left(x_{i\ell}-\hat{x}_{i\ell}\right)^2\Big\}}\right)$$
# $$=\beta_\ell+\frac{E\left[x_{i\ell}u_i\right]-E\left[\hat{\gamma}_0u_i\right]-E\left[\hat{\gamma}_1x_{i1}u_i\right]-\dots-E\left[\hat{\gamma}_{\ell-1}x_{i\ell-1}u_i\right]-E\left[\hat{\gamma}_{\ell+1}x_{i\ell+1}u_i\right]-\dots-E\left[\hat{\gamma}_{k}x_{ik}u_i\right]}{\text{Var}\left(x_{i\ell}\right)}$$
```


### Derivation of the Inconsistency in OLS Estimators

In the case of simple linear regression, deriving the \textbf{inconsistency} (sometimes referred to as \textbf{asymptotic bias}) is
$$\text{plim}\left(\hat{\beta}_1\right)-\beta_1=\beta_1+\text{Cov}(x_1,u)/\text{Var}(x_1)-\beta_1=\text{Cov}(x_1,u)/\text{Var}(x_1)$$
The direction of inconsistency is thus solely dependent of the direction of the correlation between $x_1$ and $u$. In cases of small correlation between $x_1$ and $u$ relative to the variance in $x_1$, the level of inconsistency will be small. Unfortunately, we can't even estimate how big this relative proportion is because $u$ is unobserved.

Deriving the inconsistency in OLS estimators in multiple regression analysis is synonymous to deriving omitted variable bias. Define a model $\hat{y}=\hat{\beta}_0+\hat{\beta}_1x_1+\dots+\hat{\beta}_kx_k$ and another model $\tilde{y}=\tilde{\beta}_0+\tilde{\beta}_1x_1+\dots+\tilde{\beta}_{k-1}x_{k-1}$, then$$\tilde{\beta}_j=\hat{\beta}_j+\hat{\beta}_k\tilde{\delta}_j$$for $j=1,\dots,k-1$. Using probability limit properties:
$$\text{plim}\left(\tilde{\beta}_j\right)=\text{plim}\left(\hat{\beta}_j\right)+\text{plim}\left(\hat{\beta}_k\tilde{\delta}_j\right)=\beta_j+\text{plim}\left(\hat{\beta}_k\right)\text{plim}\left(\tilde{\delta}_j\right)=\beta_j+\beta_k\delta_j$$
Thus, we can derive the inconsistency as
$$\text{Inconsistency}\left(\tilde{\beta}_j\right)=\text{plim}\left(\hat{\beta}_j\right)-\beta_j=\beta_j+\beta_k\delta_j-\beta_j=\beta_k\delta_j$$
where
$$\delta_j=\text{Cov}\left(x_j,x_k\right)/\text{Var}\left(x_j\right)$$
The similarity between the bias and inconsistency from omitting key variables is evident. The difference is that the inconsistency is expressed in terms of the population variance of $x_j$ and the population covariance between $x_j$ and $x_k$, while the bias is based on their sample counterparts as we condition on the values of $x_j$ and $x_k$ in the sample.

\underline{Summary of Inconsistency in $\tilde{\beta}_j$ when $x_k$ is Omitted}
\begin{tabular}{|lll|}
\hline
 & $\text{Corr}(x_j,x_k)>0$ & $\text{Corr}(x_j,x_k)<0$ \\
\hline
$\beta_k>0$ & Positive Inconsistency & Negative Inconsistency \\
$\beta_k<0$ & Negative Inconsistency & Positive Inconsistency \\
\hline
\end{tabular}

As with the case in deriving bias from omitting key variables, deriving the sign and magnitude of the inconsistency in the case where several variables are omitted is harder. It's important to remember that if we have the model $y=\beta_0+\beta_1x_1+\dots+\beta_kx_k+u$ where any of the regressors is correlated with $u$ but the other explanatory variables are uncorrelated with $u$, \emph{all} of the OLS estimators are generally inconsistent. However, if $x_j$ is uncorrelated with $x_\ell$ (and $x_\ell$ is uncorrelated with $u$), then $\beta_\ell$ remains consistent despite any correlation between $x_j$ and $u$. All of these facts are directly synonymous to those discussed for omitted variable bias.

### Asymptotic Normality and Large Sample Inference

Consistency of an estimator is an important property, but it alone does not allow us to perform statistical inference. As discussed in the previous chapter, under the classical linear model assumptions, the sampling distributions are normal. Recall that the normality of the OLS estimators hinges on the normality of the distribution of the error term, $u$, in the population. If the errors $u_1, u_2,\dots, u_n$ are random draws from some distribution other than the normal distribution, $\hat{\beta}_j$ will not be normally distributed, which means the $t$ statistics will not have $t$ distributions and the $F$ statistics will not have $F$ distributions. Ultimately, this is a potentially serious problem because our statistical inference requires being able to obtain critical values or p-values from the $t$ or $F$ distributions. Fortunately, even if the distribution of $y$ is not approximately normal, we do not have to abandon the $t$ statistics for determining which variables are statistically significant. Invoking the central limit theorem, we can conclude that the OLS estimators satisfy \textbf{asymptotic normality}, which means they are approximately normally distributed in sufficiently large sample sizes. In other words, under the Gauss-Markov assumptions and for large enough samples, the OLS slope estimators have the following properties:

\begin{enumerate}
\item $\sqrt{n}\left(\hat{\beta}_j-\beta_j\right)\overset{\text{a}}{\sim}\mathcal{N}\left(0,\sigma_u^2/a_j^2\right)$, where $\sigma_u^2/a_j^2$ is the \textbf{asymptotic variance} of $\sqrt{n}\left(\hat{\beta}_j-\beta_j\right)$ for the slope coefficients and $a_j^2=\text{plim}\left(n^{-1}\sum_{i=1}^{n}\hat{r}_{ij}^2\right)$.
\item $\hat{\sigma}_u^2$ is a consistent estimator of $\sigma_u^2=\text{Var}(u)$
\item For each $j$,
$$\left(\hat{\beta}_j-\beta_j\right)/\text{sd}\left(\hat{\beta}_j\right)\overset{\text{a}}{\sim}\mathcal{N}\left(0,1\right)$$
and
$$\left(\hat{\beta}_j-\beta_j\right)/\text{se}\left(\hat{\beta}_j\right)\overset{\text{a}}{\sim}\mathcal{N}\left(0,1\right)$$
\end{enumerate}

\underline{Proof of Asymptotic Normality of the OLS Slope Coefficients (SLR)}

Suppose that our model can be written as $y=\beta_0+\beta_1x+u$, then
$$\sqrt{n}\left(\hat{\beta}_1-\beta_1\right)=\sqrt{n}\left(\beta_1+\frac{\sum_{i=1}^{n}(x_i-\bar{x})u_i}{\sum_{i=1}^{n}(x_i-\bar{x})^2}-\beta_1\right)$$
$$=\sqrt{n}\left(\frac{n^{-1}\sum_{i=1}^{n}(x_i-\bar{x})u_i}{n^{-1}\sum_{i=1}^{n}(x_i-\bar{x})^2}\right)$$
$$=(1/s_x^2)\left[n^{-1/2}\sum_{i=1}^{n}(x_i-\bar{x})u_i\right]$$
where $s_x^2$ denotes the sample variance of $x$. By the law of large numbers
$$s_x^2=n^{-1}\sum_{i=1}^{n}(x_i-\bar{x})^2\overset{\text{p}}{\to}E\left[(x-E[x])^2\right]=\sigma_x^2=\text{Var}(x)$$
Next,
$$n^{-1/2}\sum_{i=1}^{n}(x_i-\bar{x})u_i=n^{-1/2}\sum_{i=1}^{n}(x_i-\mu_x)u_i+n^{-1/2}\sum_{i=1}^{n}(\mu_x-\bar{x})u_i$$
$$=n^{-1/2}\sum_{i=1}^{n}(x_i-\mu_x)u_i+(\mu_x-\bar{x})\left[n^{-1/2}\sum_{i=1}^{n}u_i\right]$$
Since $\{u_i\}$ is a sequence of i.i.d. random variables with zero mean and variance $\sigma_u^2$ (recall the G-M assumptions),
$$n^{-1/2}\sum_{i=1}^{n}u_i=n^{-1/2}(n\bar{u})=\sqrt{n}\bar{u}\overset{\text{a}}{\sim}\mathcal{N}(0,\sigma_u^2)$$
by the central limit theorem.

Additionally, by the law of large numbers
$$\text{plim}(\mu_x-\bar{x})=\mu_x-\mu_x=0$$
A standard result in asymptotic theory is that if $\text{plim}(w_n)=0$ and $z_n$ has an asymptotic normal distribution, then $\text{plim}(w_n z_n)=0$. Thus, we can conclude $\text{plim}\left((\mu_x-\bar{x})\left[n^{-1/2}\sum_{i=1}^{n}u_i\right]\right)=0$.

Next, because $x$ are $u$ are uncorrelated under the zero conditional mean assumption, $\{(x_i-\mu_x)u_i:i=1,\dots,n\}$ is an indefinite sequence of i.i.d. random variables with mean zero and variance $\sigma_u^2\sigma_x^2$ under the homoskedasticity assumption. Thus, $n^{-1/2}\sum_{i=1}^{n}(x_i-\mu_x)u_i$ has an asymptotic $\text{Normal}(0,\sigma_u^2\sigma_x^2)$ distribution.

Haven proven that both $\text{plim}\left((\mu_x-\bar{x})\left[n^{-1/2}\sum_{i=1}^{n}u_i\right]\right)=0$ and $\text{plim}\left(n^{-1/2}\sum_{i=1}^{n}(x_i-\mu_x)u_i\right)=0$, we can invoke the result from asymptotic theory that if $z_n$ has an asymptotic normal distribution and $\text{plim}(v_n-z_n)=0$, then $v_n$ has the same asymptotic normal distribution as $z_n$. Collectively, it follows that $n^{-1/2}\sum_{i=1}^{n}(x_i-\mu_x)u_i$ also has an asymptotic $\text{Normal}(0,\sigma_u^2\sigma_x^2)$ distribution.

Finally, we can rewrite
$$\sqrt{n}\left(\hat{\beta}_1-\beta_1\right)=(1/\sigma_x^2)\left[n^{-1/2}\sum_{i=1}^{n}(x_i-\bar{x})u_i\right]+[(1/s_x^2)-(1/\sigma_x^2)]\left[n^{-1/2}\sum_{i=1}^{n}(x_i-\bar{x})u_i\right]$$
Since we've shown $\text{plim}(1/s_x^2)=1/\sigma_x^2$, the plim of the second term is zero. Therefore, the asymptotic distribution of $\sqrt{n}\left(\hat{\beta}_1-\beta_1\right)$ is $\text{Normal}(0,\{\sigma_u^2\sigma_x^2\}/\{\sigma_x^2\}^2)=\text{Normal}(0,\sigma_u^2/\sigma_x^2)=\text{Normal}(0,1)$ because $a_1^2=\sigma_x^2$ in the simple regression case. $\blacksquare$

\vspace{1cm}

This theorem ultimately states that (regardless of the population distribution of $u$) the OLS estimators, when properly standardized, have approximate standard normal distributions. This approximation comes about by the central limit theorem because the OLS estimators involve (in a complicated way) the use of sample averages. Effectively, the sequence of distributions of averages of the underlying errors is approaching normality for virtually any population distribution.

Because we know under the CLM assumptions the $t_{n-k-1}$ distribution holds exactly and the $t$ distribution approaches the standard normal distribution as the sample size grows, $t$ testing and the construction of confidence intervals are carried out exactly as under the classical linear model assumptions, even when the normality of $u$ assumption does not hold.

If the sample size is not very large, then the $t$ distribution can be a poor approximation to the distribution of the $t$ statistics when $u$ is not normally distributed. Unfortunately, there are no general prescriptions on how big the sample size must be before the approximation is good enough. Some econometricians feel $n=30$ is satisfactory, but this cannot be sufficient for all possible distributions of u. Depending on the distribution of $u$, more observations may be necessary before the central limit theorem delivers a useful approximation. Further, the quality of the approximation depends not just on $n$, but on the degrees of freedom, $n-k-1$. Hence, with more independent variables in the model, a larger sample size is usually needed to use the $t$ approximation. Additionally, the asymptotic normality of the OLS estimators also implies that the $F$ statistics have approximate $F$ distributions in large sample sizes. Thus, for testing exclusion restrictions or other multiple hypotheses, nothing changes from what we have done before.

\textit{Note:} One should remember that the normality of the error terms assumption is equivalent to stating that the distribution of $y$ given the explanatory variables is normal. Because $y$ is observed (and $u$ is not) it's much easier to think about whether the distribution of $y$ is likely to be normal. Also, one must pay special attention to the required assumptions for the asymptotically normality of the OLS slope coefficients. Specifically, if the homoskedasticity assumption does not hold, the $t$ statistics and confidence intervals are no longer valid.

### The Lagrange Multiplier Statistic

Sometimes it is useful to have other ways to test multiple exclusion restrictions beyond the $F$ test. The \textbf{Lagrange multiplier (LM) statistic} (or \textbf{n-R-squared statistic}), which is a test statistic with large-sample justification that can be used to test for omitted variables, heteroskedasticity, and serial correlation, among other model specification problems, is one such way of testing multiple exclusion restrictions.

The general procedure for obtaining the LM statistic is as follows:

Consider the usual multiple regression model with $k$ independent variables $y=\beta_0+\beta_1x_1+\dots+\beta_kx_k+u$, where we would like to test whether the last $q$ independent variables all have zero population parameters. Thus, the null hypothesis can be stated as
$$\text{H}_0: \beta_{k-q+1}=\dots=\beta_k=0$$
and the alternative is that at least one of the parameters is different from zero. Using the null hypothesis, we can define the restricted model as $y=\beta_0+\beta_1x_1+\dots+\beta_{k-q+1}x_{k-q+1}+u$. Thus, to obtain the LM statistic

\begin{enumerate}
\item Estimate the restricted model, which takes the form $y=\tilde{\beta}_0+\tilde{\beta}_1x_1+\dots+\tilde{\beta}_{k-q+1}x_{k-q+1}+\tilde{u}$
\item Regress $\tilde{u}$ on \emph{all} of the dependent variables in the unrestricted model. This is an example of an \textbf{auxiliary regression}, a regression that is used to compute a test statistic but whose coefficients are not of direct interest.
\item Compute $LM=nR_u^2$, where $n$ is the sample size and $R_u^2$ is the coefficient of determination from regressing $\tilde{u}$ on \emph{all} of the dependent variables in the unrestricted model (performed in the second step).
\item Compare $LM$ to the appropriate critical value, $c$, in a $\chi_q^2$ distribution. If $LM > c$, the null hypothesis is rejected, just as we did with $F$ testing.
\end{enumerate}

If the null hypothesis is true, the R-squared from the auxiliary regression should be “close” to zero, subject to sampling error, because $\tilde{u}$ will be approximately uncorrelated with all the independent variables. It turns out that, under the null hypothesis, the sample size multiplied by the usual R-squared from the auxiliary regression is distributed asymptotically as a chi-square random variable with $q$ degrees of freedom, which allows for testing the joint significance of a set of $q$ independent variables.

Unlike with the $F$ statistic, the degrees of freedom in the unrestricted model plays no role in carrying out the LM test. All that matters is the number of restrictions being tested $(q)$, the size of the auxiliary R-squared $(R_u^2)$, and the sample size $(n)$. The degrees of freedom in the unrestricted model plays no role because of the asymptotic nature of the LM statistic.

With a large sample, important discrepancies between the outcomes of LM and $F$ tests are rare. As with the $F$ statistic, we must be sure to use the same observations in steps (i) and (ii). If data are missing for some of the independent variables that are excluded under the null hypothesis, the residuals from step (i) should be obtained from a regression on the reduced data set.

###  Asymptotic Efficiency of OLS

Analogous to how OLS estimators are BLUE under the Gauss-Markov assumptions, OLS is also \textbf{asymptotically efficient} among a certain class of estimators under the Gauss-Markov assumptions.

Consider the model $y=\beta_0+\beta_1x+u$. Under the zero conditional mean assumption, there are a wide range of consistent for $\beta_0$ and $\beta_1$. Let $z_i=g(x_i)$ for any function of $x$, then
$$\tilde{\beta}_1=\frac{\sum_{i=1}^{n}(z_i-\bar{z})y_i}{\sum_{i=1}^{n}(z_i-\bar{z})x_i}$$
is a consistent estimator of $\beta_1$. To show this, we can plug in $\beta_0+\beta_1x_i+u_i$ for $y_i$, yielding the following
$$\tilde{\beta}_1=\beta_1+\frac{n^{-1}\sum_{i=1}^{n}(z_i-\bar{z})u_i}{n^{-1}\sum_{i=1}^{n}(z_i-\bar{z})x_i}$$
Invoking the law of large numbers to show the numerator and denominator converge to $\text{Cov}(z,u)$ and $\text{Cov}(z,x)$ respectively,
$$\text{plim}\left(\tilde{\beta}_1\right)=\text{plim}\left(\beta_1+\frac{n^{-1}\sum_{i=1}^{n}(z_i-\bar{z})u_i}{n^{-1}\sum_{i=1}^{n}(z_i-\bar{z})x_i}\right)=\beta_1+\frac{\text{Cov}(z,u)}{\text{Cov}(z,x)}=\beta_1$$
provided the zero conditional mean assumption holds, which allows for $\text{Cov}(z,u)=0$, and $z$ and $x$ are correlated (recall, it is possible that $g(x)$ and $x$ are uncorrelated because correlation measures \emph{linear} dependence.)

It is more difficult to show that $\tilde{\beta}_1$ is asymptotically normal. Nevertheless, using arguments similar to those in the proof of asymptotic normality of the OLS slope coefficients, it can be shown that $\sqrt{n}\left(\tilde{\beta}_1-\beta_1\right)$ is asymptotically normal with mean zero and asymptotic variance $\sigma_u^2\text{Var}(z)/\left[\text{Cov}(z,x)\right]^2$. The asymptotic variance of the OLS estimator is obtained when $z=x$, in which case $\sqrt{n}\left(\hat{\beta}_1-\beta_1\right)$ is asymptotically normal with mean zero and asymptotic variance $\sigma_u^2\text{Var}(z)/\left[\text{Var}(x)\right]^2$. By the Cauchy-Schwartz inequality $\left[\text{Cov}(z,x)\right]^2\leq\left[\text{Var}(x)\right]^2$, which implies that, under the Gauss-Markov assumptions, the OLS estimator has a smaller asymptotic variance than any estimator of the aforementioned form.

## Exercises

### Problems

\begin{enumerate}
\item $$\text{plim}\left(\hat{\beta}_0\right)=\text{plim}\left(\bar{y}-\hat{\beta}_1\bar{x}_1\right)$$
$$=\text{plim}(\bar{y})-\text{plim}(\hat{\beta}_1)\cdot\text{plim}(\bar{x}_1)$$
Using the fact that $\bar{y}$ and $\bar{x_1}$ are consistent estimators for their corresponding expectations (from the LLN) and that $\hat{\beta}_1$ is a consistent estimator of $\beta_1$,
$$=E[y]-\beta_1E[x_1]=\beta_0$$
\item  If $funds$ and $risktol$ are positively correlated, the inconsistency in $\tilde{\beta}_1$ depends on the correlation between $pctstck$ and $risktol$. Assuming $pctstck$ and $risktol$ are also positively correlated, then the inconsistency in $\tilde{\beta}_1$ is positive.
\item No, I highly doubt $cigs$ has a normal distribution in the U.S. adult population because $cigs$ is strictly nonnegative and a large proportion (more than half) of the population doesn't smoke. Thus, not only does the distribution fail to conform the symmetry of a normal distribution, but it also fails to take on no particular value with zero probability.
\item $$\text{plim}\left(\tilde{\beta}_0\right)=\text{plim}\left(\bar{y}-\tilde{\beta}_1\bar{x}\right)$$
$$=\text{plim}(\bar{y})-\text{plim}(\tilde{\beta}_1)\cdot\text{plim}(\bar{x})$$
Using the fact that $\bar{y}$ and $\bar{x}$ are consistent estimators for their corresponding expectations (from the LLN) and that $\tilde{\beta}_1$ is a consistent estimator of $\beta_1$,
$$=E[y]-\beta_1E[x]=\beta_0$$
\item 
\begin{enumerate}
\item Using the normal distribution to estimate the probability that score exceeds 100, the answer would not be zero but would be very small. This is contradictory to the fact that $score$ cannot exceed 100.
\item The normal distribution somewhat fits that left tail but is certainly imperfect. More specifically, there are "spikes" in the proportions around scores of 20, 50, and 60, which accounts for some of the misfit between the histogram and the normal distribution.
\end{enumerate}
\item 
\begin{enumerate}
\item 
\item 
\item 
\item $\hat{\beta}_1$ is a consistent estimator for $\beta_1$ because, as we showed in part(c), $\text{Cov}(x,v)=0$. On the other hand, $\hat{\beta}_1$ is unbiased only if $\beta_2=0$ since $E[v|x]$ depends on $x$ unless $\beta_2=0$ (as showed in part (b)).
\item Consider the multiple regression model $y=\beta_0+\beta_1x+\beta_2x^2+u$,
$$\frac{\partial y}{\partial x}=\beta_1+2\beta_2x=\beta_1\text{ if }x=0,$$
\item Assuming $\beta_2\neq 0$, it's more valuable to consistently estimate $\beta_1$ and $\beta_2$ because $\beta_2$ allows for diminishing or increasing marginal returns to $y$ from $x$ and allows for interpreting the partial effect of $x$ on $y$ from values of $x$ other than its mean.
\end{enumerate}
\end{enumerate}

### Computer Exercises

C1)
```{r Chapter 5 Computer Exercise 1,echo=TRUE,include=TRUE,comment=NA,fig.align='center',fig.width=4,fig.height=3}
#i
lm1 <- lm(wage ~ educ + exper + tenure, data = wage1)
ggplot(mapping = aes(lm1$residuals))+
  geom_histogram(fill = "lightblue", color = "darkblue", bins = 27)

#ii
lm2 <- lm(lwage ~ educ + exper + tenure, data = wage1)
ggplot(mapping = aes(lm2$residuals))+
  geom_histogram(fill = "lightblue", color = "darkblue", bins = 27)

#iii
#Closer with log-level model

rm(lm1,lm2)
```

C2)
```{r Chapter 5 Computer Exercise 2,echo=TRUE,include=TRUE,comment=NA}
#i
lm1 <- lm(colgpa ~ hsperc + sat, data = gpa2)
summary(lm1)

#ii
lm2 <- lm(colgpa[1:2070] ~ hsperc[1:2070] + sat[1:2070], data = gpa2)
summary(lm2)

#iii
coef(summary(lm2))[, "Std. Error"][[2]] / coef(summary(lm1))[, "Std. Error"][[2]]
sqrt(nrow(gpa2))/sqrt(2070)

rm(lm1,lm2)
```

C3)
```{r Chapter 5 Computer Exercise 3,echo=TRUE,include=TRUE,comment=NA}
BWGHT <- as_tibble(bwght) %>%
  filter(!is.na(motheduc), !is.na(fatheduc))

restricted_model <- lm(bwght ~ cigs + parity + faminc, data = BWGHT)

lm_statistic <- nrow(BWGHT) * summary(lm(restricted_model$residuals ~ cigs + parity + faminc + motheduc + fatheduc, data = BWGHT))[["r.squared"]]

lm_statistic

pchisq(q = lm_statistic, df = 2, lower.tail = FALSE)

#Compared to the F-Statistic
linearHypothesis(lm(bwght ~ cigs + parity + faminc + motheduc + fatheduc, data = BWGHT), c("motheduc = 0","fatheduc = 0"))

rm(BWGHT, restricted_model, lm_statistic)
```

C4)
```{r Chapter 5 Computer Exercise 4,echo=TRUE,include=TRUE,comment=NA}
#i
K401KSUBS <- as_tibble(k401ksubs) %>%
  filter(fsize == 1)

z_inc <- standardize(K401KSUBS$inc)

sum(z_inc^3) / (length(z_inc) - 1)

z_linc <- standardize(log(K401KSUBS$inc))

sum(z_linc^3) / (length(z_linc) - 1)

#inc has more skewness and therefore seems less likely to be normally distributed

#ii
z_bwght <- standardize(bwght2$bwght)

sum(z_bwght^3) / (length(z_bwght) - 1)

z_lbwght <- standardize(log(bwght2$bwght))

sum(z_lbwght^3) / (length(z_lbwght) - 1)

#log(bwght) has more skewness and therefore seems less likely to be normally distributed

rm(K401KSUBS, z_inc, z_linc, z_bwght, z_lbwght)

#iii
#This statement is certainly fallacious. Part (ii) serves as an example
#But cases where y is close to zero (or even negative) will likely also
#disprove this statement

#iv
#In regression analysis, we should be concerned with the conditional distributions
```

\newpage

# Chapter 6

## Notes

### Beta Coefficients

Sometimes, in econometrics, a key variable is measured on a scale that is difficult to interpret. One way of alleviating interpretation difficulties is to look at what happens to the dependent variable when one of the regressors is one \emph{standard deviation} higher. A similar and perhaps better method of doing such is to \emph{standardize} all of the variables in a model. This means computing the z-score for every variable in the sample and then running a regression using the z-scores.

Suppose we start with the original OLS equation $y_i=\hat{\beta}_0+\hat{\beta}_1x_{i1}+\dots+\hat{\beta}_kx_{ik}+\hat{u}_i$. Then, if we subtract $\bar{y}$ from each side and use the fact that the sample average of the residuals is zero, we get
$$y_i-\bar{y}=\hat{\beta}_1(x_{i1}-\bar{x}_1)+\dots+\hat{\beta}_k(x_{ik}-\bar{x}_k)+\hat{u}_i$$
If we divide each side by the sample standard deviation of $y$ $(\hat{\sigma}_y)$, the equation becomes
$$(y_i-\bar{y})/\hat{\sigma}_y=\hat{\beta}_1/\hat{\sigma}_y(x_{i1}-\bar{x}_1)+\dots+\hat{\beta}_k/\hat{\sigma}_y(x_{ik}-\bar{x}_k)+\hat{u}_i/\hat{\sigma}_y$$
$$(y_i-\bar{y})/\hat{\sigma}_y=\left(\hat{\sigma}_1/\hat{\sigma}_y\right)\hat{\beta}_1\left[(x_{i1}-\bar{x}_1)/\hat{\sigma}_1\right]+\dots+\left(\hat{\sigma}_k/\hat{\sigma}_y\right)\hat{\beta}_k\left[(x_{ik}-\bar{x}_k)/\hat{\sigma}_k\right]+(\hat{u}_i/\hat{\sigma}_y),$$
which can be rewritten (after dropping the $i$ subscript) as
$$z_y=\hat{b}_1z_1+\dots+\hat{b}_kx_k+error,$$
where $z_y$ denotes the z-score of $y$, $z_1$ is the z-score of $x_1$, and so on. These $\hat{b}_j$ are called \textbf{standardized coefficients} or \textbf{beta coefficients}, which represent the predicted standard deviations change in $y$ for a one standard deviation change in $x_j$. Thus, we are measuring effects not in terms of the original units of $y$ or the $x_j$, but in standard deviation units. For example, if $x_1$ increases by one standard deviation, then $\hat{y}$ changes by $\hat{b}_1$ standard deviations. Even in cases where we're interested in estimating some form of an elasticity, comparing beta coefficient magnitudes can be helpful. It should be noted that, whether we use standardized or unstandardized variables does not affect statistical significance: the $t$ statistics are the same in both cases.

### Logarithmic Function Forms

As we've discussed, using the natural log of variables in a model is a common practice in applied econometrics. Consider the model
$$\widehat{\ln(y)}=\hat{\beta}_0+\hat{\beta}_1\ln(x_1)+\hat{\beta}_2x_2$$
Then, $\hat{\beta}_1$ is the approximated predicted percentage change in $y$ for a 1\% change in $x_1$ and $100\cdot\hat{\beta}_2$ is the is the approximated predicted percentage change in $y$ for a 1 unit change in $x_2$. It turns out, that for larger values of $\hat{\beta}_1$ and $\hat{\beta}_2$, these approximate percentage changes become less accurate. Using some simple algebra and calculus, we can derive the \emph{exact} predicted percentage change in $y$ for a change in $x_2$ as
$$\%\Delta\hat{y}=100\cdot\left[exp\left(\hat{\beta}_2\Delta x_2\right)-1\right],$$
where the multiplication by 100 turns the proportionate change into a percentage change. While this is not an unbiased estimator (because $exp(\cdot)$ is a nonlinear function), it is a consistent estimator.

This adjustment is not as crucial for small percentage changes. The logarithmic approximation to percentage changes has an advantage that justifies its reporting even when the percentage change is large. For one, the exact predicted change reports a different value if $x$ changes by $a$ to that if $x$ changes by $-a$. Essentially, using logarithmic approximation is similar in spirit to calculating an arc elasticity of demand, where the averages of prices and quantities are used in the denominators in computing the percentage changes. Another advantage of using logarithmic form is, when $y>0$, models using $\ln(y)$ as the dependent variable often satisfy the CLM assumptions more closely than models using the level of $y$. Strictly positive variables often have conditional distributions that are heteroskedastic or skewed; taking the log can mitigate, if not eliminate, both problems. Another potential benefit of using logs is that taking the log of a variable often narrows its range, which can make OLS estimates less sensitive to outliers (be cautious of this when $\ln(\cdot)$ can lead to large values such as when $\cdot$ is close to zero. Generally speaking, when dealing with large integer values (such as a positive dollar amount), the natural log is often used in application. Additionally, in cases where a variable is nonnegative but can take on the value 0, $\ln(1+\cdot)$ is sometimes used. Finally, one should take caution in comparing logarithmic forms to level forms using $R^2$. It is \emph{not} legitimate to compare R-squareds from models where $y$ is the dependent variable in one case and $\ln(y)$ is the dependent variable in the other. These measures explain variations in different variables. 

### Quadratic Function Forms

Quadratic functions are also used quite often in applied economics to capture decreasing or increasing marginal effects. Consider the following estimated equation
$$y=\beta_0+\beta_1x+\beta_2x^2+u,$$
then we can solve for the approximate predicted change in $y$ as
$$\Delta\hat{y}=\left(\hat{\beta}_1+2\hat{\beta}_2\right)\Delta x$$
In cases where $\hat{\beta}_1>0$ and $\hat{\beta}_2<0$, the corresponding explanatory variable has a diminishing marginal effect on $y$ and the function is "concave." Thus, we can solve for the $x$ (denoted $x^*$) where the tuning point occurs, or equivalently, $y$ is maximized by
$$x^*=-\hat{\beta}_1/\left(2\hat{\beta}_2\right)$$

In cases where $\hat{\beta}_1<0$ and $\hat{\beta}_2>0$, the corresponding explanatory variable has an increasing marginal effect on $y$ and the function is "convex." Again, we can solve for the $x$ (denoted $x^*$) where the tuning point occurs, or equivalently, $y$ is maximized by
$$x^*=-\hat{\beta}_1/\left(2\hat{\beta}_2\right)$$

Some other forms that use quadratics include using quadratics along with logarithms to estimate nonconstant elasticities and using cubic and even a quartic term. Estimating such a model causes no complications. Interpreting the parameters is more involved and requires additional thought but is fairly straightforward using calculus.

### Models with Interaction Terms

Sometimes, it is natural for the partial effect, elasticity, or semi-elasticity of the dependent variable with respect to an explanatory variable to depend on the magnitude of yet another explanatory variable. Consider the model
$$y=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_1x_2+u$$
In this case, there is an \textbf{interaction effect} between $x_1$ and $x_2$. Using calculus, we find the ceteris paribus partial effect of $x_1$ on $y$ as
$$\frac{\Delta y}{\Delta x_1}=\beta_1+\beta_3x_2$$
Thus, if $\beta_3>0$, then the ceteris paribus effect of $x_1$ on $y$ is greater for larger values of $x_2$ and $\beta_1$ only measures the ceteris paribus effect of $x_1$ on $y$ when $x_2=0$ (assuming $\beta_3\neq 0$).

Often, it is useful to reparameterize a model so that the coefficients on the original variables have an interesting meaning. Reparametrizing the model, we can obtain
$$y=\alpha_0+\delta_1x_1+\delta_2x_2+\beta_3(x_1-\mu_1)(x_2-\mu_2)+u,$$
where $\delta_1$ represents the ceteris paribus effect of $x_1$ on $y$ at the mean value of $x_2$. Therefore, if we subtract the means of the variables (in practice, we would typically use the sample means) before creating the interaction term, the coefficients on the original variables have a useful interpretation. Plus, we immediately obtain standard errors for the partial effects at the mean values. Nothing prevents us from replacing $\mu_1$ or $\mu_2$ with other values of the explanatory variables that may be of interest (such as the median, mode or the lower and upper quartiles in the sample.)

### Average Partial Effects

In cases in which we specify models that have nonconstant partial effects (such as when using interaction terms or quadratics), often, we want a single value to describe the relationship between the dependent variable $y$ and each explanatory variable. One popular summary measure is the \textbf{average partial effect (APE)} or the \textbf{average marginal effect}. If we estimated the model $y=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_1x_2+u$, the predicted ceteris paribus partial effect of $x_1$ on $y$ as
$$\hat{\beta}_1+\hat{\beta}_3x_{i2}$$
So, the estimated partial effect of $x_1$ on $y$ depends on $x_2$, and thus it's likely each observation will have a unique partial effect. Instead, we can report the average partial effect as
$$\text{APE}_{y}=\hat{\beta}_1+\hat{\beta}_3\bar{x}_2$$

### Adjusted R-Squared

Because $R^2$ strictly increases as we add more explanatory variables to a model, it's often attractive to use the \textbf{adjusted R-squared} (denoted $\bar{R}^2$) as it imposes a penalty for adding additional independent variables to a model. To inspire the intuition behind the adjusted R-squared, consider the \textbf{population R-squared}, which is defined as $\rho^2=1-\sigma_u^2/\sigma_y^2$ and represents the proportion of the variation in $y$ in the population explained by the independent variables. $R^2$ estimates this value by estimating $\sigma_u^2$ as $\text{SSR}/n$ and $\sigma_u^2$ as $\text{SST}/n$, which we know are both biased as estimators. $\bar{R}^2$ instead uses the unbiased estimators $\text{SSR}/(n-k-1)$ and $\text{SST}/(n-1)$. Thus,
$$\bar{R}^2=1-[\text{SSR}/(n-k-1)]/[\text{SST}/(n-1)],$$
which can be rewritten as
$$\bar{R}^2=1-(1-R^2)(n-1)/(n-k-1)$$
It's tempting to this $\bar{R}^2$ is a better estimator of $\rho^2$ than $R^2$. Unfortunately, $\bar{R}^2$ is not generally known to be a better estimator than $R^2$ and is not unbiased as the ratio of two unbiased estimators is not an unbiased estimator.

Interestingly, adding a new independent variable to a regression equation, $\bar{R}^2$ increases if, and only if, the $t$ statistic on the new variable is greater than one in absolute value. Similarly, $\bar{R}^2$ increases when a group of variables is added to a regression if, and only if, the $F$ statistic for joint significance of the new variables is greater than one. One final note of importance is that it's possible to obtain a negative $\bar{R}^2$, which indicates a very poor model fit relative to the number of degrees of freedom.

### Choosing Between Different Models

Previously, we've seen how to use an $F$ statistic to decide,whether at least one variable in the group affects the dependent variable. This test does not allow us to decide which of the variables has an effect. In some cases, we want to choose a model without redundant independent variables, and the adjusted R-squared can help with this. Suppose we are choosing between two models
$$y=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3p+u$$
and
$$y=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3q+u$$
These two equations are \textbf{nonnested models} because neither equation is a special case of the other. $F$ statistics only allow us to test nested models: one model (the restricted model) is a special case of the other model (the unrestricted model). The adjusted R-squared (and thus the $R^2$ since the models have the same number of regressors) can be used to guide selecting between the two models.

Comparing $\bar{R}^2$ to choose among different nonnested sets of independent variables can be valuable when these variables represent different functional forms. Consider
$$y=\beta_0+\beta_1\ln(x)+u$$
and
$$y=\beta_0+\beta_1x+\beta_2x^2+u,$$
where both models allow for capturing nonconstant returns to $y$ from $x$; however the first model only has one explanatory variable and uses logarithmic form while the second has two explanatory variables and uses a quadratic. In general, the model with the larger $\bar{R}^2$ is preferred. Unfortunately, there is an important limitation in using $\bar{R}^2$ to choose between nonnested models as we cannot use it to choose between different functional forms for the dependent variable. For example, if we wanted to decide on using $y$ or $\ln(y)$ as the dependent variable, neither $R^2$ nor $\bar{R}^2$ can be used for this purpose the different dependent variables will have different amounts of variation to explain. Thus, comparing the adjusted R-squareds from regressions with these different forms of the dependent variables does not tell us anything about which model fits better because they are fitting two separate dependent variables.

### Prediction Analysis

Suppose we have estimated the equation $\hat{y}=\hat{\beta}_0+\hat{\beta}_1x_1+\dots+\hat{\beta}_kx_k$. When we plug in particular values of the independent variables, we obtain a prediction for $y$, which is an estimate of the expected value of $y$ given the particular values for the explanatory variables. Let $c_1,c_2,\dots,c_k$ denote particular values for each of the $k$ independent variables, then the parameter we would like to estimate is
$$\theta_0=\beta_0+\beta_1x_1+\dots+\beta_kx_k=E\left[y\middle|x_1=c_1,\dots,x_k=c_k\right]$$
and the estimator is
$$\hat{\theta}_0=\hat{\beta}_0+\hat{\beta}_1c_1+\dots+\hat{\beta}_kc_k$$
In practice, this is easy to compute, but if we want some measure of the uncertainty in this predicted value, it's natural to construct a confidence interval for $\theta_0$, which is centered at $\hat{\theta}_0$. However, to obtain a confidence interval for $\theta_0$, we need a standard error for $\hat{\theta}_0$. Then, with a large df, we can construct a 95% confidence interval using the rule of thumb $\hat{\theta}_0\pm 2\cdot\text{se}\left(\hat{\theta}_0\right)$. To obtain the standard error for $\hat{\theta}_0$, start by writing $\beta_0$ as
$$\beta_0=\theta_0-\beta_1c_1-\dots-\beta_kc_k$$
and plug it into $y=\beta_0+\beta_1x_1+\dots+\beta_kx_k+u$. Thus, we get
$$y=\theta_0+\beta_1(x_1-c_1)+\dots+\beta_k(x_k-c_k)+u$$
Finally, we obtain the predicted value ($\hat{\theta}_0$) and its standard error ($\text{se}\left(\hat{\theta}_0\right)$) from the intercept, which we can use to construct a confidence interval for $\theta_0$. This result allows us to construct a confidence interval for the \emph{average} value of $y$ for the subpopulation with a given set of explanatory variables. But a confidence interval for the average person in the subpopulation is not the same as a confidence interval for a \emph{particular} unit from the population. In forming a confidence interval for an unknown outcome on $y$, we must also account for the variance in the unobserved error.

Let $y^0$ denote the value for which we would like to construct a confidence interval or \textbf{prediction interval}. For example, $y_0$ could represent a person or firm not in our original sample. Let $x^0_1\dots,x^0_k$ be the new values of the independent variables (assuming they are observed) and let $u^0$ be the unobserved error. Thus,
$$y^0=\beta_0+\beta_1x^0_1+\dots+\beta_kx^0_k+u^0$$
Just as we did for predicting the average among a subpopulation, our best prediction of $y^0$ is the expected value of $y^0$ given the explanatory variables, which we estimate from the OLS regression line $\hat{y}^0=\hat{\beta}_0+\hat{\beta}_1x^0_1+\dots+\hat{\beta}_kx^0_k$. Thus, the prediction error is
$$\hat{e}^0=y^0-\hat{y}^0$$
and the expected value equals
$$E\left[\hat{e}^0\middle|x^0\right]=E\left[y^0\middle|x^0\right]-E\left[\hat{y}^0\middle|x^0\right]$$
$$=\left(\beta_0+\beta_1x^0_1+\dots+\beta_kx^0_k+E\left[u^0\middle|x^0\right]\right)-E\left[\hat{\beta}_0+\hat{\beta}_1x^0_1+\dots+\hat{\beta}_kx^0_k\middle|x^0\right]=0$$
since the $\hat{\beta}_j$ is unbiased for all $j$ and $u^0$ has a zero mean. The variance of $\hat{e}^0$ (called the \textbf{variance of the prediction error}) is also easily derivable.
$$\text{Var}\left(\hat{e}^0\middle|x^0\right)=\text{Var}\left(y^0-\hat{y}^0\middle|x^0\right)$$
$$=\text{Var}\left(y^0\middle|x^0\right)+\text{Var}\left(\hat{y}^0\middle|x^0\right)-2\text{Cov}\left(y^0,\hat{y}^0\middle|x^0\right)$$
$$=\text{Var}\left(u^0\middle|x^0\right)+\text{Var}\left(\hat{y}^0\middle|x^0\right)-2\text{Cov}\left(u^0,\hat{\beta}_0+\hat{\beta}_1x^0_1+\dots+\hat{\beta}_kx^0\middle|x^0\right)$$
$$=\sigma_u^2+\text{Var}\left(\hat{y}^0\middle|x^0\right)$$
since $u^0$ is uncorrelated with each $\hat{\beta}_j$ because $u^0$ is uncorrelated with the errors in the sample used to obtain $\hat{\beta}_j$.
It directly follows that
$$\text{se}\left(\hat{e}^0\right)=\sqrt{\left[\text{se}\left(\hat{y}^0\right)\right]^2+\hat{\sigma}_u^2}$$
and the prediction interval for $y^0$ is
$$\hat{y}^0\pm t_{.025}\cdot\text{se}\left(\hat{e}^0\right)$$

### Residual Analysis

Often, it's useful to perform \textbf{residual analysis} in which individual observations are examined to see whether the actual value of the dependent variable is above or below the predicted value; that is, to examine the residuals for the individual observations. An example of residual analysis can be used to rank MBA programs. By regressing the median starting salary on a variety of student characteristics (such as GMAT scores, median college GPA, etc.) residuals can be obtained. The school with the largest residual has the highest predicted value added. An additional example would be determining which professional athletes are overpaid or underpaid relative to their performance. Ultimately, residual analysis can be used to determine whether particular members of the sample have predicted values that are well above or well below the actual outcomes.

### Prediction Analysis with the Dependent Variable in Logarithmic Form

Consider
$$\ln(y)=\beta_0+\beta_1x_1+\dots+\beta_kx_k+u$$
Given the OLS estimators, our prediction of $\ln(y)$ (given the value(s) of the independent variables) is
$$\widehat{\ln(y)}=\hat{\beta}_0+\hat{\beta}_1x_1+\dots+\hat{\beta}_kx_k$$
While it's tempting to predict $\hat{y}$ by taking $\exp(\widehat{\ln(y)})=\exp(\hat{\beta}_0+\hat{\beta}_1x_1+\dots+\hat{\beta}_kx_k)$; however, this will systematically underestimate the expected value of $y$. Under the CLM assumptions,
$$E\left[y\middle|x\right]=E\left[\exp(\beta_0+\beta_1x_1+\dots+\beta_kx_k+u)\middle|x\right]$$
$$=E\left[\exp(u)\middle|x\right]\cdot\exp(\beta_0+\beta_1x_1+\dots+\beta_kx_k)$$
$$=\exp(\sigma_u^2/2)\cdot\exp(\beta_0+\beta_1x_1+\dots+\beta_kx_k)$$
Because $E\left[\exp(u)\middle|x\right]=\exp(\sigma_u^2/2)$ under the normality of the error term assumption from the CLM assumptions. Adjusting this equation to obtain predictions of $y$ using observable values
$$\hat{y}=\exp(\hat{\sigma}_u^2/2)\cdot\exp(\widehat{\ln(y))}$$
This prediction is not unbiased but is consistent. Unfortunately, there are no unbiased predictions of $y$, but the above prediction often works well. However, it does rely on the normality of the error term, u. If we just assume that $u$ is independent of the explanatory variables, then we have
$$\hat{y}=\hat{\alpha}_0\exp(\widehat{\ln(y))}$$
where $\hat{\alpha}_0$ is an estimate of $\alpha_0=E[\exp(u)|x]$ and replaces $\hat{\sigma}_u^2/2$ as $E\left[\exp(u)\middle|x\right]$ no longer equals $\exp(\sigma_u^2/2)$. To obtain $\hat{alpha}_0$ the method of moments estimator
$$\hat{\alpha}_0=n^{-1}\sum_{i=1}^{n}\exp(\hat{u}_i),$$
which is a biased but consistent estimator of $\alpha_0$. Another biased yet consistent estimator of $\alpha_0$ can be obtained from simple linear regression through the origin (see page 207 for further details).

### Comparing Models where the Dependent Variable Appears in Alternaitve Forms

As we discussed earlier, $R^2$ and $\bar{R}^2$ cannot be used to compare models where the dependent variables are in different forms (such as $y$ vs $\ln(y)$). Fortunately, there are two easy ways to find a goodness-of-fit measure in which the $\ln(y)$ model that can be compared with an R-squared from a model where $y$ is the dependent variable. Consider the equation estimated by OLS,
$$\hat{y}=\hat{\beta}_0+\hat{\beta}_1x_1+\dots+\hat{\beta}_kx_k$$
Recall that the $R^2$ from this model is simply the squared correlation between $y_i$ and $\hat{y}_i$. Taking the predicted values we derived earlier, $\hat{y}=\hat{\alpha}_0\exp(\widehat{\ln(y))}$ for all observations $i$, we can compute the squared correlation coefficient between $y_i$ and these values as an R-squared, which is comparable to the $R^2$ from the level model.

An alternative form uses the sum of squared residuals. Recall $R^2=1-\text{SSR}/\text{SST}$. If we define the residuals as
$$\hat{r}_i=y_i-\hat{\alpha}_0\exp(\widehat{\ln(y_i)}),$$
an alternative goodness-of-fit measure that can be compared with the R-squared from the linear model for $y$ is
$$1-\frac{\sum_{i=1}^{n}\hat{r}_i^2}{\sum_{i=1}^{n}\left(y_i-\bar{y}\right)^2}$$

### Bootstrapping

In many cases where formulas for standard errors are hard to obtain mathematically, or where they are thought not to be very good approximations to the true sampling variation of an estimator, we can rely on a \textbf{resampling method}. The general idea is to treat the observed data as a population that we can draw samples from. The most common resampling method is the \textbf{bootstrap}. While there are several versions of the bootstrap, we'll focus on the \textbf{nonparametric bootsrap}. Suppose we have an estimate, $\hat{\theta}$, of a population parameter, $\theta$, from a random sample of size $n$. Then, we can obtain a valid standard error for $\hat{\theta}$ by computing the estimate from different random samples drawn from the original data. To implement this process, draw $n$ numbers, with replacement, randomly from the original data set. This produces a new data set (of size $n$) that consists of the original data, but with many observations appearing multiple times. If $\hat{\theta}^{(b)}$ denotes the sample estimate from the bootstrap sample $b$, the \textbf{bootstrap sample error} of $\hat{\theta}$ is just the sample standard deviation of $\hat{\theta}^{(b)}$
$$\text{bse}(\hat{\theta})=\sqrt{\frac{1}{m-1}\sum_{i=1}^{m}\left(\hat{\theta}^{(b)}-\bar{\hat{\theta}^{(b)}}\right)^2},$$
where $m$ is the number of times we resample from the original data set.

## Exercises

### Problems

\begin{enumerate}
\item No, this generality is not necessary. In general, we used quadratics to allow for diminishing (or increasing) marginal effects of the independent variable on the dependent variable. In some cases, allowing for nonconstant effects using quadratics is reasonable but certainly not always. In this example, including $roe^2$ is definitely not necessary since the estimated coefficient on $roe^2$ is both statistically and practically insignificant.
\item Considering the model $c_0y_i=\beta_0+c_1x_{i1}+\dots+c_kx_{ik}+u_i$,
$SSR=\sum_{i=1}^{n}(c_oy_i-\tilde{\beta}_0-\tilde{\beta}_1c_1x_{i1}-\dots-\tilde{\beta}_kc_kx_{ik})^2$, and the OLS first order conditions are:
$$\frac{\partial{SSR}}{\partial{\tilde{\beta}_0}}=-2\sum_{i=1}^{n}(c_oy_i-\tilde{\beta}_0-\tilde{\beta}_1c_1x_{i1}-\dots-\tilde{\beta}_kc_kx_{ik})=0,$$
which solves for $\tilde{\beta}_0$ by
$$\tilde{\beta}_0=c_0\bar{y}-c_1\tilde{\beta}_1\bar{x}_1-\dots-c_k\tilde{\beta}_k\bar{x}_k$$
and
$$\frac{\partial{SSR}}{\partial{\tilde{\beta}_j}}=-2\sum_{i=1}^{n}x_{ij}(c_oy_i-\tilde{\beta}_0-\tilde{\beta}_1c_1x_{i1}-\dots-\tilde{\beta}_kc_kx_{ik})=0$$
Solving for $\tilde{\beta}_j$,
$$\sum_{i=1}^{n}x_{ij}(c_oy_i-\tilde{\beta}_0-\tilde{\beta}_1c_1x_{i1}-\dots-\tilde{\beta}_kc_kx_{ik})=0$$
$$\to\sum_{i=1}^{n}\hat{x}_{ij}(c_oy_i-\tilde{\beta}_0-\tilde{\beta}_1c_1x_{i1}-\dots-\tilde{\beta}_kc_kx_{ik})+\sum_{i=1}^{n}\hat{r}_{ij}(c_oy_i-\tilde{\beta}_0-\tilde{\beta}_1c_1x_{i1}-\dots-\tilde{\beta}_kc_kx_{ik})=0$$
$$\to 0+\sum_{i=1}^{n}\hat{r}_{ij}(c_oy_i-\tilde{\beta}_jc_jx_{ij})+\sum_{i=1}^{n}\hat{r}_{ij}(\tilde{\beta}_0-\tilde{\beta}_1c_1x_{i1}-\dots-\tilde{\beta}_{j-1}c_{j-1}x_{ij-1}-\tilde{\beta}_{j+1}c_{j+1}x_{ij+1}-\dots-\tilde{\beta}_kc_kx_{ik})=0$$
$$\to \sum_{i=1}^{n}\hat{r}_{ij}(c_oy_i)-\tilde{\beta}_jc_j\sum_{i=1}^{n}\hat{r}_{ij}x_{ij}+0=0$$
$$\to c_0\sum_{i=1}^{n}\hat{r}_{ij}y_i-\tilde{\beta}_jc_j\sum_{i=1}^{n}\hat{r}_{ij}^2-\tilde{\beta}_jc_j\sum_{i=1}^{n}\hat{r}_{ij}\hat{x}_{ij}=0$$
$$\to c_0\sum_{i=1}^{n}\hat{r}_{ij}y_i=\tilde{\beta}_jc_j\sum_{i=1}^{n}\hat{r}_{ij}^2$$
$$\to \tilde{\beta}_j=\frac{c_0}{c_j}\cdot\frac{\sum_{i=1}^{n}\hat{r}_{ij}^2}{\sum_{i=1}^{n}\hat{r}_{ij}y_i}$$
$$\to \tilde{\beta}_j=\frac{c_0}{c_j}\cdot\hat{\beta}_j$$
Returning to $\tilde{\beta}_0=c_0\bar{y}-c_1\tilde{\beta}_1\bar{x}_1-\dots-c_k\tilde{\beta}_k\bar{x}_k$ and replacing $\tilde{\beta}_j$ with $\frac{c_0}{c_j}\cdot\hat{\beta}_j$
$$\tilde{\beta}_0=c_0\bar{y}-c_1\cdot\frac{c_0}{c_1}\hat{\beta}_1\bar{x}_1-\dots-c_k\cdot\frac{c_0}{c_k}\hat{\beta}_k\bar{x}_k$$
$$\to\tilde{\beta}_0=c_0\left(\bar{y}-\hat{\beta}_1\bar{x}_1-\dots-\hat{\beta}_k\bar{x}_k\right)$$
$$\to\tilde{\beta}_0=c_0\hat{\beta}_0$$
\item
\begin{enumerate}
\item $$\frac{\partial{\widehat{\ rdintens}}}{\partial{\ sales}}=.00030-0000000140\ sales$$
$$.00030-0000000140\ sales < 0$$
$$sales < `r .00030/.0000000140`$$
\item Yes, I would keep $sales^2$ in the equation because its t-statistic has a one-sided p-value of approximately `r pt(-.0000000070 / .0000000037, df = 29, lower.tail = TRUE)`.
\item $$\widehat{rdintens}=2.613+\underset{(.14)}{.30}\ salesbil - \underset{(`r .0000000037 * 1000^2`)}{`r .0000000070 * 1000^2`}\ salesbil^2$$
$$n=32, R^2=.1484$$
\item I prefer the equation reported in part (c) since it uses fewer zeros and is thus easier to read.
\end{enumerate}
\item 
\begin{enumerate}
\item $$\ln(wage)=\beta_0+\beta_1educ+\beta_2educ\cdot pareduc+\beta_3exper+\beta_4tenure+u$$
$$\to\Delta\ln(wage)=\beta_1\Delta educ+\beta_2\Delta educ\cdot pareduc$$
$$\to\Delta\ln(wage)/\Delta educ=\beta_1+\beta_2pareduc$$
I expect $\beta_2>0$ because I expect returns to education (in wage) to be greater for those who have more educated parents.
\item Using $pareduc=32$ and $pareduc=24$, the estimated difference in the return to education is .00078(32-24)=`r .00078*(32-24)`.
\item No, the estimated return to education now depends negatively on parent education. The $t$ statistic on $educ\cdot pareduc$ is approximately `r -.0016/.0012`, which gives a two-sided p-value of `r pt(-.0016/.0012, df = 716, lower.tail = T)*2`. Hence, we fail to reject $H_0$ at every conventional significance level.
\end{enumerate}
\item Example 4.2 is interested in the effect of school size on student performance. It does not make sense to include $sci11$ to an equation where $math10$ is the dependent variable. This would be a case of overspecifying the model in which we include an explanatory variable that does not have a partial effect on the dependent variable (in the population).
\item $F=\frac{(.232-.229)/2}{(1-.232)/671}\approx `r ((.232-.229)/2)/((1-.232)/671)`$. The corresponding p-value is approximately `r pf(((.232-.229)/2)/((1-.232)/671), df1 = 2, df2 = 671, lower.tail = FALSE)`, which offers little support for including $atndrte^2$ and $ACT\cdot atndrte$ in the model.
\item Considering $\widehat{prate}$ is the dependent model in all of the equations, the second model is generally preferred since it has the greatest $\bar{R}^2$.
\item 
\begin{enumerate}
\item I would argue we should not include $attend$ because lower attendance rates may be a consequence of large amounts of alcohol consumption. If we included $attend$, $\beta_{alcohol}$ would be the partial effect of $alcohol$ on $colGPA$ after controlling for students' attendance rates.
\item Yes, we should include $SAT$ and $hsGPA$ as explanatory variables in a multiple regression model. Including these variables again allows for a more "controlled" interpretation of $\beta_{alcohol}$. Although including both may lead to a high level of multicollinearity, this is not a concern because we're only interested in estimating $\beta_{alcohol}$. The benefit of including both is that we can control for the quality of students by using measurements from their performance in high school.
\end{enumerate}
\item 
\begin{enumerate}
\item $$\exp(-1.96\hat{\sigma})\exp(\widehat{logy^0})\leq\exp(\hat{\sigma}^2/2)\exp(\widehat{logy^0})\leq\exp(1.96\hat{\sigma})\exp(\widehat{logy^0})$$
$$\exp(-1.96\hat{\sigma})\leq\exp(\hat{\sigma}^2/2)\leq\exp(1.96\hat{\sigma})$$
$$-1.96\hat{\sigma}\leq\hat{\sigma}^2/2\leq1.96\hat{\sigma}$$
$$-3.92\leq\hat{\sigma}\leq 3.92$$
$$0\leq\hat{\sigma}\leq 3.92$$
With the dependent variable appearing in logarithmic form, an estimated SER of 3.92 is quite large. Thus, I would argue most applications would satisfy this condition
\item The SER from example 6.7 is about `r summary(lm(lsalary ~ lsales + lmktval + ceoten, data = ceosal2))[["sigma"]]`, which is well below 3.92.
\end{enumerate}
\item 
\begin{enumerate}
\item The first equation is more relevant because it doesn't control for $read4$, which is another measure of student performance. Using the first equation, the estimated effect of a 10\% increase in expenditures per student is .901 percentage points.
\item Yes, while the signs of the coefficients remain the same, the sizes of the coefficients change considerably.
\item I prefer the equation with the smaller adjusted R-squared in this case because it gives a better understanding of the effect of interest. Controlling for $read4$, another measure of student performance, doesn't make sense in interpreting the return to student performance from additional expenditures per student.
\end{enumerate}
\end{enumerate}

### Computer Exercises

C1)
```{r Chapter 6 Computer Exercise 1,echo=TRUE,include=TRUE,comment=NA}
#i
summary(lm(lprice ~ ldist, data = kielmc))$coefficients[,1:3]

#ii
summary(lm(lprice ~ ldist + lintst + larea + lland + rooms + baths + age, data = kielmc))$coefficients[,1:3]

#iii
summary(lm(lprice ~ ldist + lintst + larea + lland + rooms + baths + age + lintstsq, data = kielmc))$coefficients[,1:3]

#iv
kielmc$ldistsq <- kielmc$ldist^2
summary(lm(lprice ~ ldist + lintst + larea + lland + rooms + baths + age + lintstsq + ldistsq, data = kielmc))$coefficients[10,4]
#No, not at conventional significance levels
```

C2)
```{r Chapter 6 Computer Exercise 2,echo=TRUE,include=TRUE,comment=NA}
#i+ii
lm1 <- lm(lwage ~ educ + exper + expersq, data = wage1)
summary(lm1)$coefficients

#iii
100 * (lm1$coefficients[[3]] + 2 * lm1$coefficients[[4]] * 5)
100 * (lm1$coefficients[[3]] + 2 * lm1$coefficients[[4]] * 20)

#iv
- lm1$coefficients[[3]] / (2 * lm1$coefficients[[4]])
sum(wage1$exper > - lm1$coefficients[[3]] / (2 * lm1$coefficients[[4]]))

rm(lm1)
```

C3)
```{r Chapter 6 Computer Exercise 3,echo=TRUE,include=TRUE,comment=NA}
#i
#d log(wage)/d exper = beta_1 + beta_3 exper

#ii
#H_0: beta_3 = 0
#H_1: beta_3 > 0

#iii
lm1 <- lm(lwage ~ educ + exper + (educ * exper), data = wage2)
pt(lm1$coefficients[[4]] / coefficients(summary(lm1))[4, "Std. Error"], df = lm1$df, lower.tail = FALSE)

#iv
wage2$educ_exper10 <- wage2$educ * (wage2$exper - 10)
confint(lm(lwage ~ educ + exper + educ_exper10, data = wage2), parm = "educ")

rm(lm1)
```

C4)
```{r Chapter 6 Computer Exercise 4,echo=TRUE,include=TRUE,comment=NA}
#i
lm1 <- lm(sat ~ hsize + hsizesq, data = gpa2)
summary(lm1)$coefficients

#ii
- lm1$coefficients[[2]] / (2 * lm1$coefficients[[3]])

#iii
#No, only those who take the SAT exam are considered

#iv
lm1 <- lm(sat ~ hsize + hsizesq, data = gpa2)
- lm1$coefficients[[2]] / (2 * lm1$coefficients[[3]])

rm(lm1)
```

C5)
```{r Chapter 6 Computer Exercise 5,echo=TRUE,include=TRUE,comment=NA}
#i
lm1 <- lm(lprice ~ llotsize + lsqrft + bdrms, data = hprice1)
summary(lm1)$coefficients

#ii
lm1$coefficients[[1]] + lm1$coefficients[[2]] * log(20000) + lm1$coefficients[[3]] * log(2500) + lm1$coefficients[[4]] * 4
mean(exp(lm1$residuals)) * exp(lm1$coefficients[[1]] + lm1$coefficients[[2]] * log(20000) + lm1$coefficients[[3]] * log(2500) + lm1$coefficients[[4]] * 4)

#iii
summary(lm(price ~ lotsize + sqrft + bdrms, data = hprice1))$r.squared

cor(hprice1$price, mean(exp(lm1$residuals)) * exp(lm1$coefficients[[1]] + lm1$coefficients[[2]] * hprice1$llotsize + lm1$coefficients[[3]] * hprice1$lsqrft + lm1$coefficients[[4]] * hprice1$bdrms))^2
#The model from part(i) is preferred

rm(lm1)
```

C6)
```{r Chapter 6 Computer Exercise 6,echo=TRUE,include=TRUE,comment=NA}
#i+ii
lm1 <- lm(voteA ~ prtystrA + expendA + expendB + (expendA*expendB), data = vote1)
summary(lm1)$coefficients

#iii
mean(vote1$expendA)
lm1$coefficients[[4]] * 100 + lm1$coefficients[[5]] * 100 * 300

#iv
lm1$coefficients[[3]] * 100 + lm1$coefficients[[5]] * 100 * 100

#v
lm1 <- lm(voteA ~ prtystrA + expendA + expendB + shareA, data = vote1)
#It doesn't make sense to to hold expendA and expendB while changing shareA

#vi
lm1$coefficients[[4]] - lm1$coefficients[[5]] * (1/3)
rm(lm1)
```

C7)
```{r Chapter 6 Computer Exercise 7,echo=TRUE,include=TRUE,comment=NA}
#i
attend$priGPAsq <- attend$priGPA^2
attend$ACTsq <- attend$ACT^2
lm1 <- lm(stndfnl ~ atndrte + priGPA + ACT + priGPAsq + ACTsq + priGPA*atndrte, data = attend)
lm1$coefficients[[3]] + 2 * lm1$coefficients[[5]] * 2.59 + lm1$coefficients[[7]] * .82

#ii
attend$priGPA_259sq <- (attend$priGPA - 2.59)^2
attend$priGPA_atndrte_82 <- attend$priGPA * (attend$atndrte - .82)
summary(lm(stndfnl ~ atndrte + priGPA + ACT + priGPA_259sq + ACTsq + priGPA_atndrte_82, data = attend))$coefficients[3, ]

rm(lm1)
```

C8)
```{r Chapter 6 Computer Exercise 8,echo=TRUE,include=TRUE,comment=NA}
#i
lm1 <- lm(price ~ lotsize + sqrft + bdrms, data = hprice1)
summary(lm1)$coefficients
round((lm1$coefficients[[1]] + lm1$coefficients[[2]] * 10000 + lm1$coefficients[[3]] * 2300 + lm1$coefficients[[4]] * 4) * 1000)

lotsize_10000 <- hprice1$lotsize - 10000
sqrft_2300 <- hprice1$sqrft - 2300
bdrms_4 <- hprice1$bdrms - 4

#ii
#For average observation
lm2 <- lm(hprice1$price ~ lotsize_10000 + sqrft_2300 + bdrms_4)
confint(lm2, parm = "(Intercept)") * 1000

#iii
cat("Lower Bound:", -2 * sd(lm2$residuals * 1000) + (lm1$coefficients[[1]] + lm1$coefficients[[2]] * 10000 + lm1$coefficients[[3]] * 2300 + lm1$coefficients[[4]] * 4) * 1000)

cat("Upper Bound:", 2 * sd(lm2$residuals * 1000) + (lm1$coefficients[[1]] + lm1$coefficients[[2]] * 10000 + lm1$coefficients[[3]] * 2300 + lm1$coefficients[[4]] * 4) * 1000)

rm(lm1, lm2, lotsize_10000, sqrft_2300, bdrms_4)
```

C9)
```{r Chapter 6 Computer Exercise 9,echo=TRUE,include=TRUE,comment=NA}
#i
lm1 <- lm(points ~ exper + age + coll + expersq, data = nbasal)
summary(lm1)$coefficients

#ii
round(-lm1$coefficients[[2]] / (2 * lm1$coefficients[[5]]))

#iii
#Because better players generally go to the draft before completing college

#iv
summary(lm(points ~ exper + age + coll + expersq + agesq, data = nbasal))
#It's not needed but suggests an increasing return to age

#v
lm1 <- lm(lwage ~ points + exper + expersq + age + coll, data = nbasal)
linearHypothesis(lm1, c("age = 0", "coll = 0"))

rm(lm1)
```

C10)
```{r Chapter 6 Computer Exercise 10,echo=TRUE,include=TRUE,comment=NA}
BWGHT2 <- as_tibble(bwght2) %>%
  filter(!is.na(mage), !is.na(npvis))

#i
lm1 <- lm(lbwght ~ npvis + npvissq, data = BWGHT2)
summary(lm1)$coefficients

#ii
-lm1$coefficients[[2]] / (2 * lm1$coefficients[[3]])
sum(BWGHT2$npvis >= 22, na.rm = T)

#iii
#In my opinion more prenatal visits shouldn't impact birth weight
#However, many visits may be an indication of problems

#iv
lm1 <- lm(lbwght ~ npvis + npvissq + mage + magesq, data = BWGHT2)
-lm1$coefficients[[4]] / (2 * lm1$coefficients[[5]])
sum(BWGHT2$mage >= -lm1$coefficients[[4]] / (2 * lm1$coefficients[[5]]), na.rm = T)

#v
summary(lm1)$r.squared
#No, only about 2.5% of the variation

#vi
lm2 <- lm(bwght ~ npvis + npvissq + mage + magesq, data = BWGHT2)
summary(lm2)$r.squared
cor(exp(lm1$fitted.values), BWGHT2$bwght)^2
#The level model is slightly better

rm(lm1,lm2, BWGHT2)
```

C11)
```{r Chapter 6 Computer Exercise 11,echo=TRUE,include=TRUE,comment=NA}
#i+ii
lm1 <- lm(ecolbs ~ ecoprc + regprc, data = apple)
summary(lm1)

#iii
range(lm1$fitted.values)
sum(apple$ecolbs == 0) / length(apple$ecolbs)

#iv
summary(lm1)$r.squared
#No, they only explain about 3% of the variation

#v
lm1 <- lm(ecolbs ~ ecoprc + regprc + faminc + hhsize + educ + age, data = apple)
linearHypothesis(lm1, c("faminc = 0", "hhsize = 0", "educ = 0", "age = 0"))

#vi
summary(lm(ecolbs ~ ecoprc, data = apple))
summary(lm(ecolbs ~ regprc, data = apple))
cor(apple$ecoprc, apple$regprc)

#vii
summary(lm(ecolbs ~ ecoprc + regprc + faminc, data = apple))$r.squared
summary(lm(ecolbs ~ ecoprc + regprc + reglbs, data = apple))$r.squared
summary(lm(ecolbs ~ ecoprc + reglbs + faminc, data = apple))$r.squared
summary(lm(ecolbs ~ reglbs + regprc + faminc, data = apple))$r.squared

rm(lm1)
```

\newpage

# Chapter 7

## Notes

### Dummy Variable Regression

In cases in which we want to know the partial effect of a qualitative factor, we often use a \textbf{dummy variable} (also called a \textbf{binary} or \textbf{zero-one variable}). Consider the model
$$y=\beta_0+\delta_1x_1+\beta_2x_2+\dots+\beta_kx_k+u,$$
where $\delta_1$ denotes the partial effect of dummy variable $x_1$ on $y$. Computing and interpreting $\delta_1$ is fairly simple and often depicted graphically as an \textbf{intercept shift}. More specifically (under the zero conditional mean assumption),
$$\delta_1=E[y|x_1=1,x_2,\dots,x_k]-E[y|x_1=0,x_2,\dots,x_k],$$
where $E[y|x_1=0,x_2,\dots,x_k]$ is the conditional expectation of the \textbf{base} (or \textbf{benchmark}) group and $E[y|x_1=1,x_2,\dots,x_k]$ is the conditional expectation of the group where $x_1=1$ [In \textbf{policy analysis} (more specifically \textbf{program evaluation}), the group where $x_1=0$ is referred to as the \textbf{control group}, and the group where $x_1=1$ is referred to as the \textbf{experimental} (or \textbf{treatment}) \textbf{group}]. Thus, the intercept for the base group can be viewed as $\beta_0$ and that for the other group can be viewed as $\beta_0+\delta_1$.

### Regression with Ordinal Variables

Sometimes, we may wish to use an \textbf{ordinal variable}, which is a variable where the ordering of the values conveys information but the magnitude of the values does not (they do not express cardinality), in our regression equation. Consider the equation $y=\beta_0+\beta_1x+u$, where $x$ is an ordinal variable that takes on the values $\{0,1,2,3,4\}$. Unfortunately, it's difficult to interpret $\beta_1$ because we do not know whether the difference between 0 and 1 is the same as the difference between 1 and 2, 2 and 3, or 3 and 4. Instead of using $x$, we can use dummy variable regression with 0 as the base group. Thus, the equation can be rewritten as
$$y=\beta_0+\delta_1x_1+\delta_2x_2+\delta_3x_3+\delta_4x_4+u,$$
where $x_j=1$ if $x=j$ and 0 otherwise. In applications where the ordinal variable takes on many values (such as a ranking of 100+ schools), one can split the variable in to several groups and then compare the $\bar{R}^2$ to see whether this action is warranted.

### Modeling Differences between Two Groups

In the general model with $k$ explanatory variables and an intercept, suppose we have two groups ($g=1$ or $g=2$), where we want to test whether the intercept and all slopes are the same across the two groups. The model takes the form
$$y=\beta_0+\beta_{g,1}x_1+\dots+\beta_{g,k}x_k+u$$
The restricted model (where we suppose the all of the slopes and the intercepts are the same for the two groups) involves $k+1$ restrictions. On the other hand, the unrestricted model, which we can think of as having a group dummy variable and $k$ interaction terms in addition to the intercept and variables themselves, has $n-2(k+1)$ degrees of freedom. Carrying out the joint hypothesis that all of the slopes and the intercepts are the same for the two groups is similar to an $F$ test; however, the main difference is that the sum of squared residuals from the unrestricted model can be obtained from two separate regressions, one for each group. If $SSR_1$ is the SSR from estimating the above equation for group 1 and $SSR_2$ is the SSR from estimating the above equation for group 2, then the $F$ statistic, which is commonly called the \textbf{Chow statistic}, becomes
$$F=\frac{[SSR_p-(SSR_1+SSR_2)]/(k+1)}{(SSR_1+SSR_2)/[n-2(k+1)]},$$
where $SSR_p$ denotes the sum of squared residual from pooling the groups and estimating a single equation.

Another case where we use the Chow test is when we only want to allow for a difference in intercepts. Thus, we're only interested in testing whether all of the slopes are the same for the two groups. In this case, the $F$ (or Chow) statistic takes the form
$$F=\frac{[SSR_p-(SSR_1+SSR_2)]/k}{(SSR_1+SSR_2)/[n-2(k+1)]},$$
where $SSR_p$ still denotes the pooled sum of squared residual, except it now allows for a difference in intercepts and thus includes a dummy variable that indicates the group (such as $female$).

## Exercises

### Problems

### Computer Exercises

\newpage














































