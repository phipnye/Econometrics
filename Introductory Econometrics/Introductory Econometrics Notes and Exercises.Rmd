---
title: "Introductory Econometrics Notes and Exercises"
output: pdf_document
header-includes:
  - \pagenumbering{gobble}
  - \usepackage{amsmath}
  - \usepackage{mathrsfs}
  - \usepackage{xfrac}
  - \newcommand{\indep}{\perp\!\!\!\!\perp}
---

```{r Setup, echo=FALSE,include=FALSE}
library(tidyverse)
library(wooldridge)
options(scipen = 99999999)
```

\newpage

# Math Review A

## Notes


### Summation Proofs

$$\sum_{i=1}^{n}(x_i-\bar{x})^2=\sum_{i=1}^{n}(x_i^2-2x_i\bar{x}+\bar{x}^2)$$
$$=\sum_{i=1}^{n}x_i^2-2\sum_{i=1}^{n}x_i\bar{x}+\sum_{i=1}^{n}\bar{x}^2$$
$$=\sum_{i=1}^{n}x_i^2-2\bar{x}\sum_{i=1}^{n}x_i+\bar{x}\sum_{i=1}^{n}\bar{x}$$
$$=\sum_{i=1}^{n}x_i^2-2n\bar{x}^2+n\bar{x}^2$$
$$=\sum_{i=1}^{n}x_i^2-n\bar{x}^2$$
$$=\sum_{i=1}^{n}x_i^2-\bar{x}\sum_{i=1}^{n}x_i$$
$$=\sum_{i=1}^{n}(x_i^2-\bar{x}x_i)$$
$$=\sum_{i=1}^{n}x_i(x_i-\bar{x})$$

$$\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})=\sum_{i=1}^{n}(x_iy_i-x_i\bar{y}-y_i\bar{x}+\bar{x}\bar)$$
$$=\sum_{i=1}^{n}x_iy_i-\sum_{i=1}^{n}x_i\bar{y}-\sum_{i=1}^{n}y_i\bar{x}+\sum_{i=1}^{n}\bar{x}\bar{y}$$
$$=\sum_{i=1}^{n}x_iy_i-\bar{y}\sum_{i=1}^{n}x_i-\bar{x}\sum_{i=1}^{n}y_i+\bar{y}\sum_{i=1}^{n}\bar{x}$$
$$=\sum_{i=1}^{n}x_iy_i-n\bar{y}\bar{x}-n\bar{x}\bar{y}+n\bar{y}\bar{x}$$
$$=\sum_{i=1}^{n}x_iy_i-n\bar{y}\bar{x}$$
$$=\sum_{i=1}^{n}x_iy_i-\bar{x}\sum_{i=1}^{n}y_i$$
$$=\sum_{i=1}^{n}(x_iy_i-\bar{x}y_i)$$
$$=\sum_{i=1}^{n}y_i(x_i-\bar{x})=\sum_{i=1}^{n}x_i(y_i-\bar{y})$$

### Natural Logorithm
\begin{enumerate}
\item $\ln(xy)=\ln(x)+\ln(y)$
\item $\ln(\frac{x}{y})=\ln(x)-\ln(y)$
\item $\ln(x^c)=c\ln(x)$
\end{enumerate}

The difference in lns can be used to approximate proportionate changes. Let $x_0$ and $x_1$ be positive values. Then, for small changes in x
$$\ln(x_1)-\ln(x_0)\approx \frac{x_1-x_0}{x_0}=\frac{\Delta x}{x_0}$$
Thus, $$ 100\cdot\Delta\ln(x)\approx \%\Delta x$$

### Elasticity

The \textbf{elasticity} of $y$ with respect to $x$ equals
$$\frac{\%\Delta y}{\%\Delta x}=\frac{(y_1-y_0)/y_0}{(x_1-x_0)/x_0}=\frac{\Delta y/y_0}{\Delta x /x_0}=\frac{\Delta y}{\Delta x}\cdot\frac{x_0}{y_0}$$
Defining a linear model $y=\beta_0+\beta_1 x$, the elasticity of $y$ with respect to $x$ equals
$$\frac{\%\Delta y}{\%\Delta x}=\frac{\Delta y/y}{\Delta x /x}=\frac{\Delta y}{\Delta x}\cdot\frac{x}{y}=\frac{\beta_1 \Delta x}{\Delta x}\cdot\frac{x}{\beta_0+\beta_1 x}=\beta_1\cdot\frac{x}{\beta_0+\beta_1 x}\approx\frac{\Delta\ln(y)}{\Delta\ln(x)}$$
If we use the above approximation for both $x$ and $y$, then the elasticity is approximately equal to $\frac{\ln(y)}{\ln(x)}$. Thus, a \textbf{constant elasticity model} is approximated by
$$\ln(y)=\beta_0+\beta_1\ln(x)$$ where $\beta_1$ is the approximate elasticity of $y$ with respect to $x$.

A \textbf{semi-elasticity model} approximates the percentage change in $y$ with respect to a unit change in $x$ and takes the form $$\ln(y)=\beta_0+\beta_1 x$$ where $\beta_1$ is the semi-elasticity of $y$ with respect to $x$. In other words, $\%\Delta y = 100\beta_1\Delta x \to \beta_1\approx\frac{\%\Delta y}{100\Delta x}$.

Another relationship of some interest is $$y=\beta_0+\beta_1 \ln(x)$$
Using calculus, we can derive $$\Delta y = \beta_1\Delta\ln(x)$$
and thus $$\beta_1 = \frac{\Delta y}{\Delta \ln(x)}\approx\frac{\Delta y}{\frac{\%\Delta x}{100}}$$
In other words, $\beta_1/100$ is the unit change in $y$ when $x$ increases by 1%.


## Exercises
```{r Problem 1,echo=FALSE,include=FALSE}
ex1_data <- c(300,440,350,1100,640,480,450,700,670,530)
```

\begin{enumerate}
\item 
\begin{enumerate}
\item Mean = `r mean(ex1_data)`
\item Median = `r median(ex1_data)`
\item Mean = `r mean(ex1_data) / 100`
Median = `r median(ex1_data) / 100`
\item Mean = `r mean(c(ex1_data[1:7],900,ex1_data[9:10]))`
Median = `r median(c(ex1_data[1:7],900,ex1_data[9:10]))`
\end{enumerate}
\item 
\begin{enumerate}
\item See below
\item `r 3 + 0.2 * 5` classes
\item `r 0.2 * (20 - 10)` classes
\end{enumerate}
\end{enumerate}
```{r Problem 2 Graph, echo=FALSE,include=TRUE,fig.align='center',fig.width=4,fig.height=3}

distance <- 0:20
missed <- 3 + 0.2*distance

ggplot(mapping = aes(x=distance,y=missed))+
  geom_line(color = "blue")+
  geom_point(color = "red", size=0.5, alpha=0.5)+
  xlab("distance")+
  ylab("missed")+
  theme_bw()
```
\begin{enumerate}
\setcounter{enumi}{2}
\item `r 120-9.8*15+.03*200` CDs. This suggests using linear functions to describe demand curves may not be realistic/a good idea. Some form of an elasticity model would likely be more suitable.
\item 
\begin{enumerate}
\item A `r 6.4-5.6` percentage point decrease.
\item A `r (6.4-5.6)/6.4`\% fall.
\end{enumerate}
\item The correct terminology would be the stock return increased by 3 percentage points, a 20\% increase in the return on the stock.
\item 
\begin{enumerate}
\item `r 100 * (42000-35000)/35000`\%
\item $\approx$ `r 100 * (log(42000)-log(35000))`\%
\end{enumerate}
\item 
\begin{enumerate}
\item \$`r round(exp(10.6 + .027 * 0),2)`

\$`r format(round(exp(10.6 + .027 * 5),2), nsmall=2L)`
\item $\%\Delta\text{salary}=100(.027)(5)=$`r .027 * 500`\%
\item `r ((exp(10.6 + .027 * 5)-exp(10.6))/(exp(10.6))-.135)/.135 * 100`\% error
\end{enumerate}
\item The intercept indicates that, with no sales tax, the proportionate growth in employment would be .043 units. The slope indicates that for every unit increase in sales tax, we would expect the proportionate growth in employment to decrease by .78 units.
\item 
\begin{enumerate}
\item See below
\item The most notable difference is the convexity/concavity of the functions. A linear model would have constant marginal returns to yield with respect to fertilizer while the given relationship displays diminishing marginal returns.
\end{enumerate}
\end{enumerate}
```{r Problem 9 Graph, echo=FALSE,include=TRUE,fig.align='center',fig.height=3,fig.width=4}
fertilizer <- seq(1,100,5)
yield <- 120 + .19 * sqrt(fertilizer)
ggplot(mapping = aes(x = fertilizer, y = yield))+
  geom_line(color = "blue")+
  geom_point(color = "red", size = 0.5, alpha = 0.5)+
  xlab("fertilizr")+
  ylab("yield")+
  theme_bw()
```

\begin{enumerate}
\setcounter{enumi}{9}
\item 
\begin{enumerate}
\item It's not of much interest by itself. It suggests a class with 0 students would expect a test score of 45.6, which doesn't make sense or have any real meaning.
\item $$\frac{\partial score}{\partial class}=.082 - .000294\cdot class=0$$
$$\to class^*=\frac{.082}{.000294}\approx 279\text{ students}$$

The highest achievable test score is about `r round(45.6 + .082 * 279 - .000147 * (279^2))`.
\item See below
\item No, this equation may give an idea about what one can expect for \texttt{score} given \texttt{class}, but it's unrealistic to expect exact results.
\end{enumerate}
\end{enumerate}
```{r Problem 10 Graph, echo=FALSE,include=TRUE,fig.align='center',fig.height=3,fig.width=4}
class_ <- seq(1,300,25)
score <- 45.6 + .082 * class_ - .000147 * class_**2
ggplot(mapping = aes(x = class_, y = score))+
  geom_line(color = "blue")+
  geom_point(color = "red", size = 0.5, alpha = 0.5)+
  xlab("class")+
  ylab("score")+
  theme_bw()
```
\newpage
\begin{enumerate}
\setcounter{enumi}{10}
\item 
\begin{enumerate}
\item $$\bar{y}=\frac{y_1+y_2}{2}=\frac{\beta_0+ \beta_1 x_1+\beta_0+ \beta_1 x_2}{2}$$
$$=\frac{2\beta_0+ \beta_1 (x_1+x_2)}{2}$$
$$=\beta_0+\beta_1 \frac{(x_1+x_2)}{2}=\beta_0+\beta_1 \bar{x}$$
\item $$\bar{y}=\frac{\sum_{i=1}^{n}y_i}{n}=\frac{\sum_{i=1}^{n}\beta_0+\beta_1 x_i}{n}$$
$$=\frac{n\beta_0+ \beta_1 \sum_{i=1}^{n}x_i}{n}$$
$$=\beta_0+\beta_1 \frac{\sum_{i=1}^{n}x_i}{n}=\beta_0+\beta_1 \bar{x}$$
\end{enumerate}
\item 
\begin{enumerate}
\item $$\bar{x}=\frac{1}{n}\sum_{i=1}^{n}x_i=\frac{1}{n}\left(\sum_{i=1}^{n_1}x_i+\sum_{i=n_1+1}^{n}x_i\right)$$
$$=\frac{1}{n}\left(n_1\bar{x_1}+n_2\bar{x_2}\right)$$
$$=\frac{n_1}{n}\bar{x_1}+=\frac{n_2}{n}\bar{x_2}$$
$$=w_1\bar{x_1}+=w_2\bar{x_2}$$
\item Yes, they represent the relative portions of the sample space in $i=1,\dots,n_1$ and $i=n_1+1,\dots,n$.
\item The case in part (a) applies to all cases for $g\in\mathbb{Z}^+$.
\end{enumerate}
\item 
\begin{enumerate}
\item No, take the sample $\{x_1,x_2\}=\{1,2\}$. Then,
$$\sum_{i=1}^{n}\frac{1}{x_i}=\frac{1}{2}+\frac{1}{2}=1$$ while $$\frac{1}{\sum_{i=1}^{n}x_i}=\frac{1}{2+2}=\frac{1}{4}$$
\item No, see part (a) where $x_i = 2\ \forall\ i$.
\end{enumerate}
\end{enumerate}

# Math Review B

## Notes

### Experiments

An \textbf{experiment} is a procedure that can theoretically be conducted an infinite number of times and has a well-defined set of outcomes.

A \textbf{random variable} is a variable that takes on numerical values and has an outcome determined by an experiment.

### Variables

A \textbf{Bernoulli} (or \textbf{binary) random variable} is a random variable that can only take on the values zero and one.

A \textbf{discrete random variable} is one that takes on only a finite or countably infinite number of values. A Bernoulli random variable is the simplest example of a discrete random variable.

A \textbf{continuous random variable} is a random variable that takes on any real value with \textit{zero} probability.

### Density Functions

A \textbf{probability density function (pdf)} summarizes the information concerning the possible outcomes of a random variable and the corresponding probabilities. A pdf of a random variable $X$ is generally denoted $f(x)\text { or }f_x \equiv P(X=x)$.

A \textbf{cumulative distribution function (cdf)} is a function that describes the cumulative probability that a random variable's value is less than (or equal to, if continuous) a given value. A cdf of a random variable $X$ is generally denoted $F(x)\text{ or }F_x \equiv P(X\leq x)$

\underline{Cumulative Distribution Function Properties:}
\begin{enumerate}
\item For any number $c$, $P(X>c)=1-F(c)$
\item For any numbers $a<b$, $P(a<X\leq b)=F(b)-F(a)$
\item For a continuous random variable X, $P(X\geq c)=P(X>c)$
\item For a continuous random variable X, $P(a<X<b)=P(a\leq X \leq b)=P(a\leq X <b )=P(a < X \leq b)$
\end{enumerate}

### Independence

Let $X$ and $Y$ be discrete random variables. Then, $(X,Y)$ have a \textbf{joint distribution}, which is fully described by the \textbf{joint probability density function} of $(X,Y)$: $$f_{X,Y}(x,y)=P(X=x,Y=y)$$

Two random variables $X$ and $Y$ are said to be \textbf{independent} if, and only if, $$f_{X,Y}(x,y)=f_X(x)f_Y(y)$$ or $$P(X=x,Y=y)=P(X=x)P(Y=y)$$ for all $x$ and $y$. The pdfs $f_X$ and $f_Y$ are often called the \textbf{marginal probability density functions} to distinguish the from the joint pdf $f_{X,Y}$.

Beyond the case of two random variables, the same concept applies. Random variables $X_1, X_2,\dots, X_n$ are \textbf{independent random variables} if, and only if, their joint pdf is the product of the individual pdfs for any $(x_1, x_2,\dots, x_n)$. This definition of independence holds for both continuous and discrete random variables.

Given independent outcomes with 'success' rate $\theta$, the pdf $(X \sim \text{Binomial}(n,\theta))$ is equal to \newline
$f(x)=\binom{n}{x}\theta^x(1-\theta)^{n-x}$ where $\binom{n}{x}={{}^{n}C_{x}}=\frac{n!}{x!(n-x)!}$.

### Conditional Distributions

The \textbf{conditional distribution} of $Y$ given $X$ is summarized by the \textbf{conditional probability density function}, defined by $$f_{Y|X}(y|x)=\frac{f_{X,Y}(x,y)}{f_{X}(x)}$$ for all values of $x$ such that $f_X(x)>0$. The interpretation of the conditional probability density function is $$f_{Y|X}(y|x)=P(Y=y|X=x)$$

### Expected Values

If $X$ is a random variable, the \textbf{expected value} (or \textbf{expectation}) of $X$, denoted $E[X]$ and sometimes $\mu_X$ or simply $\mu$, is a weighted average of all possible values of $X$. The weights are determined by the probability distribution function. Sometimes, the expected value is called the \textit{population mean}, especially when we want to emphasize that $X$ represents some variable in a population. For a discrete random variable $X$ that takes on values $\{x_1,\dots,x_n\}$, $$E[X]=x_1f(x_1)+\dots+x_nf(x_n)=\sum_{i=1}^{n}x_if(x_i)$$ If $X$ is a continuous random variable, then $E[X]$ is defined through an integral as $$E[X]=\int_{-\infty}^{\infty}xf(x)dx,$$which we assume is well-defined.

\underline{Expected Values Properties}
\begin{enumerate}
\item For a constant $c$, $E[c]=c$
\item For any constants $a$ and $b$, $E[aX+b]=aE[x]+b$
\item If $\{a_1,a_2,\dots,a_n\}$ are constants and $\{X_1,X_2,\dots,X_n\}$ are random variables, then $$E[a_1X_1+a_2X_2+\dots+a_nE[X_n]=E\left[\sum_{i=1}^{n}a_iX_i\right]=\sum_{i=1}^{n}a_iE[X_i]$$
\end{enumerate}

For $X\sim \text{Binomial}(n,\theta)$, we can rewrite $X$ as $Y_1+\dots+Y_n$, where each $Y_i\sim\text{Bernoulli}(\theta)$. Then, $$E[X]=\sum_{i=1}^{n}E[Y_i]=\sum_{i=1}^{n}\theta=n\theta$$

### Variance

For a random variable $X$, $$\text{Var}(x)=E[(X-\mu)^2]$$ The \textbf{variance} tells us the expected distance from $X$ to its mean and is sometimes denoted $\sigma_x^2 \text{ or just } \sigma^2$. For a Bernoulli random variable $X$ $$\sigma_x^2 = E[X^2]-E[X]^2=\theta - \theta^2=\theta(1-\theta)$$

\underline{Variance Properties}
\begin{enumerate}
\item $\text{Var}(x)=0$ if, and only if, there is a constant c such that $P(X=c)=1$, in which case $E[X]=c$. In other words, this first property says that the variance of any constant is zero and if a random variable has zero variance, then it is essentially constant.
\item For any constants $a$ and $b$, $\text{Var}(aX+b)=a^2\text{Var}(x)$
\item For any constants $a$ and $b$, $$\text{Var}(aX+bY)=a^2\text{Var}(x)+2ab\text{Cov}(X,Y)+b^2\text{Var}(Y)$$
\item If $\{X_1,\dots,X_n\}$ are pairwise uncorrelated random variables and $a_i:i=1,\dots,n$ are constants then $$\text{Var}(a_1X_1+\dots+a_nX_n)=a_1^2\text{Var}(X_1)+\dots+a_n^2\text{Var}(X_n)=\text{Var}\left(\sum_{i=1}^{n}a_iX_i\right)=\sum_{i=1}^{n}a_i^2\text{Var}(X_i)$$
\end{enumerate}

### Standard Deviation
The \textbf{standard deviation} of a random variable, denoted $\text{sd}(X)$, is simply the positive square root of the variance: $\text{sd}(X)=+\sqrt{\text{Var}(X)}$. The standard deviation is sometimes denoted $\sigma_X$, or simply $\sigma$, when the random variable is understood.

\underline{Standard Deviation Properties}
\begin{enumerate}
\item For any constant $c$, $\text{sd}(c)=0$.
\item For any constants $a$ and $b$, $$\text{sd}(aX+b)=|a|\text{sd}(X)=|a|\sigma_X$$
\end{enumerate}

### Standardized Random Variables

If $X$ is a random variable, we can redefine a random variable $$Z\equiv \frac{X-\mu}{\sigma},$$ which we can write as $Z=aX+b$, where $a\equiv(1/\sigma)$ and $b\equiv -(\mu/\sigma)$. Then, $$E[Z]=aE[X]+b=(\mu/\sigma)-(\mu/\sigma)=0$$ and $$\text{Var}(Z)=a^2\text{Var}(X)=1$$ Thus, the random variable $Z$ has a mean of zero and a variance (and therefore a standard deviation) equal to one. This procedure is sometimes known as standardizing the random variable $X$, and $Z$ is called a \textbf{standardized random variable}.

### Skewness and Kurtosis

We can use the standardized version of a random variable to define other features of the distribution of a random variable. These features are described by using what are called \textit{higher order moments}. For example, the third moment of the standardized random variable $Z$ is used to determine whether a distribution is symmetric about its mean. We can write $$E[Z^3]=\frac{E[(X-\mu)^3]}{\sigma^3}$$ Generally, $\frac{E[(X-\mu)^3]}{\sigma^3}$ is viewed as a measure of \textbf{skewness} in the distribution of $X$. If $X$ has a symmetric distribution about $\mu$, then $Z$ has a symmetric distribution about zero. That means the density of $Z$ at any two points $z$ and $-z$ is the same.

It also can be informative to compute the fourth moment of $Z$ $$E[Z^4]=\frac{E[(X-\mu)^4]}{\sigma^4}$$The fourth moment $E[Z^4]$ is called a measure of \textbf{kurtosis} in the distribution of $X$. Generally, larger values mean that the tails in the distribution of $X$ are thicker.

### Covariance and Correlation

The \textbf{covariance} between two random variables $X$ and $Y$, sometimes called the \textit{population covariance} to emphasize that it concerns the relationship between two variables describing a population, is defined as the expected value of the product $(X-\mu_X)(Y-\mu_Y)$: $$\text{Cov}(X,Y)\equiv E\left[(X-\mu_X)(Y-\mu_Y)\right]$$which is sometimes denoted $\sigma_{XY}$. If $\sigma_{XY}>0$, then, on average, when $X$ is above its mean, $Y$ is also above its mean. If $\sigma_{XY}<0$,  then, on average, when $X$ is above its mean, $Y$ is below its mean. Note that $$\text{Cov}(X,Y) = E\left[(X-\mu_X)(Y-\mu_Y)\right]= E\left[(X-\mu_X)Y\right]$$$$= E\left[X(Y-\mu_Y)\right]=E[XY]-E[X]E[Y]$$ Covariance measures the amount of linear dependence between two random variables. A positive covariance indicates that two random variables move in the same direction, while a negative covariance indicates they move in opposite directions.

\underline{Covariance Properties}
\begin{enumerate}
\item If $X$ and $Y$ are independent, then $\text{Cov}(X,Y)=0$

This property stems from the fact that $E[XY]=E[X]E[Y]$ when $X$ and $Y$ are independent. It is important to remember that the converse of is not true: zero covariance between $X$ and $Y$ does not imply that $X$ and $Y$ are independent.
\item For any constants $a_1,b_1,a_2,$ and $b_2$, $$\text{Cov}(a_1X+b_1,a_2Y+b_2)=a_1a_2\text{Cov}(X,Y)$$
\item From the \textbf{Cauchy-Schwartz inequality}: $$|\text{Cov}(X,Y)|\leq \text{sd}(X)\text{sd}(Y)$$
\end{enumerate}

The fact that the covariance depends on units of measurement is a deficiency that is overcome by the \textbf{correlation coefficient} between $X$ and $Y$: $$\text{Corr}(X,Y)\equiv\frac{\text{Cov}(X,Y)}{\text{sd}(X)\text{sd}(Y)}=\frac{\sigma_{XY}}{\sigma_X\sigma_Y}$$the correlation coefficient between $X$ and $Y$ is sometimes denoted $\rho_{XY}$ (and is sometimes called the \textit{population correlation}).

\underline{Correlation Properties}
\begin{enumerate}
\item $-1\leq\text{Corr}(X,Y)\leq 1$
\item For any constants $a_1,b_1,a_2,$ and $b_2$, $$\text{Corr}(a_1X+b_1,a_2Y+b_2)=\text{Corr}(X,Y)$$ if $a_1a_2>0$ or $$\text{Corr}(a_1X+b_1,a_2Y+b_2)=-\text{Corr}(X,Y)$$ if $a_1a_2<0$.
\end{enumerate}

### Conditional Expectation

The \textbf{conditional expectation} of a random variable is the expected or average value of one random variable, called the dependent or explained variable, that depends on the values of one or more other variables, called the independent or explanatory variables. When $Y$ is a discrete random variable $$E[Y|x]=\sum_{i=1}^{n}y_i f_{Y|x}(y_i|x)$$When $Y$ is a continuous random variable $$E[Y|x]=\int_{-\infty}^{\infty}y_i f_{Y|x}(y_i|x)dy$$
\newpage
\underline{Conditional Expecation Properties}
\begin{enumerate}
\item $E[c(X)|X]=c(X)$, for any function $c(X)$.
\item For any functions $a(X)$ and $b(X)$, $$E[a(X)Y+b(X)|X]=a(X)E[Y|X]+b(X)$$
\item If $X$ and $Y$ are independent, $E[Y|X]=E[Y]$.
\item From the \textbf{law of iterated expectations} $E[E[Y|X]]=E[Y]$.
\item From a more general version of the law of iterated expectation $E[Y|X]=E[E[Y|X,Z]|X]$.
\item If $E[Y|X]=E[Y]$, then $\text{Cov}(X,Y)=0$.
\item If $E[Y^2]<\infty$ and $E[g(X)^2]<\infty$ for some function $g$, then $E[[Y-E[Y|X]]^2|X]\leq E[[Y-g(X)]^2|X]$ and $E[[Y-E[Y|X]]^2]\leq E[[Y-g(X)]^2]$. This property is very useful in predicting or forecasting contexts. The first inequality says that, if we measure prediction inaccuracy as the expected squared prediction error, conditional on $X$, then the conditional mean is better than any other function of $X$ for predicting $Y$. The conditional mean also minimizes the unconditional expected squared prediction error.
\end{enumerate}

### Conditional Variance
Given random variables $X$ and $Y$, the variance of $Y$, conditional on $X = x$, is simply the variance associated with the conditional distribution of $Y$, given $X = x:E[[Y-E[Y|x]]^2|x]$. The formula can be rewritten as $$\text{Var}(Y|X=x)=E[Y^2|x]-E[Y|x]^2$$

\underline{Conditional Variance Properties}
\begin{enumerate}
\item If $X$ and $Y$ are independent, then $\text{Var}(Y|X)=\text{Var}(Y)$
\end{enumerate}

### Normal Distribution

A \textbf{normal random variable} is a continuous random variable that can take on any value. Its probability density function has the familiar bell-shaped graph and can be written as $$f(x)=\frac{1}{\sigma\sqrt{2\pi}}exp[-(x-\mu)^2/2\sigma^2],\ -\infty<x<\infty$$ We say that $X$ has a \textbf{normal distribution} with expected value $\mu$ and variance $\sigma^2$, written as $X\sim\mathcal{N}(\mu,\sigma^2)$.Because the normal distribution is symmetric about $\mu$, $\mu$ is also the median of X. The normal distribution is also sometimes called the Gaussian distribution after Carl Friedrich Gauss.

One special case of the normal distribution is the \textbf{standard normal distribution} where the mean is zero and the variance is unity. If a random variable $Z$ has a Normal(0,1) distribution, then we say it has a standard normal distribution, and its pdf is given by $$\phi(z)=\frac{1}{\sqrt{2\pi}}exp(-z^2/2),\ -\infty<z<\infty$$

\underline{Normal Distribution Properties}
\begin{enumerate}
\item If $X\sim\mathcal{N}(\mu,\sigma^2)$, then $(X-\mu)/\sigma\sim\mathcal{N}(0,1)$
\item If $X\sim\mathcal{N}(\mu,\sigma^2)$, then $aX+b\sim\mathcal{N}(a\mu+b,a^2\sigma^2)$
\item If $X$ and $Y$ are jointly normally distributed, then they are independent if, and only if, $\text{Cov}(X,Y)=0$
\item Any linear combination of independent, identically distributed normal random variables has a normal distribution.
\end{enumerate}

### Chi-Square Distribution

The chi-square distribution is obtained directly from independent, standard normal random variables. Let $Z_i, i=1,2,\dots,n$ be independent random variables, each distributed as standard normal. Define a new random variable as the sum of the squares of the $Z_i$: $$X=\sum_{i=1}^{n}Z_i^2$$Then, $X$ has what is known as a \textbf{chi-square distribution} with $n$ \textbf{degrees of freedom}. We write this as $X\sim\chi_n^2$ where the expected value of $X$ is $n$ and the variance of $X$ is $2n$.

### t Distribution

A \textbf{t distribution} is obtained from a standard normal and a chi-square random variable. Let Z have a standard normal distribution and let X have a chi-square distribution with $n$ degrees of freedom. Further, assume that $Z$ and $X$ are independent. Then, the random variable $$T=\frac{Z}{\sqrt{X/n}}$$has a t distribution with $n$ degrees of freedom. This is denoted by $T\sim t_n$ where $n$ comes from the degrees of freedom of the chi-square random variable in the denominator. The pdf of the t distribution has a shape similar to that of the standard normal distribution (maintaining a zero expected value), except that it is more spread out (with a variance of $n/(n-2)$ and therefore has more area in the tails. As the degrees of freedom gets large, the t distribution approaches the standard normal distribution.

### F Distribution
To define an F random variable, let $X_1\sim\chi_{k_1}^2$, and $X_2\sim\chi_{k_2}^2$ and assume that $X_1$ and $X_2$ are independent. Then, the random variable $$F=\frac{X_1/k_1}{X_2/k_2}$$ has an \textbf{F distribution} with $(k_1,k_2)$ degrees of freedom. This is denoted as $F\sim F_{k_1,k_2}$

## Exercises
\begin{enumerate}
\item His or her eventual SAT score is viewed as a random variable because his or her score is a variable that takes on numerical values and has an outcome determined by an experiment (the test). The test score is stochastic as it can change from day-to-day or depending on other various circumstances/conditions.
\item 
\begin{enumerate}
\item $P(X\leq 6)=$`r pnorm(6, mean = 5, sd=2)`
\item $P(X>4)=1-P(X\leq 4)=$`r 1-pnorm(4, mean = 5, sd=2)`
\item $P(|X-5|>1)=P([X-5>1] \text{ or }[X-5<-1])=1-P(4<X<6)=1-(P(X<6)-P(X<4))=$`r 1-(pnorm(6, mean = 5, sd=2)-pnorm(4, mean = 5, sd=2))`
\end{enumerate}
\item 
\begin{enumerate}
\item `r .5^10` or `r 100 * .5^10`\%
\item `r .5^10 * 4170` mutual funds
\item $P(\text{At Least One})=1-P(\text{None})=1-(1-.5^{10})^{4170}=$`r 1-(1-.5^10)^4170`

Similarly, this equals $1-\binom{4170}{0}(.5^{10})^0(1-.5^{10})^{4170}=$`r 1-pbinom(0, 4170, .5**10)`
\item $P(X\geq5) = 1-P(X<4)=$`r 1-pbinom(4, 4170, .5**10)`
\end{enumerate}
\item $P(X\geq.6)=1-P(X<.6)=F(.6)=$`r 3*.6^2+2*.6^3`
\item 
\begin{enumerate}
\item $P(\text{At least one})=1-P(\text{None})=1-\binom{12}{0}(.2)^0(.8)^{12}=$`r 1-pbinom(0,12,.2)`
\item $P(X\geq 2)=1-P(X<1)=1-\binom{12}{1}(.2)^1(.8)^{11}=$`r 1-pbinom(1,12,.2)`
\end{enumerate}
\item $E[X]=\int_{0}^{3}\frac{x^2}{9}xdx=\frac{1}{9}\int_{0}^{3}x^3dx=\frac{1}{9}\left[\frac{x^4}{4}\right]_{0}^{3}=\frac{81}{36}=\frac{9}{4}$
\item $E[\text{Made FTs}]=.74*8$=`r .74*8`
\item $E[GPA]=3.5(\frac{2}{9})+3(\frac{7}{9})=\frac{28}{9}\approx$`r round(28/9,2)`
\item $E[\text{salary}]=52.3\times1000=$\$`r 52.3*1000`

$\sigma_{\text{salary}}=|1000|\times14.6$\$`r 1000*14.6`
\item 
\begin{enumerate}
\item $E[GPA|SAT=800]=.70+.002(800)=$`r .70+.002*(800)`

$E[GPA|SAT=1400]=.70+.002(800)=$`r .70+.002*(1400)`
The difference in expected GPAs is fairly large, but the difference in SAT scores is also rather large. I don't feel these estimates are entirely unreasonable.
\item $E[GPA]=E[E[GPA|SAT]]=E[.70+.002(1100)]=.70+.002(1100)=$`r .70+.002*(1100)`
\item No, we don't know any particular student's GPA given his or her SAT score. The provided formula only allows us to derive an expected GPA given an SAT score.
\end{enumerate}
\item 
\begin{enumerate}
\item $E[X]=1/2(-1)+1/2(1)=0$

$E[X^2]=1/2(-1)^2+1/2(1)^2=1$

\item $E[X]=1/2(1)+1/2(2)=3/2$

$E[1/X]=1/2(1)+1/2(1/2)=3/4$

\item From part(a), $E[X^2]=1\neq(E[X])^2=0^2=0$

From part(b), $E[1/X]=3/4\neq(1/E[X])=2/3$

\item $$E[F]=E\left[\frac{X_1/k_1}{X_2/k_2}\right]$$Because $k_1$ and $k_2$ are constants, $$=\frac{k_2}{k_1}E\left[\frac{X_1}{X_2}\right]$$Using the fact that $X_1$ and $X_2$ are assumed independent $$=\frac{k_2}{k_1}E[X_1]E[X_2^{-1}]$$Using the fact that $X_1$ is a chi-square random variable (and thus has a mean of $k_1$) $$=\frac{k_2}{k_1}k_11E[X_2^{-1}]=k_2E[X_2^{-1}]=E\left[\frac{k_2}{X_2}\right]=E\left[\frac{1}{X_2/k_2}\right]$$As we showed in parts (a-c), $E\left[\frac{1}{X_2/k_2}\right]$ has a nonlinear 'internal' function, and thus, we cannot conclude that $E[F]=1$.
\end{enumerate}

\end{enumerate}

# Math Review C

## Notes

### Populations, Parameters, and Random Sampling

A \textbf{population} is any well-defined group of subjects, which could be individuals, firms, cities, or many other possibilities.

A \textbf{random sample} is a sample obtained by sampling randomly from the specified population. In particular, no unit is more likely to be selected than any other unit, and each draw is independent of all other draws.

When $\{Y_1,\dots,Y_n\}$ is a random sample from the density $f(y;\theta)$, we also say that the $Y_i$ are \textbf{independent, identically distributed} (or \textbf{i.i.d.}) random variables from $f(y;\theta)$.

### Estimators

An \textbf{estimator} is a rule for combining data to produce a numerical value for a population parameter; the form of the rule does not depend on the particular sample obtained. More generally, an estimator $W$ of a parameter $\theta$ can be expressed as an abstract mathematical formula: $$W=h(Y_1,Y_2,\dots,Y_n)$$for some known function $h$ of the random variables $Y_1,Y_2,\dots,Y_n$

### Unbiasedness
An estimator, $W$ of $\theta$, is an \textbf{unbiased estimator} if $$E[W]=\theta$$for all possible values of $\theta$.

The distribution of an estimator is often called its \textbf{sampling distribution}, because this distribution describes the likelihood of various outcomes of $W$ across different random samples.

If $W$ is a \textbf{biased estimator} of $\theta$, its bias is defined $$\text{Bias}(W)\equiv E[W]-\theta$$

The \textbf{sample average} is an unbiased estimator of the population variance and is defined as  $$\bar{Y}=\frac{1}{n}\sum_{i=1}^{n}Y_i$$

\underline{Proof of Unbiasedness}

$$E[\bar{Y}]=E\left[\frac{1}{n}\sum_{i=1}^{n}Y_i\right]$$
$$=\frac{1}{n}E\left[\sum_{i=1}^{n}Y_i\right]$$
$$=\frac{1}{n}\sum_{i=1}^{n}E[Y_i]=\frac{1}{n}(n\mu)=\mu$$

The \textbf{sample variance} is an unbiased estimator of the population variance and is defined as  $$S^2=\frac{1}{n-1}\sum_{i=1}^{n}(Y_i-\bar{Y})^2$$

\underline{Proof of Unbiasedness}
$$E[S^2]=E\left[\frac{1}{n-1}\sum_{i=1}^{n}(Y_i-\bar{Y})^2\right]$$
$$=\frac{1}{n-1}E\left[\sum_{i=1}^{n}Y_i^2-2\sum_{i=1}^{n}Y_i\bar{Y}+\sum_{i=1}^{n}\bar{Y}^2\right]$$
$$=\frac{1}{n-1}E\left[\sum_{i=1}^{n}Y_i^2-2\bar{Y}\sum_{i=1}^{n}Y_i+\bar{Y}\sum_{i=1}^{n}\bar{Y}\right]$$
$$=\frac{1}{n-1}E\left[\sum_{i=1}^{n}Y_i^2-2n\bar{Y}^2+n\bar{Y}^2\right]$$
$$=\frac{1}{n-1}\Bigg\{\sum_{i=1}^{n}E[Y_i^2]-nE[\bar{Y}^2]\Bigg\}$$
Using the facts that $\text{Var}(\bar{Y})=\frac{\sigma_Y}{n}$ and $\sigma_Y=E[Y_i^2]-E[Y_i]^2$,
$$=\frac{1}{n-1}\Bigg\{\sum_{i=1}^{n}\left(\sigma_Y+E[Y_i]^2\right)-n\left(\frac{\sigma_Y}{n}+E[\bar{Y}]^2\right)\Bigg\}$$
$$=\frac{1}{n-1}\Bigg\{n\sigma_Y+n\mu_Y^2-\sigma_Y-n\mu_Y^2\Bigg\}$$
$$=\frac{1}{n-1}\left[(n-1)\sigma_Y\right]=\sigma_Y$$

The \textbf{sample covariance} is defined as $$S_{XY}=\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar{X})(Y_i-\bar{Y})$$ and is an unbiased and consistent estimator of $\sigma_{XY}$

The \textbf{sample correlation coefficient} is defined as $$R_{XY}=\frac{S_{XY}}{S_XS_Y}=\frac{\sum_{i=1}^{n}(X_i-\bar{X})(Y_i-\bar{Y})}{\sqrt{\sum_{i=1}^{n}(X_i-\bar{X})^2}\sqrt{\sum_{i=1}^{n}(Y_i-\bar{Y})^2}}$$ and is a consistent but biased estimator of $\rho_{XY}$. Because $S_{XY}$, $S_X$, and $S_Y$ are consistent for the corresponding population parameter, $R_{XY}$ is a consistent estimator of the population correlation, $\rho_{XY}$. However, $R_{XY}$ is a biased estimator for two reasons. First, $S_X$ and $S_Y$ are biased estimators of $\sigma_X$ and $\sigma_Y$, respectively. Second, $R_{XY}$ is a ratio of estimators, so it would not be unbiased, even if $S_X$ and $S_Y$ were.

### Efficiency

The variance of an estimator is often called its \textbf{sampling variance} because it is the variance associated with a sampling distribution. Remember, the sampling variance is not a random variable; it is a constant, but it might be unknown.

An estimator, $W_1$ is \textbf{efficient} relative to $W_2$ when $\text{Var}(W_1)\leq\text{Var}(W_2)$ for all $\theta$, with strict inequality for at least one value of $\theta$.

One way to compare estimators that are not necessarily unbiased is to compute the \textbf{mean squared error (MSE)} of the estimators. If $W$ is an estimator of $\theta$, then the MSE of $W$ is defined as$$\text{MSE}(W)=E[(W-\theta)^2]$$The MSE measures how far, on average, the estimator is away from $\theta$. It can be shown that $\text{MSE}(W)=\text{Var}(W)+[\text{Bias}(W)]^2$, so that $\text{MSE}(W)$ depends on the variance and bias (if any is present). This allows us to compare two estimators when one or both are biased.

### Consistency

Let $W_n$ be an estimator of $\theta$ based on a sample $Y_1,Y_2,\dots,Y_n$ of size $n$. Then, $W_n$ is a \textbf{consistent estimator} of $\theta$ if for every $\epsilon > 0$, $$P(|W_n-\theta|>\epsilon)\to 0 \text{ as }n\to\infty$$When $W_n$ is consistent, we also say that $\theta$ is the \text{probability limit} of $W_n$, written as $\text{plim}(W_n)=\theta$. Unlike unbiasedness—which is a feature of an estimator for a given sample size—consistency involves the behavior of the sampling distribution of the estimator as the sample size $n$ gets large.

\textsl{\underline{Asymptotic Unbiasedness $\leftarrow$ Consistency + Bounded Variance}}

Consider an estimator $W_n$ for a parameter $\theta$. Asymptotic unbiasedness means that the bias of the estimator goes to zero as $n\to\infty$, which means that the expected value of the estimator converges to the true value of the parameter. Consistency is a stronger condition than this; it requires the estimator (not just its expected value) to converge to the true value of the parameter (with convergence interpreted in various ways). Since there is generally some non-zero variance in the estimator, it will not generally be equal to (or converge to) its expected value. Assuming the variance of the estimator is bounded, consistency ensures asymptotic unbiasedness, but asymptotic unbiasedness is not enough to get consistency. To put it another way, under some mild conditions, asymptotic unbiasedness is a necessary but not sufficient condition for consistency.

\textsl{\underline{Asymptotic Unbiasedness + Vanishing Variance $\rightarrow$ Consistency}}

If you have an asymptotically unbiased estimator, and its variance converges to zero, this is sufficient to give weak consistency. (This follows from Markov's inequality, which ensures that convergence in mean-square implies convergence in probability). Intuitively, this reflects the fact that a vanishing variance means that the sequence of random variables is converging closer and closer to the expected value, and if the expected value converges to the true parameter (as it does under asymptotic unbiasedness) then the random variable is converging to the true parameter.

More simply, unbiased estimators are not necessarily consistent, but those whose variances shrink to zero as the sample size grows are \emph{consistent}. For example, the sample variance and standard deviation formulas without \textbf{Bessel's correction} are biased estimators; however, they are also consistent because the converge in probability toward their population values as $n\to\infty$.

The \textbf{law of large numbers (LLN)} is a theorem that states the average from a random sample converges in probability to the population average. It also holds for stationary and weakly dependent time series. This result comes from the fact that $\text{Var}(\bar{Y})=\frac{\sigma_Y}{n}$, which approaches $0$ as $n\to\infty$.

\newpage
\underline{Plim Properties}
\begin{enumerate}
\item If $\theta$ is a parameter and $\gamma=g(\theta)$ is a newly-defined parameter for some continuous function $g(\theta)$. If $\text{plim}(W_n)=\theta$, then the estimator of $\gamma, G_n=g(W_n),$ has a plim defined by$$\text{plim}(G_n)=\gamma$$This is often stated as $$\text{plim}(g(W_n))=g\left(\text{plim}(W_n)\right)$$
\item If $\text{plim}(T_n)=\alpha$ and $\text{plim}(U_n)=\beta$, then
\begin{enumerate}
\item $\text{plim}(T_n+U_n)=\alpha+\beta$
\item $\text{plim}(T_nU_n)=\alpha\beta$
\item $\text{plim}(T_n/U_n)=\alpha/\beta$, if $\beta\neq 0$
\end{enumerate}
\end{enumerate}

### Asymptotic Normality

Let $\{Z_n:n=1,2,\dots\}$ be a sequence of random variables, such that for numbers $z$, $$P(Z_n\leq z)\to\Phi(z)\text{ as }n\to\infty$$ where $\Phi(z)$ is the standard normal cumulative distribution function. Then, $Z_n$ is said to have an \textbf{asymptotic standard normal distribution}. This is sometimes written as $Z_n\overset{\text{a}}{\sim}\mathcal{N}(0,1)$, where the "$a$" stands for “asymptotically” or “approximately.”

The \textbf{central limit theorem (CLT)} states that the average from a random sample (and many other estimators that depend on the sample mean) for any population (with finite variance), when standardized, has an asymptotic standard normal distribution. More formally, for a random sample $\{Y_1,Y_2,\dots,Y_n\}$ with a mean $\mu$ and a variance $\sigma^2$. Then, $$Z_n=\frac{\bar{Y}-\mu}{\sigma/\sqrt{n}}$$ has an asymptotic standard normal distribution. Note that $Z_n$ is the standardized version of $\bar{Y_n}$: $E[\bar{Y_n}]=\mu$ has been subtracted off and divided by $\text{sd}(\bar{Y_n})=\sigma/\sqrt{n}$.

### Maximum Likelihood

The \textbf{maximum likelihood estimator} of $\theta$, call it $W$, is the value of $\theta$ that maximizes the \textbf{likelihood function} $$L(\theta;Y_1,Y_2,\dots,Y_n)=f(Y_1;\theta)f(Y_2;\theta)\dots f(Y_n;\theta)$$which equals $P(Y_1=y_1,Y_2=y_2,\dots,Y_n=y_n)$ in the discrete case. Usually, it is more convenient to work with the \textbf{log-likelihood function}, which is obtained by taking the natural log of the likelihood function: $$\mathscr{L}(\theta)=\ln\left(L(\theta;Y_1,Y_2,\dots,Y_n)\right)=\sum_{i=1}^{n}\ln[f(Y_i,\theta)]=\sum_{i=1}^{n}\ell(\theta;X_i)$$where we use the fact that the log of the product is the sum of the logs.

### Least Squares

A \text{least squares estimator} is an estimator of a parameter that minimizes the sum of squared differences. That is, an estimator, $W$ is a least squares estimator if it minimizes $$\sum_{i=1}^{n}(W-\theta)^2$$It should be noted that the principles of least 
squares, method of moments, and maximum likelihood often result in the same estimator. In other cases, the estimators are similar but not identical.

### Confidence Intervals
A \textbf{condidence interval} is a rule used to construct a random interval so that a certain percentage of all data sets, determined by the confidence level, yields an interval that contains the population value. Thus, a 95% confidence interval of an estimator will contain the true population value 95% of the time. Theoretically, for the sample average the 95% confidence interval can be constructed as follows: $$P\left(-1.96<\frac{\bar{Y}-\mu}{\sigma/\sqrt{n}}<1.96\right)=.95$$
$$\to CI_{95}=[\bar{y}-1.96(\sigma/\sqrt{n}),\bar{y}+1.96(\sigma/\sqrt{n})]$$where $\mu$ is the hypothesized population mean. In practice, however, $\sigma$ is unknown and must be estimated with $s$. Unfortunately, this does not preserve the 95% level of confidence because $s$ depends on the particular sample. In other words, the random interval $[\bar{Y}\pm 1.96(S/\sqrt{n})]$ no longer contains $\mu$ with probability .95 because the constant $\sigma$ has been replaced with the random variable $S$. Thus, rather than using the standard normal distribution, we must rely on the t distribution. The t distribution arises from the fact that $$\frac{\bar{Y}-\mu}{S/\sqrt{n}}\sim t_{n-1}$$The denominator $S/\sqrt{N}$ is an estimate of the $\text{sd}(\bar{Y})$. In general, these estimators of \textbf{sampling standard deviations} are referred to as \textbf{standard errors}.

### Hypothesis Testing

A \textbf{Type I error} is an error in which a true null hypothesis is rejected.

A \textbf{Type II error} is an error in which one fails to reject a false null hypothesis.

A \textbf{significance level} is the probability of Type I error, which is generally denoted $$\alpha=P(\text{Reject }H_0|H_0)$$

A \textbf{$p$-value} is the \emph{largest} significance level at which we could carry out a test and still fail to reject the null hypothesis. Formally,$$p-\text{value}=P(T>t|H_0)=1-\Phi(t)$$where $\Phi(\cdot)$ is the standard normal cdf.

```{r Math Review C Values, include=FALSE,echo=FALSE}
#Ex4
y <- c(165.76,96.32,76.08,185.35,116.43,162.08,152.04,161.75,92.88,149.94,64.75,127.07,133.55,77.70,206.39,108.33,118.17)
x <- c(374,209,253,432,367,361,288,369,206,316,145,355,295,223,459,290,307)
y_over_x <- y/x
W_1 <- sum(y_over_x)/17
W_2 <- mean(y)/mean(x)
#Ex7
wage_before <- c(8.30,9.40,9.00,10.50,11.40,8.75,10.00,9.50,10.80,12.55,12.00,8.65,7.75,11.25,12.65)
wage_after <- c(9.25,9.00,9.25,10.00,12.00,9.50,10.25,9.50,11.50,13.10,11.50,9.00,7.75,11.50,13.00)
wage_dif <- wage_after-wage_before
```


## Exercises
\begin{enumerate}
\item 
\begin{enumerate}
\item $$E[\bar{Y}]=E[\frac{1}{4}(Y_1+Y_2+Y_3+Y_4)]=\frac{1}{4}\big\{E[Y_1]+E[Y_2]+E[Y_3]+E[Y_4]\big\}=\frac{1}{4}(4\mu)=\mu$$

$$\text{Var}(\bar{Y})=\text{Var}\left(\frac{1}{4}(Y_1+Y_2+Y_3+Y_4)\right)=\frac{1}{16}\sum_{i=1}^{4}Var(Y_i)=\frac{1}{16}\sum_{i=1}^{4}\sigma^2=\frac{\sigma^2}{4}$$
\item $$E[W]=E\left[\frac{1}{8}Y_1+\frac{1}{8}Y_2+\frac{1}{4}Y_3+\frac{1}{2}Y_4\right]$$
$$=\frac{1}{8}E[Y_1]+\frac{1}{8}E[Y_2]+\frac{1}{4}E[Y_3]+\frac{1}{2}E[Y_4]$$
$$=\frac{1}{8}\mu+\frac{1}{8}\mu+\frac{1}{4}\mu+\frac{1}{2}\mu=\mu$$

$$\text{Var}(W)=\text{Var}\left(\frac{1}{8}Y_1+\frac{1}{8}Y_2+\frac{1}{4}Y_3+\frac{1}{2}Y_4\right)$$
$$=\frac{1}{64}\text{Var}(Y_1)+\frac{1}{64}\text{Var}(Y_2)+\frac{1}{16}\text{Var}(Y_3)+\frac{1}{4}\text{Var}(Y_4)=\frac{11}{32}\sigma^2$$

\item I prefer $\bar{Y}$ over $W$ because $\bar{Y}$ is a more efficient unbiased estimator.

\textit{Note:} Parts (a and b) make use of the fact that the sample is iid and thus the variance of the sums of the variables is equal to the sum of the variances of the variables.
\end{enumerate}
\item 
\begin{enumerate}
\item $\sum_{i=1}^{n}a_i=1$
\item $$\text{Var}(W_a)=\text{Var}\left(\sum_{i=1}^{n}a_iY_i\right)=\sum_{i=1}^{n}\text{Var}(a_iY_i)=a_1^2\sigma^2+\dots+a_n^2\sigma^2=\sigma^2\sum_{i=1}^{n}a_i^2$$

$$\min_{a_1,\dots,a_n}\text{Var}(W_a)=a_1^2\sigma^2+\dots+a_n^2\sigma^2\text{ s.t. }a_1+\dots+a_n=1$$
$$\mathscr{L}=a_1^2\sigma^2+\dots+a_n^2\sigma^2+\lambda(1-a_1-\dots-a_n)$$
$$\frac{\partial\mathscr{L}}{\partial a_1}=2a_1\sigma^2-\lambda=0$$
$$\dots$$
$$\frac{\partial\mathscr{L}}{\partial a_n}=2a_n\sigma^2-\lambda=0$$
$$\frac{\partial\mathscr{L}}{\partial \lambda}=1-a_1-\dots-a_n=0$$
$$\rightarrow a_1=\dots=a_n\text{ and } \sum_{i=1}^{n}a_i=1$$
$$\rightarrow \sum_{i=1}^{n}a_1=1$$
$$\rightarrow a_1^{*}=\dots=a_n^{*}=\frac{1}{n}$$
\end{enumerate}
\item 
\begin{enumerate}
\item $E[W_1]=E\left[\frac{n-1}{n}(\bar{Y})\right]=\frac{n-1}{n}E[\bar{Y}]=\frac{n-1}{n}(\mu)\neq \mu$

$\text{bias}(W_1)=E[W_1]-\mu=\frac{n-1}{n}(\mu)-\mu=\frac{-\mu}{n}$

$E[W_2]=E\left[\frac{\bar{Y}}{2}\right]=\frac{1}{2}E[\bar{Y}]=\frac{\mu}{2}$

$\text{bias}(W_2)=E[W_2]-\mu=\frac{\mu}{2}-\mu=\frac{-\mu}{2}$

One important difference is that the bias of $W_1$ converges to 0 as $n\to\infty$, while the bias of $W_2$ is constant.
\item $\text{plim}(W_1)=\text{plim}\left(\frac{n-1}{n}(\bar{Y})\right)=\text{plim}\left(\frac{n-1}{n}\right)\text{plim}(\bar{Y})=1\cdot\mu=\mu$
$\text{plim}(W_2)=\text{plim}\left(\frac{\bar{Y}}{2}\right)=\text{plim}(\bar{Y})/\text{plim}(2)=\frac{\mu}{2}$

$W_1$ is consistent.

\item $$\text{Var}(W_1)=\text{Var}\left(\frac{n-1}{n}(\bar{Y})\right)=\left(\frac{n-1}{n}\right)^2\text{Var}(\bar{Y})=\left(\frac{(n-1)^2\sigma^2}{n^3}\right)$$
$$\text{Var}(W_2)=\text{Var}\left(\frac{\bar{Y}}{2}\right)=\frac{1}{4}\text{Var}(\bar{Y})=\frac{\sigma^2}{4n}$$

\item While $\bar{Y}$ is unbiased regardless of the value of $\mu$, when $\mu$ is "close" to zero, the bias of $W_1$ is also close to zero (especially for large samples). Thus, it may be worthwhile to consider $W_1$ over $\bar{Y}$ if $W_1$ is efficient relative to $\bar{Y}$. We know $\text{Var}(\bar{Y})=\frac{\sigma^2}{n}$ and $\text{Var}(W_1)=\left(\frac{(n-1)^2\sigma^2}{n^3}\right)$. Using these calculations to evaluate when $W_1$ is efficient relative to $\bar{Y}$:
$$\left(\frac{(n-1)^2\sigma^2}{n^3}\right)\leq\frac{\sigma^2}{n}$$
$$\left(\frac{n-1}{n}\right)^2\leq 1$$
which holds for all positive values of $n$. Thus, $W_1$ is efficient relative to $\bar{Y}$ and, given the small amount of bias, it may be a better estimator of $\mu$.
\end{enumerate}
\item 
\begin{enumerate}
\item $E[Z]=E\left[E[Z|X]\right]=E\left[E\left[\frac{Y}{X}|X\right]\right]=E\left[\frac{1}{X}E\left[Y|X\right]\right]=E\left[\frac{1}{X}\theta X\right]=E[\theta]=\theta$
\item $E[W_1]=E\left[n^{-1}\sum_{i=1}^{n}(Y_i/X_i)\right]=n^{-1}\sum_{i=1}^{n}E\left[(Y_i/X_i)\right]=n^{-1}\sum_{i=1}^{n}\theta=n^{-1}(n\theta)=\theta$
\item In general, the average of the ratios, $Y_i/X_i$, is not the ratio of the averages $\bar{Y}/\bar{X}$. $$E[W_2|X_1,\dots,X_2]=E\left[\frac{\bar{Y}}{\bar{X}}|X_1,\dots,X_2\right]$$
$$=\frac{1}{\bar{X}}E[\bar{Y}|X_1,\dots,X_n]$$
$$=\frac{1}{\bar{X}}E\left[n^{-1}\sum_{i=1}^{n}Y_i|X_1,\dots,X_n\right]$$
$$=\frac{1}{n\bar{X}}\sum_{i=1}^{n}E[Y_i|X_1,\dots,X_n]$$
$$=\frac{1}{n\bar{X}}\left(n\theta \bar{X}\right)=\theta$$
\item $W_1=n^{-1}\sum_{i=1}^{n}(Y_i/X_i)=$`r W_1`

$W_2=\frac{\bar{Y}}{\bar{X}}=$`r W_2`

Yes, they are similar.
\end{enumerate}
\item 
\begin{enumerate}
\item $G$ is not an unbiased estimator of $\gamma$ because $G$ has a nonlinear relationship with $\bar{Y}$. As we concluded in Math Review B, the expected value of the ratio is not the ratio of the expected value.
\item $\text{plim}(G)=\text{plim}\left(\frac{\bar{Y}}{1-\bar{Y}}\right)=\text{plim}(\bar{Y})/\text{plim}(1-\bar{Y})=\theta/(\text{plim}(1)-\text{plim}(\bar{Y}))=\frac{\theta}{1-\theta}=\gamma$
\end{enumerate}
\item 
\begin{enumerate}
\item $$H_0:\mu=0$$
\item $$H_1:\mu<0$$
\item $t=\frac{\bar{y}-\mu}{s/\sqrt{n}}=\frac{-32.8-0}{466.4/\sqrt{900}}\approx$`r -32.8/(466.4/30)`

$p\approx$`r pt(-32.8/(466.4/30),899)`

We reject the null hypothesis at the 5\% level but fail to reject $H_0$ at the 1\% level.

\item We've already shown there is a statistically significant difference at the 5\% level but not at the 1\% level. On the other hand, I would struggle to argue there is a practical significance when there is only a 32.8 ounce difference in alcohol consumption over an entire year.

\item This analysis implicitly assumes all other factors that affect liquor consumption have remained the same. Factors such as such as income, or changes in price due to transportation costs, are assumed constant over the two years.
\end{enumerate}
\item 
\begin{enumerate}
\item $CI_{95}=[$`r mean(wage_dif)-qt(.025,length(wage_dif)-1,lower.tail = F)*sd(wage_dif)/sqrt(length(wage_dif))`,`r mean(wage_dif)+qt(.025,length(wage_dif)-1,lower.tail = F)*sd(wage_dif)/sqrt(length(wage_dif))`$]$
\item $$H_0:\mu=0$$
$$H_1:\mu>0$$
\item $t=\frac{\bar{d}-0}{s_d/\sqrt{n}}=$`r (mean(wage_dif))/(sd(wage_dif)/sqrt(length(wage_dif)))`

For a one-sided test, we would reject $H_0$ at the 5\% level but fail to reject $H_0$ at the 1\% level.
\item $p=$`r pt((mean(wage_dif))/(sd(wage_dif)/sqrt(length(wage_dif))), length(wage_dif)-1,lower.tail=F)`
\end{enumerate}
\item 
\begin{enumerate}
\item $\bar{Y}=\frac{188}{429}\approx$`r 188/429`
\item $$\text{sd}(\bar{Y})=\frac{\sigma_\theta}{\sqrt{n}}=\sqrt{\frac{\theta(1-\theta)}{n}}$$
\item $t=\frac{\bar{Y}-.5}{\text{se}(\bar{Y})}=\frac{\bar{Y}-.5}{\sqrt{\bar{Y}(1-\bar{Y})/n}}=$`r (188/429 - .5) / (sqrt(188/429 * (1-188/429)/429))`

$p=$`r pt((188/429 - .5) / (sqrt(188/429 * (1-188/429)/429)),428)`

Thus, we reject $H_0$ at the 1\% level.
\end{enumerate}
\item 
\begin{enumerate}
\item $E[X]=200(.65)=130$
\item $\text{sd}(X)=\sqrt{|200|\times.65(1-.65)}=$`r sqrt(200*.65*.35)`
\item $t=\frac{(115/200)-.65}{\sqrt{(.65)(.35)/200}}$=`r (115/200 - .65)/(sqrt(.65*.35/200))`

$p=$`r pt((115/200 - .65)/(sqrt(.65*.35/200)),199)`

\item The value calculated in part(c) is a $p$-value which is the probability of rejecting a true null hypothesis. In the previous part, we would reject the dictator's claim at the 5% level but fail to reject the claim at the 1% level.
\end{enumerate}
\item $CI_{95}=\left[.394-1.96\sqrt{(.394)(1-.394)/419},.394+1.96\sqrt{(.394)(1-.394)/419}\right]=[$`r .394-1.96*sqrt(.394*(1-.394)/419)`,`r .394+1.96*sqrt(.394*(1-.394)/419)`$]$

Based on his average up to the strike, there is not very strong evidence against $\theta = .400$, as this value is well within the 95\% confidence interval.

\item $t=\frac{\bar{y}-0}{s/\sqrt{n}}=\frac{.132-0}{1.27/20}\approx$`r .132 / (1.27/20)`

The difference is statistically greater than zero at the 5\% level but not at the 1\% level.
\end{enumerate}

# Chapter 1

## Notes

### Data Types

\textbf{Nonexperimental data} are not accumulated through controlled experiments on individuals, firms, or segments of the economy. Non-experimental data are sometimes called \textbf{observational data}, or \textbf{retrospective data}, to emphasize the fact that the researcher is a passive collector of the data.

\textbf{Experimental data} are often collected in laboratory environments in the natural sciences, but they are more difficult to obtain in the social sciences. Although some social experiments can be devised, it is often impossible, prohibitively expensive, or morally repugnant to conduct the kinds of controlled experiments that would be needed to address economic issues.

An \textbf{empirical analysis} uses data to test a theory or to estimate a relationship.

A \textbf{cross-sectional data set} consists of a sample of individuals, households, firms, cities, states, countries, or a variety of other units, taken at a given point in time. An important feature of cross-sectional data is that we can often assume that they have been obtained by \textbf{random sampling} from the underlying population. Sometimes, however, the random sampling assumption is not appropriate for a variety of reasons such as respondents' willingness to answer or sampling from units that are large relative to the population. Nevertheless, random sampling is often assumed with cross-sectional data. The analysis of cross-sectional data is closely aligned with the applied microeconomics fields, such as labor economics, state and local public finance, industrial organization, urban economics, demography, and health economics

A \textbf{time series data set} consists of observations on a variable or several variables over time. Unlike the arrangement of cross-sectional data, the chronological ordering of observations in a time series conveys potentially important information. A key feature of time series data that makes them more difficult to analyze than cross-sectional data is that economic observations can rarely, if ever, be assumed to be independent across time.

A \textbf{pooled cross section} is a data configuration where independent cross sections, usually collected at different points in time, are combined to produce a single data set.

A \textbf{panel data} (or \textbf{longitudinal data}) \textbf{set} consists of a time series for each cross-sectional member in the data set. The key feature of panel data that distinguishes them from a pooled cross section is that the \emph{same} cross-sectional units are followed over a given time period.

### Causality

In econometrics, we're often concerned about finding a \textbf{causaul effect}, or  A \textbf{ceteris paribus} (meaning all other relevant factors are held fixed) change in one variable that has an effect on another variable.

## Exercises

### Problems
\begin{enumerate}
\item 
\begin{enumerate}
\item I would randomly assign (that is without other factors that affect student performance in mind) fourth grade students to varying class sizes and compare students' performances across the various groups.
\item I might expect a negative correlation between class size and test score because generally, larger classes have less funding per student and students in larger classes receive less individualized instruction. There are many additional factors positively correlated with student performance and negatively correlated with class size.
\item No, causality can only be established when ceteris-paribus is satisfied; however, this is not the case as some of the confounding factors that wouldn't be controlled for are listed in part(b).
\end{enumerate}
\item 
\begin{enumerate}
\item All else equal, do job training programs improve worker productivity?
\item No. For one, perhaps a firm that requires job training because it has less-skilled workers and wants to increase its production efficiency. There are many other factors like this on the firm side that make me believe that a firm’s decision to train its workers will be independent of worker characteristics. Additionally, perhaps the firm does not require but offers job training. It's likely individuals who actually do the job training have different characteristics to those who do not. Some factors may include innate ability, intelligence, and motivation.
\item The quality of the equipment.
\item No, causality can only be established when ceteris-paribus is satisfied; however, this is not the case as some of the confounding factors that wouldn't be controlled for (such as the quality of the equipment).
\end{enumerate}
\item No. Again, ceteris-paribus has not been established. Many other confounding factors correlated with "work" and "study" would not be controlled.
\item 
\begin{enumerate}
\item Ideally, panel data that contains corporate tax rates and GSP.
\item Theoretically, it'd be possible to do a controlled experiment, but it would not be ethical. It would require randomly assigning individuals to varying levels of corporate tax rates and measuring the GSP within these groups.
\item If other factors impacting GSP growth and tax rates are sufficiently controlled for, then yes. Otherwise, such correlational analysis will likely be biased and not convincing.
\end{enumerate}
\end{enumerate}

### Computer Exercises

C1)
```{r Chapter 1 Computer Exercises C1, include=TRUE,echo=TRUE}
#i
mean(wage1$educ)
min(wage1$educ)
max(wage1$educ)

#ii
mean(wage1$wage)
#It seems low for the year 2013

#iii
#Consumer Price Index (CPI) for the years 1976 and 2013.
cpi_1976 <- 55.6
cpi_2013 <- 230.280

#iv
mean(wage1$wage)*cpi_2013/cpi_1976
#Yes, the average wage now seems more reasonable

#v
#women
sum(wage1$female)
#men
sum(wage1$female==0)

rm(cpi_1976,cpi_2008)
gc()
```

C2)
```{r Chapter 1 Computer Exercises C2, include=TRUE,echo=TRUE}
#i
#number of women
sum(bwght$male==0)
#number of women who report smoking during pregnancy
sum(bwght$male==0 & bwght$cigs>0)

#ii and iii
mean(bwght$cigs)
#No, this average includes males, a more descriptive average may be:
mean(bwght$cigs[bwght$male==0])
#for all women and
mean(bwght$cigs[bwght$male==0 & bwght$cigs>0])
#for those that reported smoking

#iv
mean(bwght$fatheduc, na.rm = T)
#There are only 1192 observations because there are 196 missing values for fatheduc

#v
mean(bwght$faminc) * 1000
sd(bwght$faminc) * 1000
```

C3)
```{r Chapter 1 Computer Exercises C3, include=TRUE,echo=TRUE}
#i
min(meap01$math4)
max(meap01$math4)
#Without knowing much about the data, the range seems to make sense covering all 100% of pass rates

#ii
sum(meap01$math4 == 100)

#iii
sum(meap01$math4 == 50)

#iv
#math
mean(meap01$math4)
#reading
mean(meap01$read4)
#The reading test seems harder to pass

#v
cor(meap01$math4,meap01$read4)
#Those with higher pass rates on one exam tend to have higher pass rates on the other
#In other words, pass rates on the math and reading tests are highly correlated

#vi
mean(meap01$exppp)
sd(meap01$exppp)

#vii
#actual
500*100/5500
#ln approx
100 * (log(6000)-log(5500))
```

C4)
```{r Chapter 1 Computer Exercises C4, include=TRUE,echo=TRUE}
#i (reporting proportion instead of fraction)
mean(jtrain2$train)

#ii
#receiving training
mean(jtrain2$re78[jtrain2$train==1])
#not receiving training
mean(jtrain2$re78[jtrain2$train==0])
#The difference does appear economically large

#iii
#receiving training
mean(jtrain2$unem78[jtrain2$train==1])
#not receiving training
mean(jtrain2$unem78[jtrain2$train==0])
#The proportion of unemployed who are not receiving training is about 11% larger

#Yes, the training seems effective, establishing ceteris paribus would make the results more convincing
```

C5)
```{r Chapter 1 Computer Exercises C5, echo=TRUE,include=TRUE}
#i
#min
min(fertil2$children)
#max
max(fertil2$children)
#mean
mean(fertil2$children)

#ii
sum(fertil2$children>0) / length(fertil2$children)

#iii
#for those who have electricity
mean(fertil2$children[fertil2$electric==1 & !is.na(fertil2$electric)])
#for those who do not have electricity
mean(fertil2$children[fertil2$electric==0 & !is.na(fertil2$electric)])
#Those without electricity have more children on average (for this sample)

#iv
#No, ceteris paribus is not established
```

C6)
```{r Chapter 1 Computer Exercises C6, echo=TRUE,include=TRUE}
county_murders <- as_tibble(countymurders) %>%
  filter(year == 1996)

#i
n_distinct(county_murders$countyid)
#number with 0 murders
n_distinct(county_murders$countyid[county_murders$murders==0])
#percent with 0 murders
100 * n_distinct(county_murders$countyid[county_murders$murders==0]) / n_distinct(county_murders$countyid)

#ii
#max number of murders
max(county_murders$murders)
#max number of executions
max(county_murders$execs)
#mean number of executions
mean(county_murders$execs)

#iii
cor(county_murders$murders,county_murders$execs)

#iv
#No, I would suspect the positive correlation may be due to executions resulting from murders
#and other crimes, which I suspect is correlated with the number of murders
rm(county_murders)
gc()
```

C7)
```{r Chapter 1 Computer Exercises C7, echo=TRUE,include=TRUE}
#i
#percent who report abusing alcohol
100 * mean(alcohol$abuse)
#employment rate
100 * mean(alcohol$employ)

#ii
100 * mean(alcohol$employ[alcohol$abuse==1])

#iii
100 * mean(alcohol$employ[alcohol$abuse==0])

#iv
#No, ceteris paribus is not established
```

C8)
```{r Chapter 1 Computer Exercises C8, echo=TRUE,include=TRUE}
#i
#assuming each obs/row corresponds to a unique student
NROW(econmath)

#ii
#for those who took econ in high school
mean(econmath$score[econmath$econhs==1])
#for those who did not take econ in high school
mean(econmath$score[econmath$econhs==0])

#iii
#No, it simply tells us the average scores for those who did and did not take econ in hs
#It may signify some level of correlation but not causality

#iv
#Randomly assigning individuals to two groups (one who does take econ in high school
#and one that does not), then performing part(ii) can be used to obtain a good causal estimate
```

# Chapter 2

## Notes

A \textbf{simple linear regression model} is a model relating a dependent variable to one independent variable and generally takes the form$$y=\beta_0+\beta_1x+u$$It is also called the \textbf{two-variable linear regression model} or \textbf{bivariate linear regression model} because it relates the two variables $x$ and $y$.

The variable $u$, called the \textbf{error term} or \textbf{disturbance} in the relationship, represents factors other than $x$ that affect $y$.

If the other factors in $u$ are held fixed, so that the change in $u$ is zero, $\Delta u = 0$, then $x$ has a linear effect on $y$:$$\Delta y=\beta_1\Delta x\text{ if }\Delta u =0$$

As long as the intercept $\beta_0$ is included in the equation, nothing is lost by assuming that the average value of $u$ in the population is zero. Mathematically, $$E[u]=0$$

One of the most important assumptions in econometrics is the \textbf{zero conditional mean assumption}. This crucial assumption says that the average value of the unobservables ($u$) is the same across all slices of the population determined by the value of $x$ and that the common average \emph{is} necessarily equal to the average of $u$ over the entire population. Thus, under this assumption (and the zero unconditional mean assumption), we write$$E[u|x]=E[u]=0$$

Using the zero conditional mean assumption and taking the expected value of $y$ given $x$ from the simple linear linear regression model gives$$E[y|x]=\beta_0+\beta_1 x,$$This is called the \textbf{population regression function (PRF)} and shows that $E[y|x]$ is a linear function of $x$. The PRF gives us a relationship between the average level of $y$ at different levels of $x$. It is important to understand that the PRF tells us how the average value of $y$ changes with $x$; it does not say that $y$ equals $\beta_0 + \beta_1 x$ for all units in the population.
\newpage

### OLS Estimators Derivation (SLR)

Let $\{(x_i,y_i):i=1,\dots,n\}$ denote a random sample of size $n$ from the population. Then, we can write$$y_i=\beta_0+\beta_1 x_i+u_i$$for each $i$ where $u_i$ is the error term for each observation $i$ because it contains all factors affecting $y_i$ other than $x_i$. Because there are two unknown parameters to estimate $(\beta_0,\beta_1)$, we might hope to obtain good estimators, which we will denote $\hat{\beta}_0$ and $\hat{\beta}_1$. Thus, we may want to find estimators that minimize the sum of squared residuals$$SSR=\sum_{i=1}^{n}(y_i-\hat{y}_i)^2,$$ where $\hat{y}_i$ represents fitted values from an estimated model using $\hat{\beta}_0$ and $\hat{\beta}_1$. Formally, $$\hat{y}_i=\hat{\beta}_0+\hat{\beta}_1 x_i$$Hence, we can rewrite $$SSR=\sum_{i=1}^{n}(y_i-\hat{\beta}_0-\hat{\beta}_1 x_i)^2$$

To minimize the sum of squared residuals, we take two first order conditions:

\underline{FOC 1:}

$$\frac{\partial SSR}{\partial\beta_0}=\sum_{i=1}^{n}-2(y_i-\hat{\beta}_0-\hat{\beta}_1 x_i)=0$$
$$\to \sum_{i=1}^{n}(y_i-\hat{\beta}_0-\hat{\beta}_1 x_i)=0$$
$$\to n\bar{y}-n\hat{\beta}_0-n\hat{\beta}_1\bar{x}=0$$
$$\to \bar{y}=\hat{\beta}_0+\hat{\beta}_1\bar{x}$$
$$\to \hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}$$
\small
\textit{Note:} The significance of this FOC is that, using least squares criteria, our line of best fit runs through the sample means $\bar{y}$ and $\bar{x}$.
\normalsize

\underline{FOC 2:}

$$\frac{\partial SSR}{\partial\beta_1}=\sum_{i=1}^{n}-2x_i(y_i-\hat{\beta}_0-\hat{\beta}_1 x_i)=0$$
$$\to\sum_{i=1}^{n}x_i(y_i-\hat{\beta}_0-\hat{\beta}_1 x_i)=0$$
$$\to\sum_{i=1}^{n}(x_iy_i-\hat{\beta}_0x_i-\hat{\beta}_1 x_i^2)=0$$
$$\to\sum_{i=1}^{n}x_iy_i=\sum_{i=1}^{n}\left(\hat{\beta}_0x_i+\hat{\beta}_1 x_i^2\right)$$
$$\to\sum_{i=1}^{n}x_iy_i=\sum_{i=1}^{n}\left((\bar{y}-\hat{\beta}_1\bar{x})x_i+\hat{\beta}_1 x_i^2\right)$$
$$\to\sum_{i=1}^{n}x_iy_i=\sum_{i=1}^{n}\left(\bar{y}x_i-\hat{\beta}_1\bar{x}x_i+\hat{\beta}_1 x_i^2\right)$$
$$\to\sum_{i=1}^{n}x_iy_i=n\bar{x}\bar{y}+\hat{\beta_1}\sum_{i=1}^{n}(x_i^2-\bar{x}x_i)$$
$$\to\sum_{i=1}^{n}(x_iy_i)-n\bar{x}\bar{y}=\hat{\beta}_1\sum_{i=1}^{n}x_i(x_i-\bar{x})$$
$$\to\sum_{i=1}^{n}(x_iy_i-\bar{x}y_i)=\hat{\beta}_1\sum_{i=1}^{n}x_i(x_i-\bar{x})$$
$$\to\hat{\beta}_1=\frac{\sum_{i=1}^{n}y_i(x_i-\bar{x})}{\sum_{i=1}^{n}x_i(x_i-\bar{x})}=\frac{\sum_{i=1}^{n}(y_i-\bar{y})(x_i-\bar{x})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}=\frac{\hat{\sigma}_{xy}}{\hat{\sigma}^2_{x}}=\hat{\rho}_{xy}\cdot\frac{\hat{\sigma}_y}{\hat{\sigma}_x}$$

Note that the FOCs are analogous to the zero mean assumptions (using residuals instead of the error term):

FOC 1 can be restated as
$$E[\hat{u}]=0\to E[y-\hat{y}]=E[y-\hat{\beta}_0-\hat{\beta}_1x]=n^{-1}\sum_{i=1}^{n}(y-\hat{\beta}_0-\hat{\beta}_1x)=0$$

And FOC 2 can be restated as:
$$E[\hat{u}|x]=E[\hat{u}]\to \hat{u}\indep x\to \text{Cov}(x,\hat{u})=0\to E[x\hat{u}]-E[x]E[\hat{u}]=E[x\hat{u}]=0$$
$$\to n^{-1}\sum_{i=1}^{n}x_i(y_i-\hat{\beta}_0-\hat{\beta}_1x_i)=0$$

### OLS Regression

The estimators $(\hat{\beta}_0,\hat{\beta}_1)$ are called the \textbf{ordinary least squares (OLS)} estimators of $\beta_0$ and $\beta_1$. A \textbf{fitted value} for $y$ when $x=x_i$ is defined as $$\hat{y}_i=\hat{\beta}_0+\hat{\beta}_1 x_i$$The \textbf{residual} for observation $i$ is the difference between the actual $y_i$ and its fitted value:$$\hat{u}_i=y_i-\hat{y}_i=y_i-\hat{\beta}_0-\hat{\beta}_1 x_i$$Thus, we defined the \textbf{OLS regression line} or \textbf{sample regression function (SRF)} as$$\hat{y}=\hat{\beta}_0+\hat{\beta}_1 x$$

\underline{Properties of OLS Statistics}
\begin{enumerate}
\item The sum, and therefore the sample average of the OLS residuals, is zero, such that$$\sum_{i=1}^{n}\hat{u}_i=0$$This result follows from FOC 1.
\item The sample covariance between the regressors and the OLS residuals is zero. This follows from FOC2, which can be written in terms of the residuals as$$\sum_{i=1}^{n}x_i\hat{u}_i=0$$
\item The point $(\bar{x},\bar{y})$ is always on the OLS regression line. In other words, if we take the OLS regression line and plug in $\bar{x}$ for $x$, then the predicted value is $\bar{y}$. This is exactly what the derivation from FOC 1 showed us.
\end{enumerate}

### Sum of Squares

We can view OLS as decomposing each $y_i$ into two parts, a fitted value and a residual. The fitted values and residuals are uncorrelated in the sample.

The \textbf{total sum of squares (SST)} can be defined as$$SST=\sum_{i=1}^{n}(y_i-\bar{y})^2$$and is a measure of the variation in the $y_i$ of the given sample. If we divided SST by $n-1$, we obtain the sample variance of $y$.

The \textbf{explained sum of squares (SSE)} can be defined as$$SSE=\sum_{i=1}^{n}(\hat{y}_i-\bar{y})^2$$and is a measure of the variation in the $\hat{y}_i$.

The \textbf{residual sum of squares (SSR)} can be defined as$$SSR=\sum_{i=1}^{n}(y_i-\hat{y})^2=\sum_{i=1}^{n}\hat{u}^2$$and measures the sample variation in the $\hat{u}_i$.

In total,$$SST=SSE+SSR$$

\underline{Proof}
$$SSE+SSR=\sum_{i=1}^{n}(\hat{y}_i-\bar{y})^2+\sum_{i=1}^{n}(y_i-\hat{y}_i)^2$$
$$=\sum_{i=1}^{n}(\hat{y}_i^2-2\bar{y}\hat{y}_i+\bar{y}^2)+\sum_{i=1}^{n}(y_i^2-2y_i\hat{y}_i+\hat{y}_i^2)$$
$$=\left[\sum_{i=1}^{n}\hat{y}_i^2-2\bar{y}\sum_{i=1}^{n}\hat{y}_i+\sum_{i=1}^{n}\bar{y}^2\right]+\left[\sum_{i=1}^{n}y_i^2-2\sum_{i=1}^{n}y_i\hat{y}_i+\sum_{i=1}^{n}\hat{y}_i^2\right]$$
$$=\left[\sum_{i=1}^{n}y_i^2-2n\bar{y}^2+\sum_{i=1}^{n}\bar{y}^2\right]+\left[\sum_{i=1}^{n}\hat{y}_i^2-2\sum_{i=1}^{n}y_i\hat{y}_i+\sum_{i=1}^{n}\hat{y}_i^2\right]$$
$$=\left[\sum_{i=1}^{n}y_i^2-2\bar{y}\sum_{i=1}^{n}y_i+\sum_{i=1}^{n}\bar{y}^2\right]+\left[\sum_{i=1}^{n}(2\hat{y}_i^2-2y_i\hat{y}_i)\right]$$
$$=\left[\sum_{i=1}^{n}(y_i^2-2\bar{y}y_i+\bar{y}^2)\right]+\left[\sum_{i=1}^{n}2\hat{y}_i(\hat{y}_i-y_i)\right]$$
$$=\sum_{i=1}^{n}(y_i-\bar{y})^2-2\sum_{i=1}^{n}\hat{y}_i(\hat{u}_i)$$
$$=SST-2\sum_{i=1}^{n}\hat{u}_i(\hat{\beta}_0+\hat{\beta_1}x_i)$$
$$=SST-2\left[\hat{\beta}_0\sum_{i=1}^{n}\hat{u}_i+\hat{\beta_1}\sum_{i=1}^{n}x_iu_i\right]=SST$$

The \textbf{R-squared} of the regression, sometimes called the \textbf{coefficient of determination}, is defined as$$R^2\equiv\frac{SSE}{SST}=1-\frac{SSR}{SST}$$
$R^2$is the ratio of the explained variation compared to the total variation; thus, it is interpreted as the fraction of the sample variation in $y$ that is explained by $x$. $R^2$ is equal to the square of the sample correlation coefficient between $y_i$ and $\hat{y}_i$.


### Nonlinear Models

In a linear regression model, changing the units of the variables has a multiplicative effect on the slope and intercepts but has not effect on the $R^2$.

In a nonlinear regression model that incorporates natural logarithms, changing the units has no effect on the slope but does change the intercept. Take for example, $\ln(y)=\beta_0+\beta_1\ln(x)+u$. If we multiply $y$ by some constant $c_1$ and $x$ by some constant $c_2$, the equation becomes
$$\ln(c_1y)=\beta_0+\beta_1\ln(c_2 x)+u$$
$$\to\ln(c_1)+\ln(y)=\beta_0+\beta_1\left[\ln(c_2)+\ln(x)\right]+u$$
$$\to\ln(y)=\alpha+\beta_1\ln(x)+u$$where $\alpha=\beta_0-\ln(c_1)+\beta_1\ln(c_2)$.

\underline{Summary of Functional Forms Involving Natural Logarithms}
\begin{tabular}{|lccc|}
\hline
Model & Dependent Variable & Independent Variable & Interpretation of $\beta_1$\\
\hline
Level-level (linear model) & $y$ & $x$ & $\Delta y=\beta_1 \Delta x$\\
Level-log & $y$ & $\ln(x)$ & $\Delta y=(\beta_1/100)\%\Delta x$\\
Log-level (semi-elasticity model) & $\ln(y)$ & $x$ & $\%\Delta y=(100\beta_1)\Delta x$\\
Log-log (constant elasticity model) & $\ln(y)$ & $\ln(x)$ & $\%\Delta y=\beta_1\%\Delta x$\\
\hline
\end{tabular}

### Gauss-Markov Assumptions

\begin{itemize}
\item Assumption 1: The population model is linear in parameters such that $$y_i=\alpha +\beta_1x_{i,1}+...+\beta_mx_{i,m}+u_i$$
\item Assumption 2: The sample data $\{x_{i,1},...x_{i,m},y_i\}$ is a random sample.
\item Assumption 3: The error term has a zero conditional mean such that $$E[u|x_{i,1},...x_{i,m}]=0$$
For a random sample, this assumption implies that $E[u_i|x_i]=0\ \forall\ i=1,2,\dots,n$
\item Assumption 4: The error term is homoskedastic such that $$Var(u|x_{i,1},...x_{i,m})=\sigma_u^2$$
\item Assumption 5: None of the regressors exhibit perfect collinearity with one another.
\item Assumption 6: There exists no serial correlation between the error terms such that $$cov(u_i,u_j)=0 \: \forall \: i\neq j$$Note: Assumption 2 is sufficient to satisfy assumption 6 in the case of cross-sectional data (see below for proof).
\end{itemize}

Under this set of assumptions, the \textbf{Gauss-Markov Theorem} states that the OLS estimator is BLUE (conditional on the sample values of the explanatory variable(s)).

\underline{Proof that Random Sampling Implies Zero Serial Correlation with Cross-Sectional Data}
Under assumption 2, we know the sample data is independent and identically distributed (i.i.d), meaning drawing one observation does not make drawing another observation any more or less likely and that the observations come from the same distribution. Thus, for some observation $i\neq j$,
$$cov(y_i,y_j)=E[y_i y_j]-E[y_i]E[y_j]=E[y_i]E[y_j]-E[y_i]E[y_j]=0$$
Similarly, by our first assumption that $y_i=\beta_0+\beta_1x_{i,1}+...+\beta_{i,m}x_{i,m}+u_i$
$$0=cov(y_i,y_j)=cov(\beta_0+\beta_1x_{i,1}+...+\beta_{i,m}x_{i,m}+u_i,\beta_0+\beta_1x_{j,1}+...+\beta_{j,m}x_{j,m}+u_j)$$
$$=\sum_{s=1}^m \sum_{t=1}^m \beta_s \beta_t cov(x_{i,s},x_{j,t}) + \sum_{s=1}^m \beta_s cov(x_{i,s},u_j) + \sum_{t=1}^m \beta_t cov(x_{j,t},u_i) + cov(u_i,u_j)$$
Using the fact that the sample is i.i.d., $$\sum_{s=1}^m \sum_{t=1}^m \beta_s \beta_t cov(x_{i,s},x_{j,t})=0$$ Also, by the exogeneity assumption, $$\sum_{s=1}^{m} \beta_s cov(x_{i,s},u_j) + \sum_{t=1}^m \beta_t cov(x_{j,t},u_i)=0$$ Leaving us with $$cov(u_i,u_j)=0$$

### Unbiasedness of OLS Estimators

$$E\left[\hat{\beta}_{1}\middle|x\right]=E\left[\frac{\sum_{i=1}^{n}y_i(x_i-\bar{x})}{\sum_{i=1}^{n}x_i(x_i-\bar{x})}\middle|x\right]$$
$$=E\left[\frac{\sum_{i=1}^{n}(\beta_0+\beta_1 x_i + u_i)(x_i-\bar{x})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}\middle|x\right]$$
$$=E\left[\frac{\beta_0\sum_{i=1}^{n}(x_i-\bar{x})+\beta_1\sum_{i=1}^{n}x_i(x_i-\bar{x}) +\sum_{i=1}^{n} u_i(x_i-\bar{x})}{SST_x}\middle|x\right]$$
$$=E\left[\frac{\beta_0(n\bar{x}-n\bar{x})+\beta_1\sum_{i=1}^{n}(x_i-\bar{x})^2 +\sum_{i=1}^{n} u_i(x_i-\bar{x})}{SST_x}\middle|x\right]$$
$$=E\left[\beta_1 + \frac{\sum_{i=1}^{n} u_i(x_i-\bar{x})}{SST_x}\middle|x\right]$$
$$=\beta_1 + \frac{E\left[\sum_{i=1}^{n} u_i(x_i-\bar{x})\middle|x\right]}{SST_x}$$
$$=\beta_1 + \frac{\sum_{i=1}^{n} E\left[u_i(x_i-\bar{x})\middle|x\right]}{SST_x}$$
$$=\beta_1 + \frac{\sum_{i=1}^{n} (x_i-\bar{x})E\left[u_i\middle|x\right]}{SST_x}=\beta_1$$
where we have used the fact that the expected value of each $u_i$ (conditional on $\{x_1, x_2,\dots, x_n\}$) is zero under the second and third Gauss-Markov assumptions. Because unbiasedness holds for any outcome on $\{x_1, x_2,\dots, x_n\}$, unbiasedness also holds without conditioning on $\{x_1, x_2,\dots, x_n\}$.

$$E\left[\hat{\beta}_0\middle|x\right]=E\left[\bar{y}-\hat{\beta}_1\bar{x}\middle|x\right]$$
$$=E\left[\bar{y}\middle|x\right]-E\left[\hat{\beta}_1\bar{x}\middle|x\right]$$
$$=E\left[n^{-1}\sum_{i=1}^{n}(\beta_0+\beta_1x_i+u_i)\middle|x\right]-\bar{x}E\left[\hat{\beta}_1\middle|x\right]$$
$$=E\left[\beta_0+\beta_1\bar{x}\middle|x\right]-\beta_1\bar{x}$$
$$=\beta_0+\beta_1\bar{x}-\beta_1\bar{x}=\beta_0$$

Unbiasedness generally fails if any of our first three assumptions fail. This means that it is important to think about the veracity of each assumption for a particular application. The first assumption requires that $y$ and $x$ be linearly related, with an additive disturbance. This can certainly fail. But we also know that $y$ and $x$ can be chosen to yield interesting nonlinear relationships. Random sampling can fail in a cross section when samples are not representative of the underlying population; in fact, some data sets are constructed by intentionally oversampling different parts of the population. Finally, the third assumption is possibly the most important of the Gauss-Markov assumptions. Using simple regression when $u$ contains factors affecting $y$ that are also correlated with $x$ can result in spurious correlation: that is, we find a relationship between $y$ and $x$ that is really due to other unobserved factors that affect $y$ and also happen to be correlated with $x$. In addition to omitted variables, there are other reasons for $x$ to be correlated with $u$ in the simple regression model.

### Sampling Variance of OLS Estimators

Using a previous derivation,
$$\hat{\beta}_1=\beta_1+\frac{\sum_{i=1}^{n}u_i(x_i-\bar{x})}{SST_x}$$

Thus,
$$\text{Var}\left(\hat{\beta}_1\middle|x\right)=\text{Var}\left(\beta_1+\frac{\sum_{i=1}^{n}u_i(x_i-\bar{x})}{SST_x}\middle|x\right)$$
$$=\frac{1}{SST_x^2}\text{Var}\left(\sum_{i=1}^{n}u_i(x_i-\bar{x})\middle|x\right)$$
$$=\frac{1}{SST_x^2}\sum_{i=1}^{n}\text{Var}\left(u_i(x_i-\bar{x})\middle|x\right)$$
$$=\frac{1}{SST_x^2}\sum_{i=1}^{n}(x_i-\bar{x})^2\text{Var}\left(u_i\middle|x\right)$$
$$=\frac{1}{SST_x^2}\sum_{i=1}^{n}(x_i-\bar{x})^2\sigma_u^2$$
$$=\frac{\sigma_u^2}{SST_x^2}\sum_{i=1}^{n}(x_i-\bar{x})^2$$
$$=\frac{\sigma_u^2}{SST_x}$$

Again, using a previous derivation,

$$\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}$$
$$\to\text{Var}\left(\hat{\beta}_0\middle|x\right)=\text{Var}\left(\bar{y}-\hat{\beta}_1\bar{x}\middle|x\right)$$
$$=\text{Var}\left(\bar{y}\middle|x\right)+\text{Var}\left(\hat{\beta}_1\bar{x}\middle|x\right)-2\text{Cov}\left(\bar{y},\hat{\beta}_1\bar{x}\middle|x\right)$$
$$=\text{Var}\left(\beta_0+\beta_1\bar{x}+\bar{u}\middle|x\right)+\bar{x}^2\text{Var}\left(\hat{\beta}_1\middle|x\right)-\frac{2\bar{x}}{n}\text{Cov}\left(\sum_{i=1}^{n}y_i,\hat{\beta}_1\middle|x\right)$$
$$=\text{Var}\left(\beta_1\bar{x}\middle|x\right)+\text{Var}\left(\bar{u}\middle|x\right)+2\text{Cov}\left(\beta_1\bar{x},\bar{u}\middle|x\right)+\bar{x}^2\frac{\sigma_u^2}{SST_x}-\frac{2\bar{x}}{n}\text{Cov}\left(\sum_{i=1}^{n}y_i,\beta_1+\frac{\sum_{i=1}^{n}u_i(x_i-\bar{x})}{SST_x}\middle|x\right)$$
$$=0+\frac{1}{n^2}\text{Var}\left(\sum_{i=1}^{n}u_i\middle|x\right)+2\cdot 0+\frac{\sigma_u^2\bar{x}^2}{SST_x}-\frac{2\bar{x}}{nSST_x}\text{Cov}\left(\sum_{i=1}^{n}y_i,\sum_{i=1}^{n}u_i(x_i-\bar{x})\middle|x\right)$$
$$=\frac{1}{n^2}\sum_{i=1}^{n}\text{Var}\left(u_i\middle|x\right)+\frac{\sigma_u^2\bar{x}^2}{SST_x}-\frac{2\bar{x}\sum_{i=1}^{n}(x_i-\bar{x})}{nSST_x}\text{Cov}\left(\sum_{i=1}^{n}y_i,\sum_{i=1}^{n}u_i\middle|x\right)$$
$$=\frac{1}{n^2}\sum_{i=1}^{n}\sigma_u^2+\frac{\sigma_u^2\bar{x}^2}{SST_x}-\frac{2\bar{x}(0)}{nSST_x}\text{Cov}\left(\sum_{i=1}^{n}y_i,\sum_{i=1}^{n}u_i\middle|x\right)$$
$$=\frac{\sigma_u^2}{n}+\frac{\sigma_u^2\bar{x}^2}{SST_x}$$
$$=\frac{SST_x\sigma_u^2+n\sigma_u^2\bar{x}^2}{nSST_x}$$
$$=\frac{\sigma_u^2(SST_x+n\bar{x}^2)}{nSST_x}$$
$$=\frac{\sigma_u^2(\sum_{i=1}^{n}(x_i-\bar{x})^2+n\bar{x}^2)}{nSST_x}$$
$$=\frac{\sigma_u^2(\sum_{i=1}^{n}(x_i^2+\bar{x}^2-2\bar{x}x_i)+n\bar{x}^2)}{nSST_x}$$
$$=\frac{\sigma_u^2(\sum_{i=1}^{n}(x_i^2)+n\bar{x}^2-2n\bar{x}^2+n\bar{x}^2)}{nSST_x}$$
$$=\frac{\sfrac{\sigma_u^2}{n}\sum_{i=1}^{n}x_i^2}{SST_x}$$
Generally, we are interested in $\text{Var}\left(\hat{\beta}_1\right)$. To summarize how this variance depends on the error variance, $\sigma_u^2$, and the total variation in $\{x_1,x_2,\dots,x_n\}$ $SST_x$: 
\begin{enumerate}
\item The larger the error variance, the larger is $\text{Var}\left(\hat{\beta}_1\right)$. This makes sense because more variation in the unobservables affecting $y$ makes it more difficult to precisely estimate $\beta_1$.
\item More variability in the independent variable is preferred: as the variability in the $x_i$ increases, the variance of $\hat{\beta}_1$ decreases. This also makes intuitive sense because the more spread out the sample of independent variables is, the easier it is to trace out the relationship between $E[y|x]$ and $x$ (i.e., it becomes easier to estimate $\beta_1$). If there is little variation in the $x_i$, then it can be hard to pinpoint how $E[y|x]$ varies with $x$. As the sample size increases, so does the total variation in the $x_i$. Therefore, a larger sample size results in a smaller variance for $\hat{\beta}_1$.
\end{enumerate}

These formulas allow us to isolate the factors that contribute to $\text{Var}\left(\hat{\beta}_0\right)$ and $\text{Var}\left(\hat{\beta}_1\right)$. But these formulas are unknown, except in the extremely rare case that $\sigma_u^2$ is known. Considering the $u_i$ are unobserved, an unbiased estimator of $\sigma_u^2$ is$$\hat{\sigma}_u^2=\frac{\sum_{i=1}^{n}\hat{u}_i^2}{n-2},$$ where the $n-2$ is the degrees of freedom in the OLS residuals due to the two OLS first order conditions:$$\sum_{i=1}^{n}\hat{u}_i=0,\sum_{i=1}^{n}x_i\hat{u}_i=0$$

Naturally, the estimators of $\text{Var}\left(\hat{\beta}_0\right)$ and $\text{Var}\left(\hat{\beta}_1\right)$ become
$$\hat{\sigma}_{\beta_0}^2=\frac{\sfrac{\sum_{i=1}^{n}\hat{u}_i^2}{n(n-2)}+\sum_{i=1}^{n}x_i^2}{SST_x}$$
and
$$\hat{\sigma}_{\beta_1}^2=\frac{\sfrac{\sum_{i=1}^{n}\hat{u}_i^2}{(n-2)}}{SST_x}$$

Additionally, the natural estimator of $\sigma_u$ is$$\hat{\sigma}_u=\sqrt{\hat{\sigma}_u^2},$$which is called the \textbf{standard error of the regresion (SER)}.

Although $\hat{\sigma}_u^2$ is an unbiased and consistent estimator of $\sigma_u^2$, $\hat{\sigma}_u$ is consistent but biased.

All together the \textbf{standard errors of the coefficients} are
$$\text{se}_{\beta_0}=\sqrt{\frac{\sfrac{\sum_{i=1}^{n}\hat{u}_i^2}{n(n-2)}+\sum_{i=1}^{n}x_i^2}{SST_x}}$$
and
$$\text{se}_{\beta_1}=\sqrt{\frac{\sfrac{\sum_{i=1}^{n}\hat{u}_i^2}{(n-2)}}{SST_x}}$$

\underline{A Note on the Conditioning on x}

In addition to restricting the relationship between $u$ and $x$ in the population, the zero conditional mean assumption—coupled with the random sampling assumption—allows for a convenient technical simplification. In particular, we can derive the statistical properties of the OLS estimators as conditional on the values of the $x_i$ in our sample. Technically, in statistical derivations, conditioning on the sample values of the independent variable is the same as treating the $x_i$ as fixed in repeated samples, which we think of as follows. We first choose $n$ sample values for $\{x_1, x_2,\dots, x_n\}$. Given these values, we then obtain a sample on $y$. Next, another sample of $y$ is obtained, using the same values for $\{x_1, x_2,\dots, x_n\}$. Then another sample of $y$ is obtained, again using the same $\{x_1, x_2,\dots, x_n\}$. And so on. While the fixed-in-repeated-samples scenario is not very realistic in non-experimental contexts, random sampling, where individuals are chosen randomly, is representative of how most data sets are obtained for empirical analysis in social sciences. Once we assume that $E[u|x]=0$, and we have random sampling, nothing is lost in derivations by treating the $x_i$ as nonrandom. The danger is that the fixed-in-repeated-samples assumption always implies that $u_i$ and $x_i$ are independent.

### Regression through the Origin

In some cases, we may wish to impose the restriction that, when $x=0$, $E[y]=0$. In these cases we perform \textbf{regression through the origin}, which takes the form$$\tilde{y}=\tilde{\beta}_1x,$$where the tildes are used to distinguish this problem from the much more common problem of estimating an intercept along with a slope. We still rely on the method of OLS to obtain the slope estimate. Thus, we must solve the for the $\tilde{\beta}_1$ that minimizes$$SSR=\sum_{i=1}^{n}\left(y_i-\tilde{\beta_1}x_i\right)^2$$Taking the partial derivative with respect to $\tilde{\beta}_1$, we obtain$$\frac{\partial SSR}{\partial\tilde{\beta}_1}=-2\sum_{i=1}^{n}x_i\left(y_i-\tilde{\beta}_1x_i\right)=0$$Solving for $\tilde{\beta}_1$:
$$\sum_{i=1}^{n}x_i\left(y_i-\tilde{\beta}_1x_i\right)=0$$
$$\to\sum_{i=1}^{n}x_iy_i-\sum_{i=1}^{n}\tilde{\beta}_1x_i^2=0$$
$$\to\tilde{\beta}_1\sum_{i=1}^{n}x_i^2=\sum_{i=1}^{n}x_iy_i$$
$$\to\tilde{\beta}_1=\frac{\sum_{i=1}^{n}x_iy_i}{\sum_{i=1}^{n}x_i^2}$$
Obtaining an estimate of $\beta_1$ using regression through the origin is not done very often in applied work, and for good reason: if the intercept b0 2 0, then $\tilde{\beta}_1$ is a biased estimator of $\beta_1$. In cases where it is appropriate, the $R$-squared is computed as $$1-\frac{\sum_{i=1}^{n}\left(y_i-\tilde{\beta}_1x_i\right)^2}{\sum_{i=1}^{n}y_i^2}=1-\frac{SSR}{SST}$$

Note: If we only regress on a constant, (i.e., we set the slope to zero and estimate an intercept only), the intercept that minimizes the sum of squared deviations is $\bar{y}$.

### Regression on a Binary Explanatory Variable

Simple regression can also be applied to the case where x is a \textbf{binary variable}, often called a \textbf{dummy variable} in the context of regression analysis. When the explanatory variable is binary, the PRF takes two forms (under the exogeneity assumption):$$E[y|x]=E\left[\beta_0+\beta_1x+u|x\right]=\beta_0+\beta_1x$$

\begin{enumerate}
\item $E[y|x=0]=\beta_0$
\item $E[y|x=1]=\beta_0+\beta_1$
\end{enumerate}

It follows that$$\beta_1=E[y|x=1]-E[y|x=0]$$

In cases where we hope to study the effect of an intervention or new policy, the idea of counterfactuals or potential outcomes are used. Define, the \textbf{control group} as those not subject to the intervention or new policy act and the \textbf{treatment group} as those subject to the intervention. Then, the causal (or treatment) effect of the intervention for unit $i$ is$$te_i=y_i(1)-y_i(0),$$the difference between the two potential outcomes. A noteworthy items about $te_i$is it is not observed for any unit $i$ because it depends on both counterfactuals. We cannot hope to estimate tei for each unit i. Instead, the focus is typically on the \textbf{average treatment effect (ATE)}, also called the \textbf{average causal effect (ACE)}. The ATE is simply the average of the treatment effects across the entire population. We can write the ATE parameter as $$\tau_{ate}=E[te_i]=E[y_i(1)-y_i(0)]=E[y_i(1)]-E[y_i(0)]$$

For each unit $i$ let $x_i$ be the program participation status—a binary variable. Then the observed outcome, $y_i$, can be written as$$y_i=(1-x_i)y_i(0)+x_iy_i(1)=y_i(0)+[y_i(1)-y_i(0)]x_i$$
Imposing a usually unrealistic assumption of a constant treatment effect$$\to y_i=y_i(0)+\tau x_i$$
Rewriting $y_i(0)$ as $\beta_0+u_i$ and $\tau$ as $\beta_1$,$$\to y_i=\beta_0+\tau x_i+u_i$$

If $x_i \indep u_i$, then $\tau$ is the unbiased estimator of the treatment effect. The assumption that $x_i \indep u_i$ is the same as $x_i \indep y_i(0)$. This assumption can be guaranteed only under \textbf{random assignment}, whereby units are assigned to the treatment and control groups using a randomization mechanism that ignores any features of the individual units. Random assignment is the hallmark of a \textbf{randomized controlled trial (RCT)}, which is considered the gold standard for determining whether medical interventions have causal effects.

## Exercises

### Problems

\begin{enumerate}
\item 
\begin{enumerate}
\item Among the many factors contained in $u$ are marriage status, desire to have kids, health, and wealth. It's likely that at least wealth is correlated with level of education.
\item No. Since it's likely $u$ is correlated with $kids$ and $educ$, a simple regression analysis will likely give a biased estimate of $\beta_1$.
\end{enumerate}
\item Rewriting the model as $y=\beta_0+\beta_1x+u+\alpha_0-\alpha_0$, define a new error term $e=u-\alpha_0$ and a new intercept $\gamma_0=\alpha_0+\beta_0$. The model becomes $y=\gamma_0+\beta_1x+e$. It follows that $E[e]=E[u-\alpha_0]=E[u]-E[\alpha_0]=\alpha_0-\alpha_0-0$.
\item 
\end{enumerate}
```{r Chapter 2 Exercise 3, include=TRUE,comment=NA,warning=FALSE,echo=FALSE}
GPA <- c(2.8,3.4,3.0,3.5,3.6,3.0,2.7,3.7)
ACT <- c(21,24,26,27,29,25,25,30)
lm1 <- lm(GPA ~ ACT)
cat("a) ")
summary(lm1)
cat("The slope tells us there is a positive correlation between GPA and ACT. The intercept gives")
cat("us an estimate of GPA if a student scores a 0 on the ACT. GPA is predicted to be")
cat(lm1$coefficients[2]*5, "higher if the ACT score increases by five points.")

cat("b) ")
cat("Fitted values = (", paste(as.character(round(lm1$fitted.values,4)), collapse=", "),")")
cat("Residuals = (", paste(as.character(round(lm1$residuals, 4)), collapse=", "),")")
cat("Sum of residuals = ", sum(lm1$residuals))

cat("c) ")
cat("Predicted value of GPA when ACT=20: ", lm1$coefficients[1]+lm1$coefficients[2]*20)

cat("d) ")
cat("The coefficient of determination (R-squared)=", unname(unlist(summary(lm1)["r.squared"])))

rm(lm1,GPA,ACT)
invisible(gc())
```

\begin{enumerate}
\setcounter{enumi}{3}
\item 
\begin{enumerate}
\item The predicted birth weight when $cigs = 0$ is 119.77 ounces. When $cigs=20$, the predicted birth weight is `r 119.77-0.514*20` ounces. This regression analysis suggests a constant loss to birth weight for every additional cigarette smoked. In this case, we would an `r 0.514*20` ounce loss in birth weight for each addition 20 cigarettes smoked.
\item No. Other factors correlated with $cigs$ and $bwght$ are not controlled for in the model. Such factors may include the health of the mother and genetics. This leads to a biased slope estimate.
\item Based on this model, $cigs$ must equal `r (125-119.77)/-0.514` for a birth weight of 125 ounces. Obviously, this doesn't have much meaning because you can't smoke a negative number of cigarettes.
\end{enumerate}
\item 
\begin{enumerate}
\item The intercept predicts a -124.84 dollar consumption value for a family with no annual income. The slope predicts an extra 0.853 dollars of consumption for each additional dollar of annual income. The intercept offers little meaning because you can't consume a negative amount. On the other hand, the slope coefficient signifies a positive correlation between consumption and annual income. Personally, the slope coefficient seems to large to me at first glance.
\item The predicted consumption when family income is \$30,000 = \$`r -124.84 + 0.853*30000`.
\item 
\end{enumerate}
\end{enumerate}
```{r Chapter 2 Exercise 5 Graph, echo=FALSE,include=TRUE,fig.align='right',fig.height=3}
inc <- seq(0, 200000, 5000)
cons <- -124.84 + 0.853 * inc
mpc <- 0.853
apc <- cons / inc

ggplot()+
  geom_line(aes(x = inc, y = mpc, color = "green"))+
  geom_line(aes(x = inc, y = apc, color = "red"))+
  scale_color_identity(name = "Propensity to Consume",
                          breaks = c("green", "red"),
                          labels = c("MPC", "APC"),
                          guide = "legend")+
  theme_classic()+
  xlab("Annual Income")+
  ylab("")
```

\begin{enumerate}
\setcounter{enumi}{5}
\item 
\begin{enumerate}
\item The slope is an estimated elasticity of $price$ with respect to $dist$. The estimate indicates a 0.312\% increase in $price$ for every percent increase in $dist$. The sign is as we'd expect. As houses move further away from the garbage incinerator, we'd expect housing prices to increase.
\item No. As the problem suggests, it's likely the city put the incinerator close to cheaper homes. Thus, the elasticity estimate is likely upwardly biased and overstates the impact of $dist$ on $price$.
\item Other factors include the size of the home, the number of rooms, the quality of the surrounding area (with regards to safety, education, etc.), and the location. As suggested in part(c), these factors are likely correlated with $dist$. 
\end{enumerate}
\item 
\begin{enumerate}
\item $E\left[u\middle|inc\right]=E\left[\sqrt{inc}\cdot e\middle|inc\right]=\sqrt{inc}\cdot E\left[e\middle|inc\right]=\sqrt{inc}\cdot E\left[e\right]=0$. Recall that $E\left[e\middle|inc\right]=E\left[e\right]$ because we are assuming that $e$ is independent of $inc$.
\item $\text{Var}\left(u\middle|inc\right)=\text{Var}\left(\sqrt{inc}\cdot e\middle|inc\right)=\left(\sqrt{inc}\right)^2\text{Var}\left(e\middle|inc\right)=inc\text{Var}\left(e\right)=\sigma_e^2inc$. Recall that $\text{Var}\left(e\middle|inc\right)=\text{Var}\left(e\right)$ because we are assuming that $e$ is independent of $inc$.
\item This notion makes sense because the greater the amount of income, the more options individuals have with that income. Some individuals may choose to spend their income while others will choose to save it. On the other hand, lower earning individuals have fewer options on how to spend their money. It's likely they have to spend a larger portion of their income on necessary consumption items and have a smaller amounts they can save.
\end{enumerate}
\item 
\begin{enumerate}
\item From the notes,$$\tilde{\beta}_1=\frac{\sum_{i=1}^{n}x_iy_i}{\sum_{i=1}^{n}x_i^2}$$
$$\to E\left[\tilde{\beta}_1\middle|x\right]=E\left[\frac{\sum_{i=1}^{n}x_iy_i}{\sum_{i=1}^{n}x_i^2}\middle|x\right]$$
$$=\frac{1}{\sum_{i=1}^{n}x_i^2}E\left[\sum_{i=1}^{n}x_iy_i\middle|x\right]$$
$$=\frac{1}{\sum_{i=1}^{n}x_i^2}\sum_{i=1}^{n}E\left[x_iy_i\middle|x\right]$$
$$=\frac{1}{\sum_{i=1}^{n}x_i^2}\sum_{i=1}^{n}x_iE\left[\beta_0+\beta_1x_i+u_i\middle|x\right]$$
$$=\frac{1}{\sum_{i=1}^{n}x_i^2}\sum_{i=1}^{n}\left(\beta_0 x_i+\beta_1x_i^2\right)$$
$$=\frac{1}{\sum_{i=1}^{n}x_i^2}\left(n\beta_0\bar{x}+\sum_{i=1}^{n}\beta_1x_i^2\right)$$
$$=\frac{n\beta_0\bar{x}+\beta_1\sum_{i=1}^{n}x_i^2}{\sum_{i=1}^{n}x_i^2}$$
Thus,$$E\left[\tilde{\beta}_1\middle|x,\beta_0=0\right]=\frac{\beta_1\sum_{i=1}^{n}x_i^2}{\sum_{i=1}^{n}x_i^2}=\beta_1$$
Another case in which $\tilde{\beta}_1$ is unbiased is when $\bar{x}=\sum_{i=1}^{n}x_i=0$.
\item 
$$\text{Var}\left(\tilde{\beta}_1\middle|x\right)=\text{Var}\left(\frac{\sum_{i=1}^{n}x_iy_i}{\sum_{i=1}^{n}x_i^2}\middle|x\right)$$
$$=\frac{1}{\left(\sum_{i=1}^{n}x_i^2\right)^2}\text{Var}\left(\sum_{i=1}^{n}x_i\left(\beta_0+\beta_1x_i+u_i\right)\middle|x\right)$$
$$=\frac{1}{\left(\sum_{i=1}^{n}x_i^2\right)^2}\text{Var}\left(\sum_{i=1}^{n}\left(\beta_0x_i+\beta_1x_i^2+x_iu_i\right)\middle|x\right)$$
$$=\frac{1}{\left(\sum_{i=1}^{n}x_i^2\right)^2}\sum_{i=1}^{n}\text{Var}\left(\beta_0x_i+\beta_1x_i^2+x_iu_i\middle|x\right)$$
$$=\frac{1}{\left(\sum_{i=1}^{n}x_i^2\right)^2}\sum_{i=1}^{n}x_i^2\text{Var}\left(u_i\middle|x\right)$$
$$=\frac{\sigma_u^2}{\left(\sum_{i=1}^{n}x_i^2\right)^2}\sum_{i=1}^{n}x_i^2$$
$$=\frac{\sigma_u^2}{\sum_{i=1}^{n}x_i^2}$$
\item $$\text{Var}\left(\tilde{\beta}_1\right)\leq\text{Var}\left(\hat{\beta}_1\right)$$
$$\to \frac{\sigma_u^2}{\sum_{i=1}^{n}x_i^2}\leq\frac{\sigma_u^2}{\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2}$$
$$\to \frac{1}{\sum_{i=1}^{n}x_i^2}\leq\frac{1}{\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2}$$
$$\to \sum_{i=1}^{n}x_i^2\geq\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2,$$which is given to us by the hint.
\item Under the Gauss Markov assumptions, $\hat{\beta}_1$ is always unbiased while $\tilde{\beta}_1$ requires the additional constraint that $y=0$ when $x=0$ to be unbiased. As we just proved, $\tilde{\beta}_1$ is efficient relative to $\hat{\beta}_1$. Thus, in the case that both estimators are unbiased, $\tilde{\beta}_1$ is preferred.
\end{enumerate}
\item 
\begin{enumerate}
\item As the problem suggests, using our previous derivation of $\hat{\beta}_1$,
$$\tilde{\beta}_1=\frac{\sum_{i=1}^{n}\left((c_2x_i-c_2\bar{x})c_1y_i\right)}{\sum_{i=1}^{n}\left(c_2x_i-c_2\bar{x}\right)^2}=\frac{c_1c_2\sum_{i=1}^{n}\left((x_i-\bar{x})y_i\right)}{c_2^2\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2}=\frac{c_1}{c_2}\hat{\beta}_1$$

We've previously derived $\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}$. Plugging in the new values and solving for $\tilde{\beta}_0$,$$\tilde{\beta}_0=c_1\bar{y}-\frac{c_1}{c_2}\hat{\beta}_1(c_2\bar{x})=c_1\left(\bar{y}-\hat{\beta}_1\bar{x}\right)=c_1\hat{\beta}_0$$
\item Again using our previous derivation of $\hat{\beta}_1$,
$$\tilde{\beta}_1=\frac{\sum_{i=1}^{n}\left(((c_2+x_i)-(c_2+\bar{x}))(c_1+y_i)\right)}{\sum_{i=1}^{n}\left((c_2+x_i)-(c_2+\bar{x})\right)^2}=\frac{\sum_{i=1}^{n}\left((x_i-\bar{x})(c_1+y_i)\right)}{\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2}=\frac{c_1\sum_{i=1}^{n}(x_i-\bar{x})+ \sum_{i=1}^{n}(x_i-\bar{x})y_i}{\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2}=\frac{\sum_{i=1}^{n}(x_i-\bar{x})y_i}{\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2}=\hat{\beta}_1$$

Using $\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}$ and plugging in the new values, $$\tilde{\beta}_0=(c_1+\bar{y})-\hat{\beta}_1(c_2+\bar{x})=\left(\bar{y}-\hat{\beta}_1\bar{x}\right)+c_1-c_2\hat{\beta}_1=\hat{\beta}_0+c_1-c_2\hat{\beta}_1$$
\item Changing the model from $\ln(y_i)=\hat{\beta_0}+\hat{\beta_1}x_i$ to $\ln(c_1y_i)=\tilde{\beta_0}+\tilde{\beta_1}x_i$, the new model can be rewritten as:$$\ln(c_1)+\ln(y_i)=\tilde{\beta_0}+\tilde{\beta_1}x_i$$Trivially, by subtracting $\ln(c_1)$ from both sides, we see $\tilde{\beta}_0=\hat{\beta}_0+\ln(c_1)$ and $\tilde{\beta}_1=\hat{\beta}_1$.
\item Changing the model from $y_i=\hat{\beta_0}+\hat{\beta_1}\ln(x_i)$ to $y_i=\tilde{\beta_0}+\tilde{\beta_1}\ln(c_2x_i)$, the new model can be rewritten as:$$y_i=\tilde{\beta_0}+\tilde{\beta_1}\left(\ln(c_2)+\ln(x_i)\right)$$Again, it's trivial to see that $\tilde{\beta}_0=\hat{\beta}_0-\tilde{\beta}_1\ln(c_2)=\hat{\beta}_0-\hat{\beta}_1\ln(c_2)$ and $\tilde{\beta}_1=\hat{\beta}_1$.
\end{enumerate}
\item 
\begin{enumerate}
\item $$\hat{\beta}_1=\frac{\sum_{i=1}^{n}(x_i-\bar{x})y_i}{\sum_{i=1}^{n}(x_i-\bar{x})^2}$$
$$=\frac{1}{SST_x}\sum_{i=1}^{n}(x_i-\bar{x})(\beta_0+\beta_1x_i+u_i)$$
$$=\frac{1}{SST_x}\left[\beta_0\sum_{i=1}^{n}(x_i-\bar{x})+\beta_1\sum_{i=1}^{n}x_i(x_i-\bar{x})+\sum_{i=1}^{n}u_i(x_i-\bar{x})\right]$$
$$=\frac{1}{SST_x}\left[\beta_0\left(n\bar{x}-n\bar{x}\right)+\beta_1\sum_{i=1}^{n}(x_i-\bar{x})^2+\sum_{i=1}^{n}u_i(x_i-\bar{x})\right]$$
$$=\beta_1+\frac{1}{SST_x}\sum_{i=1}^{n}u_i(x_i-\bar{x})$$
$$=\beta_1+\sum_{i=1}^{n}\frac{d_iu_i}{SST_x}$$
$$=\beta_1+\sum_{i=1}^{n}w_iu_i$$
\item $$E\left[\left(\hat{\beta}_1-\beta_1\right)\cdot\bar{u}\middle|x\right]=E\left[\left(\beta_1+\sum_{i=1}^{n}w_iu_i-\beta_1\right)\cdot\bar{u}\middle|x\right]$$
$$=E\left[\sum_{i=1}^{n}\bar{u}w_iu_i\middle|x\right]$$
$$=\sum_{i=1}^{n}E\left[\bar{u}w_iu_i\middle|x\right]$$
$$=\sum_{i=1}^{n}w_iE\left[\bar{u}u_i\middle|x\right]$$
$$=\sum_{i=1}^{n}w_iE\left[\bar{u}u_i\middle|x\right]$$
Because $u_i$ and $u_j$ are pairwise uncorrelated for all $i\neq j$, $E[u_iu_j]=E[u_i]E[u_j]=0\cdot0=0$. Thus, $E\left[\bar{u}u_i\middle|x\right]=\frac{1}{n}E\left[u_i^2|x\right]=\frac{1}{n}\bigg\{E\left[u_i^2|x\right]-E\left[u_i|x\right]^2\bigg\}=\frac{1}{n}\bigg\{\text{Var}(u_i|x)\bigg\}=\frac{\sigma_u^2}{n}$. Hence,
$$\sum_{i=1}^{n}w_iE\left[\bar{u}u_i\middle|x\right]=\frac{\sigma_u^2}{n}\sum_{i=1}^{n}w_i=\frac{\sigma_u^2}{n}\cdot 0=0$$
\item $$\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}=\beta_0+\beta_1\bar{x}+\bar{u}-\hat{\beta}_1\bar{x}=\beta_0+\bar{u}-\left(\hat{\beta}_1-\beta_1\right)\bar{x}$$
\item $$\text{Var}\left(\hat{\beta}_0\middle|x\right)=\text{Var}\left(\beta_0+\bar{u}-\left(\hat{\beta}_1-\beta_1\right)\bar{x}\middle|x\right)$$
$$=\text{Var}\left(\bar{u}-\hat{\beta}_1\bar{x}\middle|x\right)$$
$$=\text{Var}\left(\bar{u}\middle|x\right)+\bar{x}^2\text{Var}\left(\hat{\beta}_1\middle|x\right)-2\bar{x}\text{Cov}\left(\bar{u},\hat{\beta}_1\middle|x\right)$$
$$=\text{Var}\left(\frac{1}{n}\sum_{i=1}^{n}u_i\middle|x\right)+\bar{x}^2\text{Var}\left(\beta_1+\frac{\sum_{i=1}^{n}u_i(x_i-\bar{x})}{SST_x}\middle|x\right)-2\bar{x}\text{Cov}\left(\bar{u},\beta_1+\frac{\sum_{i=1}^{n}u_i(x_i-\bar{x})}{SST_x}\middle|x\right)$$
$$=\frac{1}{n^2}\text{Var}\left(\sum_{i=1}^{n}u_i\middle|x\right)+\frac{\bar{x}^2}{SST_x^2}\text{Var}\left(\sum_{i=1}^{n}u_i(x_i-\bar{x})\middle|x\right)-\frac{2\bar{x}}{SST_x}\text{Cov}\left(\frac{1}{n}\sum_{i=1}^{n}u_i,\sum_{i=1}^{n}u_i(x_i-\bar{x})\middle|x\right)$$
$$=\frac{1}{n^2}\sum_{i=1}^{n}\text{Var}\left(u_i\middle|x\right)+\frac{\bar{x}^2}{SST_x^2}\sum_{i=1}^{n}\text{Var}\left(u_i(x_i-\bar{x})\middle|x\right)-\frac{2\bar{x}\sum_{i=1}^{n}(x_i-\bar{x})}{nSST_x}\text{Cov}\left(\sum_{i=1}^{n}u_i,\sum_{i=1}^{n}u_i\middle|x\right)$$
$$=\frac{1}{n^2}\sum_{i=1}^{n}\sigma_u^2+\frac{\bar{x}^2}{SST_x^2}\sum_{i=1}^{n}(x_i-\bar{x})^2\text{Var}\left(u_i\middle|x\right)-\frac{2\bar{x}(n\bar{x}-n\bar{x})}{nSST_x}\text{Cov}\left(\sum_{i=1}^{n}u_i,\sum_{i=1}^{n}u_i\middle|x\right)$$
$$=\frac{\sigma_u^2}{n}+\frac{\sigma_u^2\bar{x}^2}{SST_x^2}\sum_{i=1}^{n}(x_i-\bar{x})^2-0$$
$$=\frac{\sigma_u^2}{n}+\frac{\sigma_u^2\bar{x}^2}{SST_x}$$
\item $$\frac{\sigma_u^2}{n}+\frac{\sigma_u^2\bar{x}^2}{SST_x}=\frac{\sigma_u^2SST_x}{nSST_x}+\frac{n\sigma_u^2\bar{x}^2}{nSST_x}$$
$$=\frac{\sigma_u^2SST_x+n\sigma_u^2\bar{x}^2}{nSST_x}$$
$$=\frac{\sum_{i=1}^{n}\left(\sigma_u^2\left(x_i-\bar{x}\right)^2\right)+n\sigma_u^2\bar{x}^2}{nSST_x}$$
$$=\frac{\sum_{i=1}^{n}\left(\sigma_u^2x_i^2-2\sigma_u^2x_i\bar{x}+\sigma_u^2\bar{x}^2\right)+n\sigma_u^2\bar{x}^2}{nSST_x}$$
$$=\frac{\sigma_u^2\sum_{i=1}^{n}x_i^2-2\sigma_u^2\bar{x}\sum_{i=1}^{n}x_i+\sigma_u^2\sum_{i=1}^{n}\bar{x}^2+n\sigma_u^2\bar{x}^2}{nSST_x}$$
$$=\frac{\sigma_u^2\sum_{i=1}^{n}x_i^2-2n\sigma_u^2\bar{x}^2+n\sigma_u^2\bar{x}^2+n\sigma_u^2\bar{x}^2}{nSST_x}$$
$$=\frac{\sigma_u^2n^{-1}\sum_{i=1}^{n}x_i^2}{\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2}$$
\end{enumerate}
\item
\begin{enumerate}
\item I would randomly assign the students to a wide variety of $hours$ and then measure the SAT results.
\item Two factors that are likely contained in $u$ are intelligence and cognitive clarity (on the day of the test). It's difficult to say whether intelligence has a positive or negative correlation with $hours$, but it's likely that cognitive clarity is positively correlated with $hours$ because more preparation tends to improve a student's ability to remain focused during the exam.
\item $\beta_1$ should be positive.
\item $\beta_0$ is the expected $sat$ for a student that spends 0 hours in the SAT preparation course.
\end{enumerate}
\item 
\begin{enumerate}
\item $$\min_{b_0}SSR = \sum_{i=1}^{n}\left(y_i-b_0\right)^2$$
$$\frac{\partial{SSR}}{\partial{b_0}} = \sum_{i=1}^{n}-2\left(y_i-b_0\right)=0$$
$$\to\sum_{i=1}^{n}y_i-\sum_{i=1}^{n}b_0=0$$
$$\to n\bar{y}=nb_0$$
$$\to b_0^*=\tilde{\beta}_0=\bar{y}$$
\item $\sum_{i=1}^{n}\tilde{u_i}=\sum_{i=1}^{n}\left(y_i-\bar{y}\right)=n\bar{y}-n\bar{y}=0$
\end{enumerate}
\item 
\begin{enumerate}
\item $1-x_i$ equals 1 when $x_i=0$ and equals 0 otherwise. On the other hand, $x_i$ equals one when $x_i=1$ and equals 0 otherwise. Thus, $\sum_{i=1}^{n}(1-x_i)$ equals the number of observations with $x_i=0$ and $\sum_{i=1}^{n}x_i$ equals the number of observations with $x_i=1$.
$$\bar{x}=\frac{1}{n}\sum_{i=1}^{n}x_i=\frac{n_1}{n}$$$\bar{x}$ is the proportion of observations where $x_i=1$.
\item $$\bar{y}_0=\frac{1}{n_0}\sum_{i=1}^{n}y_i(0)=n_0^{-1}\sum_{i=1}^{n}(1-x_i)y_i$$
$$\bar{y}_1=\frac{1}{n_1}\sum_{i=1}^{n}y_i(1)=n_1^{-1}\sum_{i=1}^{n}x_iy_i$$
\item $\bar{y}=\frac{n_0}{n}\bar{y}_0+\frac{n_1}{n}\bar{y}_1=\bar{y}_0\left[\frac{1}{n}\sum_{i=1}^{n}\left(1-x_i\right)\right]+\bar{y}_1\left[\frac{1}{n}\sum_{i=1}^{n}x_i\right]=\bar{y}_0\left[\frac{1}{n}\left(n-n\bar{x}\right)\right]+\bar{y}_1\left[\frac{1}{n}\left(n\bar{x}\right)\right]=\left(1-\bar{x}\right)\bar{y}_0+\bar{x}\bar{y}_1$
\item $n^{-1}\sum_{i=1}^{n}x_i^2-\bar{x}^2=n^{-1}\sum_{i=1}^{n}x_i-\bar{x}^2=n^{-1}\left(n\bar{x}\right)-\bar{x}^2=\bar{x}-\bar{x}^2=\bar{x}\left(1-\bar{x}\right)$
\item $$n^{-1}\sum_{i=1}^{n}x_iy_i-\bar{x}\bar{y}=\frac{n_1}{n}\bar{y}_1-\bar{x}\left((1-\bar{x})\bar{y}_0+\bar{x}\bar{y}_1\right)$$
$$=\bar{x}\bar{y}_1-\bar{x}\left(\bar{y}_0-\bar{x}\bar{y}_0+\bar{x}\bar{y}_1\right)$$
$$=\bar{x}\bar{y}_1-\bar{x}\bar{y}_0+\bar{x}^2\bar{y}_0-\bar{x}^2\bar{y}_1$$
$$=\bar{x}\left(\bar{y}_1-\bar{y}_0+\bar{x}\bar{y}_0-\bar{x}\bar{y}_1\right)$$
$$=\bar{x}\left(\bar{y}_1(1-\bar{x})-\bar{y}_0(1-\bar{x})\right)$$
$$=\bar{x}(1-\bar{x})(\bar{y}_1-\bar{y}_0)$$
\item $$\hat{\beta}_1=\frac{\sum_{i=1}^{n}(x_i-\bar{x})y_i}{\sum_{i=1}^{n}(x_i-\bar{x})^2}$$
$$=\frac{\sum_{i=1}^{n}x_iy_i-\bar{x}\sum_{i=1}^{n}y_i}{\sum_{i=1}^{n}x_i^2-2\bar{x}\sum_{i=1}^{n}x_i+\sum_{i=1}^{n}\bar{x}^2}$$
$$=\frac{\sum_{i=1}^{n}x_iy_i-n\bar{x}\bar{y}}{\sum_{i=1}^{n}x_i^2-2n\bar{x}^2+n\bar{x}^2}$$
$$=\frac{n^{-1}\big\{\sum_{i=1}^{n}x_iy_i-n\bar{x}\bar{y}\big\}}{n^{-1}\big\{\sum_{i=1}^{n}x_i^2-n\bar{x}^2\big\}}$$
$$=\frac{n^{-1}\sum_{i=1}^{n}x_iy_i-\bar{x}\bar{y}}{n^{-1}\sum_{i=1}^{n}x_i^2-\bar{x}^2}$$
$$=\frac{\bar{x}(1-\bar{x})(\bar{y}_1-\bar{y}_0)}{\bar{x}(1-\bar{x})}=\bar{y}_1-\bar{y}_0$$
\item $\bar{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}=(1-\bar{x})\bar{y}_0+\bar{x}\bar{y}_1-\left(\bar{y}_1-\bar{y}_0\right)\bar{x}=\bar{y}_0-\bar{x}\bar{y}_0+\bar{x}\bar{y}_1-\bar{x}\bar{y}_1+\bar{x}\bar{y}_0=\bar{y}_0$
\end{enumerate}
\item As proven in the previous problem, $\hat{\beta}_1=\bar{y}_1-\bar{y}_0$. Thus, it equals the sample average (proportion) of those who were employed and participated in the program minus the sample average (proportion) of those who were unemployed but did not partake in the program. Thus, $\hat{\beta}_1$ is simply the the difference in employment rates between those who participated in the program and those who did not within the given sample.
\item
\begin{enumerate}
\item $E\left[n^{-1}\sum_{i=1}^{n}\left[y_i(1)-y_i(0)\right]\right]=n^{-1}\sum_{i=1}^{n}\left(E\left[y_i(1)\right]-E\left[y_i(0)\right]\right)=n^{-1}\left(nE\left[y_i(1)\right]-nE\left[y_i(0)\right]\right)=E\left[y_i(1)\right]-E\left[y_i(0)\right]=\tau_{ate}$
\item $\bar{y}_0$ is the sample average of $y$ for the observations where $x_i=0$. $\bar{y}_1$ is the sample average of $y$ for the observations where $x_i=1$. $\bar{y}(1)\text{ and }\bar{y}(0)$ are relative to the entire sample such that $\bar{y}(\cdot)=n_{\cdot}n^{-1}\bar{y}_\cdot$.
\end{enumerate}
\item 
\begin{enumerate}
\item The difference in means estimator is generally no longer unbiased because the ceteris paribus condition is no longer satisfied. More specifically, the decision to participate may be correlated with the independent and dependent variables, which would cause the estimator to be biased.
\item One example that would cause bias is if wealthier individuals tended to not participate. It's likely wealth is correlated with both $unemployment$ and $program$, which would cause for a biased estimator.
\end{enumerate}
\item $$\text{Var}(u_i|x_i)=\text{Var}\left((1-x_i)u_i(0)+x_iu_i(1)\middle|x\right)$$
$$=\text{Var}\left((1-x_i)u_i(0)\middle|x\right)+\text{Var}\left(x_iu_i(1)\middle|x\right)$$
$$=(1-x_i)^2\text{Var}\left(u_i(0)\middle|x\right)+x_i^2\text{Var}\left(u_i(1)\middle|x\right)$$
$$=(1-x_i)^2\sigma_0^2+x_i^2\sigma_1^2$$
\item 
\begin{enumerate}
\item $P(\text{All }x=1 \text{ or all } x=0)=P(\text{All }x=1 \cup\text{ all } x=0)=P(\text{All }x=1)+ P(\text{ All } x=0)-P(\text{All }x=1 \cap\text{ all } x=0)=\rho^n+(1-\rho)^n-P(\emptyset)=\rho^n+(1-\rho)^n$. Because $0<\rho<1$ (and thus $0<(1-\rho)<1$),$$\lim_{n\to\infty}P(\text{All }x=1)=\lim_{n\to\infty}\rho^n=0$$Likewise,$$\lim_{n\to\infty}P(\text{All }x=0)=\lim_{n\to\infty}(1-\rho)^n=0$$
\item n=10:

$P(\text{All }x=1 \text{ or all } x=0)=P(\text{All }x=1 \cup\text{ all } x=0)=0.5^{10}+(1-0.5)^{10}=2\cdot 0.5^{10}\approx$`r 2*0.5^10`

n=100

$P(\text{All }x=1 \text{ or all } x=0)=P(\text{All }x=1 \cup\text{ all } x=0)=0.5^{100}+(1-0.5)^{100}=2\cdot 0.5^{100}\approx$`r 2*0.5^100`
\item n=10:

$P(\text{All }x=1 \text{ or all } x=0)=P(\text{All }x=1 \cup\text{ all } x=0)=0.9^{10}+(1-0.9)^{10}\approx$`r 0.9^10*0.1^10`

n=100

$P(\text{All }x=1 \text{ or all } x=0)=P(\text{All }x=1 \cup\text{ all } x=0)=0.9^{100}+(1-0.9)^{100}\approx$`r 0.9^100*0.1^100`
\end{enumerate}
\end{enumerate}


### Computer Exercises

```{r Chapter 2 Computer Exercise 1,echo=TRUE,include=TRUE,comment=NA}
#i
mean(k401k$prate)
mean(k401k$mrate)

#ii
lm1 <- lm(prate ~ mrate, data = k401k)
summary(lm1)

#iii
#The intercept suggests a participation rate of 83.07546% when the match rate is 0%
#The coefficient on mrate suggests an additional 5.861079% in participation rate for
#every additional percentage of prate

#iv
unname(lm1$coefficients[1])+unname(lm1$coefficients[2])*3.5
#The percentage exceeds 100% as this is a linear model. Clearly, this is not a reasonable
#prediction

#v
summary(lm1)["r.squared"]
#About 7.5%. Personally, I would consider this a lot for one variable, but I 
#expected it to be higher
rm(lm1)
```

```{r Chapter 2 Computer Exercise 2,echo=TRUE,include=TRUE,comment=NA}
#i
mean(ceosal2$salary)
mean(ceosal2$ceoten)

#ii
sum(ceosal2$ceoten==0)
max(ceosal2$ceoten)

#iii
lm(log(salary) ~ ceoten, data = ceosal2)
#An additional year as CEO is predicted to increase salary by about 1%.
```

```{r Chapter 2 Computer Exercise 3,echo=TRUE,include=TRUE,comment=NA}
#i
lm1 <- lm(sleep ~ totwrk, data = sleep75)
summary(lm1)
#The intercept (3586.37695) is the predicted minutes of sleep for an individual with
#0 minutes spent in paid work

#ii
unname(lm1$coefficients[2])*120
#A loss in 18 minutes of sleep for additional hours of totwrk does not seem large
rm(lm1)
```

```{r Chapter 2 Computer Exercise 4,echo=TRUE,include=TRUE,comment=NA}
#i
mean(wage2$wage)
mean(wage2$IQ)
sd(wage2$IQ)

#ii
lm1 <- lm(wage ~ IQ, data = wage2)
unname(lm1$coefficients[2])*15
summary(lm1)["r.squared"]
#IQ explains about 9.5% of the variation in wage

#iii
lm1 <- lm(log(wage) ~ IQ, data = wage2)
unname(lm1$coefficients[2])*15*100
rm(lm1)
```


```{r Chapter 2 Computer Exercise 5,echo=TRUE,include=TRUE,comment=NA}
#i
#ln(rd) = beta_0+ beta_1 ln(sales) + u

#ii
lm1 <- lm(lrd ~ lsales, data = rdchem)
summary(lm1)
#The estimated elasticity suggests a 1.07573% increase in rd for every percentage
#increase in sales
```

```{r Chapter 2 Computer Exercise 6,echo=TRUE,include=TRUE,comment=NA}
#i
#I expected a diminishing marginal return in the pass rate for each additional dollar spent.
#For one, the pass rate cannot exceed 100%. Additionally, only so much can be spent on a student
#before no additional gains are possible. Thus, a constant marginal return seems unlikely/unreasonable.

#ii
#The percentage change in math10 for a 1% increase in expend is beta_1/100
#Thus, beta_1/10 is the percentage point change in math10 given a 10% increase in expend

#iii
lm1 <- lm(math10 ~ lexpend, data = meap93)
summary(lm1)

#iv
unname(lm1$coefficients[2]) / 10

#v
max(meap93$math10)
#In this data set, the largest value of math10 is 66.7, which isn't particularly close to 100
rm(lm1)
```

```{r Chapter 2 Computer Exercise 7,echo=TRUE,include=TRUE,comment=NA}
#i
mean(charity$gift)
mean(charity$gift > 0)

#ii
mean(charity$mailsyear)
max(charity$mailsyear)
min(charity$mailsyear)

#iii
lm1 <- lm(gift ~ mailsyear, data = charity)
summary(lm1)

#iv
#The slope coefficient predicts about a 2.7 Dutch guilders increase in gift for every additional
#mailsyear. If each mailing costs on guilder, the charity is expected to make a net gain on each
#mailing, but this doesn't mean the charity makes a net gain on every mailing

#v
min(lm1$fitted.values)
#With this regression result, a negative value for mailsyear would be required to predict 0 for
#gift, which is unfeasible
rm(lm1)
```

```{r Chapter 2 Computer Exercise 8,echo=TRUE,include=TRUE,comment=NA}
#i
x <- runif(500, 0, 10)
mean(x)
sd(x)

#ii
u <- runif(500, 0, 36)
#No, the sample average is not zero because all of the u are positive
sd(u)

#iii
y <- 1 + 2 * x + u
lm1 <- lm(y ~ x)
summary(lm1)
#No, they are not equal because the unobserved factors are not included in the regression model

#iv
sum(lm1$residuals)
sum(lm1$residuals * x)

#v
sum(u)
sum(u * x)
#The sum of the error terms and the errors terms times x generally won't sum
#to 0 like the residuals will

#vi
x <- runif(500, 0, 10)
mean(x)
sd(x)
u <- runif(500, 0, 36)
sd(u)
y <- 1 + 2 * x + u
lm2 <- lm(y ~ x)
summary(lm2)
#The results are not the same because the samples are different
rm(x,y,u,lm1,lm2)
```

```{r Chapter 2 Computer Exercise 9,echo=TRUE,include=TRUE,comment=NA}
county_murders <- as_tibble(countymurders) %>%
  filter(year == 1996)

#i
sum(county_murders$murders == 0)
sum(county_murders$execs > 0)
max(county_murders$execs)

#ii
lm1 <- lm(murders ~ execs, data = county_murders)
summary(lm1)

#iii
#The model predicts an additional 58.555 executions for every additional murder in a county.
#This does not suggest a deterrent effect of capital punishment

#iv
min(lm1$fitted.values)
unname(lm1$residuals[which(county_murders$execs == 0)[1]])

#v
#A simple regression analysis not well suited for determining whether capital punishment
#has a deterrent effect on murders because there are many unobserved factors that are likely
#correlated with both murders and execs, which leads to a biased slope estimate. Also, there's
#likely reverse causality in which murders causes more execs
rm(county_murders,lm1)
```

```{r Chapter 2 Computer Exercise 10 parts i-ii,echo=TRUE,include=TRUE,comment=NA}
#i
length(catholic$math12)
mean(catholic$math12)
sd(catholic$math12)
mean(catholic$read12)
sd(catholic$read12)

#ii
lm1 <- lm(math12 ~ read12, data = catholic)
```

$$\widehat{math12} = `r unname(unlist(lm1["coefficients"]))[1]`+`r unname(unlist(lm1["coefficients"]))[2]`read12$$
$$n = `r length(catholic$math12)`, R^2 = `r unname(unlist(summary(lm1)["r.squared"]))`$$

```{r Chapter 2 Computer Exercise 10 parts iii-v,echo=TRUE,include=TRUE,comment=NA}
#iii
#Yes, the intercept suggests prediction of 15.15304 on math12 for a score of 0 on read12

#iv
#I'm not surprised by the coefficient. I would expect the scores to be positively correlated.
#I'm somewhat surprised by how large the coefficient of determination is.

#v
#I would counter their statement by stating it's more correlational rather than causal.
#Unobserved factors correlated with math12 and read12 are uncontrolled for, which would
#make the slope coefficient biased (if interpreted as a causal effect)
```

```{r Chapter 2 Computer Exercise 11,echo=TRUE,include=TRUE,comment=NA}
#i
length(gpa1$colGPA)
mean(gpa1$colGPA)
max(gpa1$colGPA)

#ii
sum(gpa1$PC)

#iii
lm1 <- lm(colGPA ~ PC, data = gpa1)
summary(lm1)
#The intercept estimate predicts a colGPA of 2.98941 for a student without a PC.
#The slope estimate predicts a 0.16952 estimate of the "treatment" effect of having a PC.
#The slope suggests a positive relationship between colGPA and PC (as one might expect).

#iv
unname(unlist(summary(lm1)["r.squared"]))
#The r-squared suggests not much of the variation in colGPA is explained by PC

#v
#No, unobserved factors that are correlated with colGPA and PC do not allow for beta_1
#to be interpreted as an unbiased estimate of the causal effect of owning a PC on colGPA.

rm(lm1)
```

\newpage

# Chapter 3

## Notes

### Multiple Regression Analysis

\textbf{Multiple regression analysis} is generally better fit to predict causal effects over simple regression analysis because it allows us to explicitly control for many factors that are correlated with the singular independent variable (from the simple regression model) and the dependent variable.

In general, the \textbf{multiple linear regression (MLR) model} takes the form$$y=\beta_0+\beta_1x_1+\beta_2x_2+\dots+\beta_kx_k+u$$

### OLS Estimators Derivation (MLR)

Synonymous to the case with simple linear regression analysis, using OLS to obtain estimators of the parameters $\beta_0,\beta_1,\dots,\beta_k$ yields the following:
$$\min_{\hat{\beta}_0,\hat{\beta}_1,\dots,\hat{\beta}_k}SSR=\sum_{i=1}^{n}\left(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_kx_{ik}\right)^2$$
$$\frac{\partial{SSR}}{\partial{\hat{\beta}_0}}=\sum_{i=1}^{n}-2\left(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_kx_{ik}\right)=0$$
$$\frac{\partial{SSR}}{\partial{\hat{\beta}_1}}=\sum_{i=1}^{n}-2x_{i1}\left(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_kx_{ik}\right)=0$$
$$\dots$$
$$\frac{\partial{SSR}}{\partial{\hat{\beta}_k}}=\sum_{i=1}^{n}-2x_{ik}\left(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_kx_{ik}\right)=0$$
These are often called the OLS \textbf{first order conditions}. As with the simple regression model, the OLS first order conditions can be obtained by the method of moments: under the assumptions that $E[u]=0$ and $E[x_ju]=0\ \forall \ j=1,\dots,k$.

From the first FOC,$$\sum_{i=1}^{n}\left(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_kx_{ik}\right)=0$$
$$\to\sum_{i=1}^{n}\hat{\beta}_0=\sum_{i=1}^{n}\left(y_i-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_kx_{ik}\right)$$
$$\to n\hat{\beta}_0=n\bar{y}-n\hat{\beta}_1\bar{x_1}-\dots-n\hat{\beta}_k\bar{x_k}$$
$$\to \hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x_1}-\dots-\hat{\beta}_k\bar{x_k}$$
\small
\textit{Note:} Like in the simple linear regression case, the significance of this FOC is that, using least squares criteria, our line of best fit runs through the sample means $\bar{y}$ and $\bar{x}_j$ for $j=1,\dots,k$.
\normalsize

To derive the slope estimators, start by regressing $x_\ell$ on all of the other regressors in the model, which takes the form$$x_\ell=\gamma_0+\gamma_1x_1+\dots+\gamma_{\ell-1}x_{\ell-1}+\gamma_{\ell+1}x_{\ell+1}+\dots+\gamma_{k}x_{k}+r_{i\ell}$$If we denote the residuals of the model as $\hat{r}_{i\ell}$ and the predicted values as $\hat{x}_{i\ell}$, then$$x_{i\ell}=\hat{x}_{i\ell}+\hat{r}_{i\ell}$$Plugging this derivation into the $(\ell+1)^{th}$ first order condition:
$$\frac{\partial{SSR}}{\partial{\beta_\ell}}=\sum_{i=1}^{n}-2x_{i\ell}\left(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_kx_{ik}\right)=0$$
$$\to\sum_{i=1}^{n}\left(\hat{x}_{i\ell}+\hat{r}_{i\ell}\right)\left(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_kx_{ik}\right)=0$$
$$\to\sum_{i=1}^{n}\hat{x}_{i\ell}\left(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_kx_{ik}\right)+\sum_{i=1}^{n}\hat{r}_{i\ell}\left(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_kx_{ik}\right)=0$$
$$\to\sum_{i=1}^{n}\hat{x}_{i\ell}\hat{u}_i+\sum_{i=1}^{n}\hat{r}_{i\ell}\left(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_kx_{ik}\right)=0$$
Because $\hat{x}_{i\ell}$ is simply a linear combination of all the other regressors in the model (i.e., $\hat{x}_{i\ell}=\hat{\gamma}_0+\hat{\gamma}_1x_{i1}+\dots+\hat{\gamma}_{\ell-1}x_{i\ell-1}+\gamma_{\ell+1}x_{i\ell+1}+\dots+\gamma_{k}x_{ik}$), it follows that $\sum_{i=1}^{n}\hat{x}_{i\ell}\hat{u}_i=0$.
$$\to\sum_{i=1}^{n}\hat{r}_{i\ell}\left(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_kx_{ik}\right)=0$$
$$\to\sum_{i=1}^{n}\hat{r}_{i\ell}\left(y_i-\hat{\beta}_\ell x_{i\ell}\right) +\sum_{i=1}^{n}\hat{r}_{i\ell}\left(-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\dots-\hat{\beta}_{\ell-1}x_{i\ell-1}-\hat{\beta}_{\ell+1}x_{i\ell+1}-\dots-\hat{\beta}_kx_{ik}\right)=0$$
Because $\hat{r}_{i\ell}$ are the residuals from regressing $x_\ell$ on all the other regressors, $\sum_{i=1}^{n}x_{ij}\hat{r}_{i\ell}=0\ \forall\ j\neq \ell$.
$$\to\sum_{i=1}^{n}\hat{r}_{i\ell}\left(y_i-\hat{\beta}_\ell x_{i\ell}\right)=0$$
$$\to\sum_{i=1}^{n}\hat{r}_{i\ell}\left(y_i-\hat{\beta}_\ell\left(\hat{x}_{i\ell}+\hat{r}_{i\ell}\right)\right)=0$$
$$\to\sum_{i=1}^{n}\hat{r}_{i\ell}\left(y_i-\hat{\beta}_\ell\hat{r}_{i\ell}\right)-\hat{\beta}_\ell\sum_{i=1}^{n}\hat{r}_{i\ell}\hat{x}_{i\ell}=0$$
Since $\sum_{i=1}^{n}\hat{r}_{i\ell}\hat{x}_{i\ell}=0$ (This can be thought of as $\sum_{i=1}^{n}\hat{r}_{i\ell}\hat{x}_{i\ell}=0\to\sum_{i=1}^{n}\hat{u}_{i}\hat{y}_{i}=0\to\hat{\beta}_0\sum_{i=1}^{n}\hat{u}_{i}+\hat{\beta}_1\sum_{i=1}^{n}x_i\hat{u}_{i}=0+0=0$ in the SLR case),
$$\to\sum_{i=1}^{n}\hat{r}_{i\ell}\left(y_i-\hat{\beta}_\ell\hat{r}_{i\ell}\right)=0$$
$$\to\hat{\beta}_\ell\sum_{i=1}^{n}\hat{r}_{i\ell}^2=\sum_{i=1}^{n}\hat{r}_{i\ell}y_i$$
$$\to\hat{\beta}_\ell=\frac{\sum_{i=1}^{n}\hat{r}_{i\ell}y_i}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}$$

### MLR Interpretation

In MLR analysis, the slope estimates $\hat{\beta}_1,\dots,\hat{\beta}_k$ are interpreted as the \textbf{partial effects} of the corresponding explanatory variables on the dependent variable. In other words, they have a \textbf{ceteris paribus} interpretation.

\newpage

\underline{OLS Fitted Values and Residuals Properties (MLR)}

\textit{Note:} The OLS fitted values and residuals have some important properties that are immediate extensions from the bivariate case:

\begin{enumerate}
\item The sample average of the residuals is zero and so $\bar{y}=\bar{\hat{y}}$
\item The sample covariance between each independent variable and the OLS residuals is zero (i.e., $\hat{\sigma}_{x_j\hat{u}}=0\ \forall\ j=1,\dots,k$). Hence,$$\hat{\sigma}_{\hat{y}\hat{u}}=0$$
\item The point $\left(\bar{x}_1,\dots,\bar{x}_k,\bar{y}\right)$ always lies on the OLS regression line: $\bar{y}=\hat{\beta}_0+\hat{\beta}_1x_1+\dots+\hat{\beta}_kx_k$
\end{enumerate}

The first two properties are immediate consequences of the OLS first order conditions used to obtain the OLS estimators. Namely,$$\sum_{i=1}^{n}u_i=0;\ \sum_{i=1}^{n}x_{ij}u_i=0,$$where $\sum_{i=1}^{n}x_{ij}u_i=0$ implies that each regressor has a zero sample covariance with $\hat{u}_i$. The third property was derived earlier in the "OLS Estimators Derivation (MLR)" section.

### Partialling Out Interpretation

Earlier, we derived $$\hat{\beta}_{\ell}=\frac{\sum_{i=1}^{n}\hat{r}_{i\ell}y_i}{\sum_{i=1}^{n}\hat{r}_{i\ell}^2}$$Let $\ell=1$ in the model $y=\beta_0+\beta_1x_1+\beta_2x_2+u$. Then, $\hat{r}_{i1}$ are the OLS residuals from a simple regression of $x_1$ on $x_2$. The above derivation shows that we can then do a simple regression of $y$ on $\hat{r}_1$ to obtain $\hat{\beta}_1$. (Note that the residuals $\hat{r}_{i1}$ have a zero sample average, and so $\hat{\beta}_1$ is the usual slope estimate from simple regression). The interpretation of this is that the residuals $\hat{r}_{i1}$ are the part of $x_{i1}$ that are uncorrelated with $x_{i2}$. In other words, $\hat{r}_{i1}$ is $x_{i1}$ after the effects of $x_{i2}$ have been partialled out. Thus, $\hat{\beta}_1$ measures the sample relationship between $y$ and $x_1$ after $x_2$ has been partialled out. This result is usually called the \textbf{Frisch-Waugh theorem}.




